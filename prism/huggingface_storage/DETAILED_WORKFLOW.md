# HippoRAG Modular Implementation - Chi tiáº¿t Workflow

## ðŸ“‹ Tá»•ng quan

HippoRAG lÃ  má»™t há»‡ thá»‘ng Retrieval-Augmented Generation (RAG) sá»­ dá»¥ng knowledge graph Ä‘á»ƒ cáº£i thiá»‡n kháº£ nÄƒng retrieval. PhiÃªn báº£n modular nÃ y chia nhá» quÃ¡ trÃ¬nh thÃ nh cÃ¡c bÆ°á»›c rÃµ rÃ ng, dá»… hiá»ƒu vÃ  maintain.

## ðŸ”„ Workflow tá»•ng thá»ƒ

```
Documents â†’ Indexing â†’ Knowledge Graph â†’ Retrieval â†’ QA â†’ Answers
    â†“           â†“              â†“              â†“         â†“
   Text    OpenIE+Emb     Graph Build    Fact+PPR   Answer
```

---

## ðŸ—ï¸ PHASE 1: INITIALIZATION & SETUP

### 1.1 Khá»Ÿi táº¡o Configuration

```python
config = RetrievalConfig(
    retrieval_top_k=10,      # Sá»‘ documents tráº£ vá»
    linking_top_k=5,         # Sá»‘ facts top Ä‘á»ƒ link
    qa_top_k=3,              # Sá»‘ docs dÃ¹ng cho QA
    passage_node_weight=0.05, # Trá»ng sá»‘ passage trong PPR
    damping=0.5,             # Damping factor cho PageRank
    synonymy_edge_topk=10,   # Top synonyms
    synonymy_edge_sim_threshold=0.8,  # NgÆ°á»¡ng similarity
    embedding_batch_size=32   # Batch size cho embedding
)
```

**Chi tiáº¿t:**
- `RetrievalConfig` Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong `typing.py`
- LÆ°u trá»¯ táº¥t cáº£ hyperparameters cáº§n thiáº¿t
- Sá»­ dá»¥ng dataclass Ä‘á»ƒ type safety
- CÃ³ thá»ƒ override tá»«ng parameter riÃªng láº»

### 1.2 Khá»Ÿi táº¡o Models

```python
# Cáº§n 3 models chÃ­nh:
embedding_model = YourEmbeddingModel()  # Äá»ƒ encode text thÃ nh vectors
openie_model = YourOpenIEModel()        # Äá»ƒ extract entities vÃ  relations
llm_model = YourLLMModel()              # Äá»ƒ generate answers
```

**Chi tiáº¿t:**
- **Embedding Model**: Chuyá»ƒn text thÃ nh dense vectors (thÆ°á»ng 768/1024 dims)
- **OpenIE Model**: Extract (subject, predicate, object) triples tá»« text
- **LLM Model**: Generate cÃ¢u tráº£ lá»i tá»« retrieved documents

---

## ðŸ—‚ï¸ PHASE 2: INDEXING WORKFLOW

### 2.1 Khá»Ÿi táº¡o HippoIndexer

```python
indexer = HippoIndexer(
    working_dir="./storage",
    openie_model=openie_model,
    embedding_model=embedding_model,
    config=config,
    force_rebuild=False
)
```

**Chi tiáº¿t trong `indexing.py`:**

1. **Táº¡o working directory** náº¿u chÆ°a tá»“n táº¡i
2. **Initialize 3 EmbeddingStores:**
   - `chunk_store`: LÆ°u documents vÃ  embeddings
   - `entity_store`: LÆ°u entities vÃ  embeddings  
   - `fact_store`: LÆ°u facts (triples) vÃ  embeddings
3. **Initialize KnowledgeGraph** vá»›i igraph
4. **Initialize OpenIEResultsManager** Ä‘á»ƒ lÆ°u OpenIE results

#### 2.1.1 EmbeddingStore Details

Má»—i `EmbeddingStore` quáº£n lÃ½:
```python
class EmbeddingStore:
    def __init__(self, embedding_model, storage_path, batch_size, store_type):
        self.embedding_model = embedding_model
        self.storage_path = storage_path          # ThÆ° má»¥c lÆ°u data
        self.store_type = store_type              # "chunk", "entity", "fact"
        self.text_to_hash_id = {}                 # Mapping text -> hash_id
        self._id_to_rows = {}                     # Mapping hash_id -> data
```

**Cáº¥u trÃºc file storage:**
```
storage/
â”œâ”€â”€ chunk_embeddings/
â”‚   â”œâ”€â”€ metadata.json        # Text mappings
â”‚   â””â”€â”€ embeddings.npy       # Numpy array vectors
â”œâ”€â”€ entity_embeddings/
â”‚   â”œâ”€â”€ metadata.json
â”‚   â””â”€â”€ embeddings.npy
â””â”€â”€ fact_embeddings/
    â”œâ”€â”€ metadata.json
    â””â”€â”€ embeddings.npy
```

### 2.2 Document Processing Pipeline

```python
result = indexer.index_documents(documents)
```

#### 2.2.1 BÆ°á»›c 1: Insert Documents vÃ o Chunk Store

**Location:** `indexing.py`, method `index_documents()`

```python
# 1. Insert documents vÃ o chunk store
self.chunk_store.insert_strings(documents)
chunk_to_rows = self.chunk_store.get_all_id_to_rows()
```

**Chi tiáº¿t process:**

1. **Compute Hash IDs:**
   ```python
   for document in documents:
       hash_id = compute_mdhash_id(document, prefix="chunk-")
       # hash_id = "chunk-" + md5(document)
   ```

2. **Check Duplicates:**
   - Chá»‰ process documents chÆ°a cÃ³ trong store
   - Skip documents Ä‘Ã£ Ä‘Æ°á»£c index

3. **Generate Embeddings:**
   ```python
   if new_documents:
       embeddings = embedding_model.batch_encode(new_documents, norm=True)
       # embeddings shape: (n_docs, embedding_dim)
   ```

4. **Save to Storage:**
   ```python
   # LÆ°u vÃ o metadata.json
   metadata = {
       'text_to_hash_id': {doc: hash_id, ...},
       'id_to_rows': {hash_id: {'hash_id': hash_id, 'content': doc}, ...}
   }
   
   # LÆ°u embeddings vÃ o .npy file
   np.save(embedding_path, embeddings)
   ```

#### 2.2.2 BÆ°á»›c 2: OpenIE Extraction

**Location:** `indexing.py`, method `_perform_openie()`

```python
ner_results, triple_results = self._perform_openie(documents, chunk_to_rows)
```

**Chi tiáº¿t process:**

1. **Check Existing Results:**
   ```python
   existing_chunk_keys = self.openie_manager.get_existing_chunk_keys()
   chunks_to_process = {k: v for k, v in chunk_to_rows.items() 
                       if k not in existing_chunk_keys}
   ```

2. **Perform OpenIE:**
   ```python
   # Gá»i OpenIE model Ä‘á»ƒ extract entities vÃ  relations
   ner_results, triple_results = self.openie_model.batch_openie(chunks_to_process)
   ```

   **Output format:**
   ```python
   ner_results = {
       "chunk-abc123": NerRawOutput(
           chunk_id="chunk-abc123",
           response="Raw model response",
           metadata={"confidence": 0.9},
           unique_entities=["AI", "machine learning", "algorithms"]
       )
   }
   
   triple_results = {
       "chunk-abc123": TripleRawOutput(
           chunk_id="chunk-abc123", 
           response="Raw model response",
           metadata={"confidence": 0.8},
           triples=[
               ("AI", "transforms", "industry"),
               ("machine learning", "uses", "algorithms")
           ]
       )
   }
   ```

3. **Merge vá»›i Existing Results:**
   ```python
   self.openie_manager.merge_results(chunks_to_process, ner_results, triple_results)
   ```

4. **Save OpenIE Results:**
   ```python
   # LÆ°u vÃ o openie_results.json
   {
       "docs": [
           {
               "idx": "chunk-abc123",
               "passage": "AI transforms industry...",
               "extracted_entities": ["AI", "machine learning"],
               "extracted_triples": [("AI", "transforms", "industry")]
           }
       ],
       "avg_ent_chars": 12.5,
       "avg_ent_words": 2.1
   }
   ```

#### 2.2.3 BÆ°á»›c 3: Extract Entities vÃ  Facts

**Location:** `indexing.py`, method `_extract_entities_and_facts()`

```python
entities, facts = self._extract_entities_and_facts(chunk_to_rows, ner_results, triple_results)
```

**Chi tiáº¿t process:**

1. **Process Triples:**
   ```python
   chunk_triples = []
   for chunk_id in chunk_ids:
       if chunk_id in triple_results:
           processed_triples = [text_processing(t) for t in triple_results[chunk_id].triples]
           chunk_triples.append(processed_triples)
   ```

2. **Extract Entity Nodes:**
   ```python
   def extract_entity_nodes(chunk_triples):
       all_entities = set()
       for chunk_triples_list in chunk_triples:
           for triple in chunk_triples_list:
               subject = str(triple[0]).strip().lower()
               obj = str(triple[2]).strip().lower()
               all_entities.add(subject)
               all_entities.add(obj)
       return list(all_entities)
   ```

3. **Flatten Facts:**
   ```python
   def flatten_facts(chunk_triples):
       flattened = []
       for chunk_triples_list in chunk_triples:
           for triple in chunk_triples_list:
               if len(triple) >= 3:
                   flattened.append(tuple(triple[:3]))
       return flattened
   ```

**Output:**
```python
entities = ["ai", "machine learning", "algorithms", "industry", ...]
facts = [
    ("ai", "transforms", "industry"),
    ("machine learning", "uses", "algorithms"),
    ...
]
```

#### 2.2.4 BÆ°á»›c 4: Index Entities vÃ  Facts

```python
# Index entities
self.entity_store.insert_strings(entities)

# Index facts
fact_strings = [str(fact) for fact in facts]  # Convert tuples to strings
self.fact_store.insert_strings(fact_strings)
```

**Chi tiáº¿t:**
- Entities Ä‘Æ°á»£c embed riÃªng vá»›i instruction Ä‘áº·c biá»‡t cho entities
- Facts Ä‘Æ°á»£c convert thÃ nh string format: `"('ai', 'transforms', 'industry')"`
- Má»—i loáº¡i cÃ³ embedding riÃªng Ä‘á»ƒ optimize cho use case

#### 2.2.5 BÆ°á»›c 5: Build Knowledge Graph

**Location:** `indexing.py`, method `_build_knowledge_graph()`

```python
self._build_knowledge_graph(chunk_to_rows, ner_results, triple_results)
```

**Chi tiáº¿t process:**

1. **Prepare Data:**
   ```python
   chunk_ids = list(chunk_to_rows.keys())
   chunk_triples = []          # Triples cho má»—i chunk
   chunk_triple_entities = []  # Entities cho má»—i chunk
   
   for chunk_id in chunk_ids:
       if chunk_id in triple_results:
           processed_triples = [text_processing(t) for t in triple_results[chunk_id].triples]
           chunk_triples.append(processed_triples)
           
           # Extract entities cho chunk nÃ y
           entities = set()
           for triple in processed_triples:
               if len(triple) >= 3:
                   entities.add(str(triple[0]))
                   entities.add(str(triple[2]))
           chunk_triple_entities.append(list(entities))
   ```

2. **Add Fact Edges:**
   ```python
   self.knowledge_graph.add_fact_edges(chunk_ids, chunk_triples)
   ```

   **Chi tiáº¿t trong `graph.py`:**
   ```python
   def add_fact_edges(self, chunk_ids, chunk_triples):
       for chunk_key, triples in zip(chunk_ids, chunk_triples):
           entities_in_chunk = set()
           
           for triple in triples:
               if len(triple) >= 3:
                   # Táº¡o hash IDs cho subject vÃ  object
                   node_key = compute_mdhash_id(content=triple[0], prefix="entity-")
                   node_2_key = compute_mdhash_id(content=triple[2], prefix="entity-")
                   
                   # Cáº­p nháº­t edge statistics (bidirectional)
                   self.node_to_node_stats[(node_key, node_2_key)] += 1
                   self.node_to_node_stats[(node_2_key, node_key)] += 1
                   
                   entities_in_chunk.add(node_key)
                   entities_in_chunk.add(node_2_key)
           
           # Map entities tá»›i chunks chá»©a chÃºng
           for node in entities_in_chunk:
               self.ent_node_to_chunk_ids[node].add(chunk_key)
   ```

3. **Add Passage Edges:**
   ```python
   num_new_chunks = self.knowledge_graph.add_passage_edges(chunk_ids, chunk_triple_entities)
   ```

   **Chi tiáº¿t:**
   ```python
   def add_passage_edges(self, chunk_ids, chunk_triple_entities):
       for idx, chunk_key in enumerate(chunk_ids):
           for chunk_ent in chunk_triple_entities[idx]:
               node_key = compute_mdhash_id(chunk_ent, prefix="entity-")
               # Connect passage node vá»›i entity node
               self.node_to_node_stats[(chunk_key, node_key)] = 1.0
   ```

4. **Add Synonymy Edges (náº¿u cÃ³ embedding model):**
   ```python
   if self.embedding_model:
       synonymy_config = {
           'topk': self.config.synonymy_edge_topk,
           'sim_threshold': self.config.synonymy_edge_sim_threshold,
           # ...
       }
       self.knowledge_graph.add_synonymy_edges(self.entity_store, synonymy_config)
   ```

   **Chi tiáº¿t trong `graph.py`:**
   ```python
   def add_synonymy_edges(self, entity_embedding_store, synonymy_config):
       # Get all entity embeddings
       entity_node_keys = list(entity_id_to_row.keys())
       entity_embs = entity_embedding_store.get_embeddings(entity_node_keys)
       
       # KNN search Ä‘á»ƒ tÃ¬m similar entities
       query_node_key2knn_node_keys = retrieve_knn(
           query_ids=entity_node_keys,
           key_ids=entity_node_keys,
           query_vecs=entity_embs,
           key_vecs=entity_embs,
           k=synonymy_config['topk']
       )
       
       # Add synonymy edges
       for node_key in entity_node_keys:
           entity = entity_id_to_row[node_key]["content"]
           
           # Chá»‰ process entities cÃ³ Ã­t nháº¥t 3 kÃ½ tá»± alphanumeric
           if len(re.sub('[^A-Za-z0-9]', '', entity)) > 2:
               nns = query_node_key2knn_node_keys[node_key]
               
               for nn, score in zip(nns[0], nns[1]):
                   if score < sim_threshold:
                       break
                   
                   if nn != node_key:
                       self.node_to_node_stats[(node_key, nn)] = score
   ```

5. **Build Final Graph Structure:**
   ```python
   self.knowledge_graph.build_graph_structure(self.entity_store, self.chunk_store)
   ```

   **Chi tiáº¿t:**
   - Add all nodes vÃ o igraph
   - Add all edges vá»›i weights
   - Create mappings tá»« node names â†’ vertex indices
   - Update entity_node_keys, passage_node_keys, etc.

#### 2.2.6 BÆ°á»›c 6: Save Everything

```python
self.knowledge_graph.save_graph()        # LÆ°u graph.pickle
self.openie_manager.save_results()       # LÆ°u openie_results.json
```

**Chi tiáº¿t file outputs:**
```
storage/
â”œâ”€â”€ chunk_embeddings/
â”‚   â”œâ”€â”€ metadata.json
â”‚   â””â”€â”€ embeddings.npy
â”œâ”€â”€ entity_embeddings/
â”‚   â”œâ”€â”€ metadata.json  
â”‚   â””â”€â”€ embeddings.npy
â”œâ”€â”€ fact_embeddings/
â”‚   â”œâ”€â”€ metadata.json
â”‚   â””â”€â”€ embeddings.npy
â”œâ”€â”€ graph.pickle              # iGraph object
â””â”€â”€ openie_results.json       # OpenIE extraction results
```

**IndexingResult:**
```python
result = IndexingResult(
    num_docs_indexed=len(documents),
    num_entities_extracted=len(entities),
    num_triples_extracted=len(facts),
    processing_time=end_time - start_time,
    graph_info=self.knowledge_graph.get_graph_info()
)
```

---

## ðŸ” PHASE 3: RETRIEVAL WORKFLOW

### 3.1 Khá»Ÿi táº¡o HippoRetriever

```python
embedding_stores = indexer.get_embedding_stores()
knowledge_graph = indexer.get_knowledge_graph()

retriever = HippoRetriever(
    knowledge_graph=knowledge_graph,
    embedding_stores=embedding_stores,
    llm_model=llm_model,
    rerank_filter=None,
    config=config
)
```

**Chi tiáº¿t trong `retrieval.py`:**

1. **Store References:**
   ```python
   self.chunk_store = embedding_stores.get('chunk_store')
   self.entity_store = embedding_stores.get('entity_store') 
   self.fact_store = embedding_stores.get('fact_store')
   ```

2. **Initialize Caches:**
   ```python
   self.query_embeddings_cache = {
       'triple': {},    # Cache cho fact retrieval embeddings
       'passage': {}    # Cache cho passage retrieval embeddings
   }
   ```

3. **Prepare Retrieval Data:**
   ```python
   def _prepare_retrieval_data(self):
       # Load precomputed embeddings cho fast retrieval
       entity_keys = self.knowledge_graph.entity_node_keys
       self.entity_embeddings = self.entity_store.get_embeddings(entity_keys)
       
       passage_keys = self.knowledge_graph.passage_node_keys  
       self.passage_embeddings = self.chunk_store.get_embeddings(passage_keys)
       
       fact_keys = self.fact_store.get_all_ids()
       self.fact_embeddings = self.fact_store.get_embeddings(fact_keys)
   ```

### 3.2 Query Processing Pipeline

```python
results = retriever.retrieve(queries, embedding_model)
```

#### 3.2.1 BÆ°á»›c 1: Get Query Embeddings

**Location:** `retrieval.py`, method `get_query_embeddings()`

```python
def get_query_embeddings(self, queries, embedding_model):
    new_queries = []
    
    for query in queries:
        if query not in self.query_embeddings_cache['triple'] or \
           query not in self.query_embeddings_cache['passage']:
            new_queries.append(query)
    
    if new_queries:
        # Embeddings cho fact retrieval
        fact_embeddings = embedding_model.batch_encode(
            new_queries, 
            instruction="query_to_fact",
            norm=True
        )
        
        # Embeddings cho passage retrieval  
        passage_embeddings = embedding_model.batch_encode(
            new_queries,
            instruction="query_to_passage", 
            norm=True
        )
        
        # Cache embeddings
        for query, fact_emb, pass_emb in zip(new_queries, fact_embeddings, passage_embeddings):
            self.query_embeddings_cache['triple'][query] = fact_emb
            self.query_embeddings_cache['passage'][query] = pass_emb
```

**Chi tiáº¿t:**
- Táº¡o 2 loáº¡i embeddings khÃ¡c nhau cho má»—i query
- `query_to_fact`: Optimize Ä‘á»ƒ match vá»›i facts/triples
- `query_to_passage`: Optimize Ä‘á»ƒ match vá»›i passages
- Cache Ä‘á»ƒ trÃ¡nh recompute cho queries giá»‘ng nhau

#### 3.2.2 BÆ°á»›c 2: Fact Scoring vÃ  Retrieval

**Cho má»—i query:**

1. **Get Fact Scores:**
   ```python
   query_fact_scores = self.get_fact_scores(query)
   ```

   **Chi tiáº¿t:**
   ```python
   def get_fact_scores(self, query):
       query_embedding = self.query_embeddings_cache['triple'].get(query)
       
       # Compute similarity vá»›i táº¥t cáº£ facts
       query_fact_scores = np.dot(self.fact_embeddings, query_embedding.T)
       query_fact_scores = np.squeeze(query_fact_scores)
       
       # Normalize scores vá» [0,1]
       query_fact_scores = min_max_normalize(query_fact_scores)
       return query_fact_scores
   ```

2. **Rerank Facts:**
   ```python
   top_k_fact_indices, top_k_facts, rerank_log = self.rerank_facts(query, query_fact_scores)
   ```

   **Chi tiáº¿t:**
   ```python
   def rerank_facts(self, query, query_fact_scores):
       link_top_k = self.config.linking_top_k
       
       # Get top facts by score
       if len(query_fact_scores) <= link_top_k:
           candidate_fact_indices = np.argsort(query_fact_scores)[::-1].tolist()
       else:
           candidate_fact_indices = np.argsort(query_fact_scores)[-link_top_k:][::-1].tolist()
       
       # Get actual facts
       fact_keys = self.fact_store.get_all_ids()
       candidate_fact_ids = [fact_keys[idx] for idx in candidate_fact_indices]
       fact_rows = self.fact_store.get_rows(candidate_fact_ids)
       candidate_facts = [eval(fact_rows[fid]['content']) for fid in candidate_fact_ids]
       
       # Apply reranking filter náº¿u cÃ³
       if self.rerank_filter:
           top_k_fact_indices, top_k_facts, _ = self.rerank_filter(
               query, candidate_facts, candidate_fact_indices, len_after_rerank=link_top_k
           )
       else:
           top_k_fact_indices = candidate_fact_indices
           top_k_facts = candidate_facts
       
       return top_k_fact_indices, top_k_facts, rerank_log
   ```

#### 3.2.3 BÆ°á»›c 3: Graph Search vá»›i Facts

```python
if len(top_k_facts) == 0:
    # Fallback vá» Dense Passage Retrieval
    sorted_doc_ids, sorted_doc_scores = self.dense_passage_retrieval(query)
else:
    # Sá»­ dá»¥ng facts Ä‘á»ƒ guide graph search
    sorted_doc_ids, sorted_doc_scores = self.graph_search_with_facts(
        query, top_k_facts, top_k_fact_indices, query_fact_scores
    )
```

**Chi tiáº¿t Graph Search trong `retrieval.py`:**

1. **Initialize Weights:**
   ```python
   phrase_weights = np.zeros(len(self.knowledge_graph.entity_node_keys))
   passage_weights = np.zeros(len(self.knowledge_graph.passage_node_keys))
   ```

2. **Compute Phrase Weights tá»« Facts:**
   ```python
   linking_score_map = {}
   phrase_scores = {}
   
   for rank, fact in enumerate(top_k_facts):
       if len(fact) >= 3:
           subject_phrase = fact[0].lower()
           object_phrase = fact[2].lower()
           fact_score = query_fact_scores[top_k_fact_indices[rank]]
           
           for phrase in [subject_phrase, object_phrase]:
               phrase_key = compute_mdhash_id(content=phrase, prefix="entity-")
               
               # Find phrase index trong entity_node_keys
               if phrase_key in self.knowledge_graph.entity_node_keys:
                   phrase_idx = self.knowledge_graph.entity_node_keys.index(phrase_key)
                   
                   # Weight by inverse frequency
                   ent_chunk_count = len(self.knowledge_graph.ent_node_to_chunk_ids.get(phrase_key, set()))
                   weighted_score = fact_score / max(ent_chunk_count, 1)
                   
                   phrase_weights[phrase_idx] += weighted_score
   ```

3. **Get DPR Scores cho Passages:**
   ```python
   dpr_sorted_doc_ids, dpr_sorted_doc_scores = self.dense_passage_retrieval(query)
   normalized_dpr_scores = min_max_normalize(dpr_sorted_doc_scores)
   
   # Set passage weights
   for i, doc_id in enumerate(dpr_sorted_doc_ids.tolist()):
       if i < len(self.knowledge_graph.passage_node_keys):
           passage_weights[doc_id] = normalized_dpr_scores[i] * self.config.passage_node_weight
   ```

4. **Combine Weights cho PPR:**
   ```python
   node_weights = self.knowledge_graph.get_node_weights_for_ppr(phrase_weights, passage_weights)
   ```

   **Chi tiáº¿t trong `graph.py`:**
   ```python
   def get_node_weights_for_ppr(self, phrase_weights, passage_weights):
       node_weights = np.zeros(self.graph.vcount())
       
       # Set phrase weights
       for i, entity_idx in enumerate(self.entity_node_idxs):
           if i < len(phrase_weights):
               node_weights[entity_idx] = phrase_weights[i]
       
       # Set passage weights
       for i, passage_idx in enumerate(self.passage_node_idxs):
           if i < len(passage_weights):
               node_weights[passage_idx] = passage_weights[i]
       
       return node_weights
   ```

5. **Run Personalized PageRank:**
   ```python
   ppr_sorted_doc_ids, ppr_sorted_doc_scores = self.knowledge_graph.run_personalized_pagerank(
       node_weights, damping=self.config.damping
   )
   ```

   **Chi tiáº¿t PPR trong `graph.py`:**
   ```python
   def run_personalized_pagerank(self, reset_prob, damping=0.5):
       # Clean reset probabilities
       reset_prob = np.where(np.isnan(reset_prob) | (reset_prob < 0), 0, reset_prob)
       
       # Run PageRank vá»›i personalized reset
       pagerank_scores = self.graph.personalized_pagerank(
           vertices=range(len(self.node_name_to_vertex_idx)),
           damping=damping,
           directed=False,
           weights='weight',
           reset=reset_prob,
           implementation='prpack'
       )
       
       # Extract scores cho document nodes only
       doc_scores = np.array([pagerank_scores[idx] for idx in self.passage_node_idxs])
       sorted_doc_ids = np.argsort(doc_scores)[::-1]
       sorted_doc_scores = doc_scores[sorted_doc_ids.tolist()]
       
       return sorted_doc_ids, sorted_doc_scores
   ```

#### 3.2.4 BÆ°á»›c 4: Extract Top Documents

```python
top_docs = []
for doc_id in sorted_doc_ids[:num_to_retrieve]:
    if doc_id < len(self.knowledge_graph.passage_node_keys):
        passage_key = self.knowledge_graph.passage_node_keys[doc_id]
        doc_content = self.chunk_store.get_row(passage_key).get("content", "")
        top_docs.append(doc_content)

results.append(QuerySolution(
    question=query,
    docs=top_docs,
    doc_scores=sorted_doc_scores[:num_to_retrieve].tolist()
))
```

---

## ðŸ¤– PHASE 4: QUESTION ANSWERING

### 4.1 QA Pipeline

```python
qa_solutions, response_messages, metadata = retriever.qa(query_solutions)
```

**Chi tiáº¿t trong `retrieval.py`:**

#### 4.1.1 Prepare QA Prompts

```python
all_qa_messages = []

for query_solution in query_solutions:
    retrieved_passages = query_solution.docs[:self.config.qa_top_k]
    
    prompt_user = ''
    for passage in retrieved_passages:
        prompt_user += f'Document: {passage}\n\n'
    prompt_user += f'Question: {query_solution.question}\nAnswer: '
    
    messages = [
        {"role": "system", "content": "You are a helpful assistant that answers questions based on the provided documents."},
        {"role": "user", "content": prompt_user}
    ]
    all_qa_messages.append(messages)
```

**Chi tiáº¿t Prompt Structure:**
```
System: You are a helpful assistant that answers questions based on the provided documents.

User: Document: Artificial intelligence is transforming the way we work and live. Machine learning algorithms can analyze vast amounts of data.

Document: Natural language processing enables computers to understand human language. Deep learning models have achieved remarkable results.

Document: Computer vision allows machines to interpret visual information. Neural networks are inspired by the human brain structure.

Question: What is artificial intelligence?
Answer: 
```

#### 4.1.2 Run LLM Inference

```python
all_qa_results = []
for qa_messages in all_qa_messages:
    try:
        result = self.llm_model.infer(qa_messages)
        all_qa_results.append(result)
    except Exception as e:
        logger.error(f"Error in QA inference: {e}")
        all_qa_results.append(("Error occurred", {}, False))

all_response_messages, all_metadata, all_cache_hits = zip(*all_qa_results)
```

#### 4.1.3 Extract Answers

```python
updated_solutions = []
for i, query_solution in enumerate(query_solutions):
    response = all_response_messages[i]
    
    # Extract answer tá»« response
    try:
        if "Answer:" in response:
            answer = response.split("Answer:")[-1].strip()
        else:
            answer = response.strip()
    except Exception as e:
        logger.warning(f"Error extracting answer: {e}")
        answer = response
    
    query_solution.answer = answer
    updated_solutions.append(query_solution)
```

### 4.2 Output Format

**Final QuerySolution:**
```python
QuerySolution(
    question="What is artificial intelligence?",
    docs=[
        "Artificial intelligence is transforming the way we work and live...",
        "Natural language processing enables computers to understand...",
        "Computer vision allows machines to interpret visual information..."
    ],
    doc_scores=[0.95, 0.87, 0.82],
    answer="Based on the provided documents, artificial intelligence is a technology that transforms the way we work and live by enabling machines to understand language, analyze data, and interpret visual information through various techniques like machine learning, natural language processing, and computer vision.",
    gold_answers=None,  # Náº¿u cÃ³ ground truth
    gold_docs=None      # Náº¿u cÃ³ ground truth documents
)
```

---

## ðŸ“Š PERFORMANCE TRACKING

### Timing Metrics

```python
# ÄÆ°á»£c track trong retriever
retriever.total_retrieval_time  # Tá»•ng thá»i gian retrieval
retriever.ppr_time             # Thá»i gian cháº¡y PageRank
retriever.rerank_time          # Thá»i gian rerank facts

logger.info(f"Total Retrieval Time: {retriever.total_retrieval_time:.2f}s")
logger.info(f"Total Recognition Memory Time: {retriever.rerank_time:.2f}s") 
logger.info(f"Total PPR Time: {retriever.ppr_time:.2f}s")
```

### Graph Statistics

```python
graph_info = knowledge_graph.get_graph_info()

GraphInfo(
    num_phrase_nodes=1250,           # Sá»‘ entity nodes
    num_passage_nodes=100,           # Sá»‘ passage nodes  
    num_total_nodes=1350,            # Tá»•ng nodes
    num_extracted_triples=890,       # Triples tá»« OpenIE
    num_triples_with_passage_node=100, # Edges passage-entity
    num_synonymy_triples=245,        # Synonymy edges
    num_total_triples=1235           # Tá»•ng edges
)
```

---

## ðŸ”§ ADVANCED FEATURES

### 1. Document Deletion

```python
success = indexer.delete_documents(documents_to_delete)
```

**Chi tiáº¿t process:**
1. Get chunk IDs Ä‘á»ƒ delete
2. Find entities vÃ  facts chá»‰ tá»“n táº¡i trong deleted chunks
3. Remove tá»« all stores (chunk, entity, fact)
4. Remove vertices tá»« graph
5. Update OpenIE results
6. Save changes

### 2. Incremental Indexing

- Há»‡ thá»‘ng tá»± Ä‘á»™ng detect documents Ä‘Ã£ Ä‘Æ°á»£c index
- Chá»‰ process documents má»›i
- Merge results vá»›i existing data
- Efficient cho large-scale deployments

### 3. Fallback Mechanisms

- Náº¿u khÃ´ng cÃ³ facts relevant â†’ fallback vá» DPR
- Náº¿u OpenIE fails â†’ táº¡o empty results
- Náº¿u graph empty â†’ pure dense retrieval
- Error handling á»Ÿ má»i level

### 4. Caching Strategy

- Query embeddings Ä‘Æ°á»£c cache
- Precomputed embeddings cho fast retrieval
- Metadata persistence Ä‘á»ƒ avoid recomputation
- File-based storage cho scalability

---

## ðŸŽ¯ KEY ADVANTAGES

### 1. **Multi-hop Reasoning**
- Graph structure cho phÃ©p reasoning qua multiple hops
- Entity relationships Ä‘Æ°á»£c preserve
- Complex queries Ä‘Æ°á»£c handle tá»‘t hÆ¡n

### 2. **Semantic Understanding**
- OpenIE extracts structured knowledge
- Entity embeddings capture semantic similarity
- Fact-based retrieval more precise than keyword matching

### 3. **Personalized Ranking**
- PPR personalizes ranking based on query
- Combines multiple signals (facts + dense retrieval)
- Adaptive weighting based on query characteristics

### 4. **Modular Design**
- Each component cÃ³ thá»ƒ optimize riÃªng
- Easy to swap different models
- Scalable architecture

---

## ðŸš€ OPTIMIZATION OPPORTUNITIES

### 1. **Batch Processing**
- Batch OpenIE calls cho efficiency
- Vectorized operations cho embeddings
- Parallel processing cho independent operations

### 2. **Memory Management**
- Lazy loading cho large embeddings
- Disk-based storage cho scalability
- Memory mapping cho fast access

### 3. **Caching**
- Query result caching
- Intermediate computation caching
- Smart cache invalidation

### 4. **Hardware Acceleration**
- GPU acceleration cho embeddings
- Optimized graph libraries
- Distributed processing cho large datasets

---

ÄÃ¢y lÃ  workflow siÃªu chi tiáº¿t cá»§a HippoRAG modular implementation. Má»—i bÆ°á»›c Ä‘Æ°á»£c giáº£i thÃ­ch rÃµ rÃ ng vá»›i code examples vÃ  data structures cá»¥ thá»ƒ. 