# Model Abstractions System

H·ªá th·ªëng Model Abstractions cung c·∫•p l·ªõp tr·ª´u t∆∞·ª£ng th·ªëng nh·∫•t ƒë·ªÉ qu·∫£n l√Ω v√† s·ª≠ d·ª•ng c√°c model t·ª´ nhi·ªÅu framework kh√°c nhau (HuggingFace, ONNX, PyTorch, TensorFlow, YOLO, etc.).

## üéØ M·ª•c ti√™u

- **Unified Interface**: T·∫•t c·∫£ models d√πng chung 1 interface b·∫•t k·ªÉ framework
- **Framework Agnostic**: H·ªó tr·ª£ nhi·ªÅu framework m√† kh√¥ng c·∫ßn thay ƒë·ªïi code logic
- **Performance Optimization**: Auto convert sang ONNX ƒë·ªÉ tƒÉng t·ªëc inference
- **Smart Memory Management**: Qu·∫£n l√Ω RAM v√† disk space t·ª± ƒë·ªông
- **Easy Extensibility**: D·ªÖ d√†ng th√™m support cho framework m·ªõi

## üèóÔ∏è Architecture

### Core Components

1. **BaseModelWrapper**: L·ªõp tr·ª´u t∆∞·ª£ng c∆° b·∫£n cho t·∫•t c·∫£ model wrappers
2. **ModelRegistry**: Registry pattern ƒë·ªÉ qu·∫£n l√Ω c√°c wrapper classes
3. **ModelManager**: Qu·∫£n l√Ω lifecycle c·ªßa models (load/unload/caching)
4. **Framework-specific Wrappers**: Implement cho t·ª´ng framework c·ª• th·ªÉ
5. **ModelConverter**: Convert models gi·ªØa c√°c format kh√°c nhau

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ModelManager                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Memory Mgmt    ‚îÇ  ‚îÇ     Model Registry          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - RAM limits   ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Disk limits  ‚îÇ  ‚îÇ  ‚îÇ    HuggingFace         ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Auto cleanup ‚îÇ  ‚îÇ  ‚îÇ    ONNX                ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ    PyTorch             ‚îÇ ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ  ‚îÇ    TensorFlow          ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ    YOLO                ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Auto Convert   ‚îÇ  ‚îÇ  ‚îÇ    Custom Pipelines    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - To ONNX      ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Framework    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îÇ    Detection    ‚îÇ                                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                BaseModelWrapper                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Common Interface:                                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - load_model()                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - unload_model()                                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - predict(inputs)                                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - to_device(device)                                ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Usage Examples

### Basic Usage

```python
from server.model_abstractions import ModelManager, ModelType, ModelFramework

# Initialize Model Manager
manager = ModelManager(
    max_models_in_ram=5,
    max_models_in_disk=10,
    auto_convert_to_onnx=True  # Auto convert ƒë·ªÉ optimize performance
)

# Register models
manager.register_model(
    model_id="bert-base-uncased",
    model_type=ModelType.TEXT_CLASSIFICATION,
    framework=ModelFramework.HUGGINGFACE
)

# Use unified interface cho b·∫•t k·ª≥ model n√†o
result = manager.predict(
    model_id="bert-base-uncased",
    inputs={"text": "This is a great product!"},
    device="cuda:0"
)
```

### Working with Different Frameworks

```python
# HuggingFace Model
hf_result = manager.predict("bert-classifier", {"text": "Hello world"})

# ONNX Model (same interface!)
onnx_result = manager.predict("resnet50-onnx", {"image": image_data})

# YOLO Model (same interface!)
yolo_result = manager.predict("yolo-detector", {"image": image_data})

# Custom Tabular Model (same interface!)
tabular_result = manager.predict("fraud-detector", {"row": feature_dict})
```

### Auto Conversion to ONNX

```python
# Enable auto conversion
manager = ModelManager(auto_convert_to_onnx=True)

# Model s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông convert sang ONNX khi load
# -> Faster inference, smaller memory footprint
result = manager.predict("any-model-id", inputs)
```

## üì¶ Framework Support

### Supported Frameworks

| Framework | Models | Status | Performance | Notes |
|-----------|--------|--------|-------------|-------|
| **HuggingFace** | Text, Image, Audio, Video | ‚úÖ | Good | Native support |
| **ONNX** | All types | ‚úÖ | Excellent | Optimized runtime |
| **PyTorch** | Custom models | ‚úÖ | Good | Via conversion |
| **TensorFlow** | All types | ‚úÖ | Good | Via tf2onnx |
| **YOLO** | Object Detection | ‚úÖ | Excellent | Ultralytics |
| **scikit-learn** | Tabular | ‚úÖ | Good | Via skl2onnx |
| **XGBoost** | Tabular | üöß | Good | Planned |
| **LightGBM** | Tabular | üöß | Good | Planned |

### Model Types Supported

- **Text**: Classification, NER, Translation, Summarization, QA, Generation
- **Image**: Classification, Object Detection, Image-to-Text
- **Audio**: Classification, Speech Recognition
- **Video**: Classification
- **Tabular**: Classification, Regression

## üîß Adding New Framework Support

### Step 1: Create Wrapper Class

```python
from .base_model import BaseModelWrapper, ModelFramework, ModelType

class MyFrameworkWrapper(BaseModelWrapper):
    def __init__(self, model_path, model_type, device="cpu", **kwargs):
        super().__init__(
            model_path=model_path,
            model_type=model_type,
            framework=ModelFramework.MY_FRAMEWORK,
            device=device
        )
    
    def load_model(self):
        # Load model implementation
        pass
    
    def unload_model(self):
        # Unload model implementation
        pass
    
    def predict(self, inputs):
        # Prediction implementation
        pass
    
    def to_device(self, device):
        # Device transfer implementation
        pass
```

### Step 2: Register Wrapper

```python
from .base_model import ModelRegistry

# Register cho specific model types
ModelRegistry.register(
    ModelFramework.MY_FRAMEWORK, 
    ModelType.TEXT_CLASSIFICATION, 
    MyFrameworkWrapper
)
```

### Step 3: Add Converter (Optional)

```python
from .model_converter import ModelConverter

def convert_my_framework_to_onnx(model_path, output_path, model_type):
    # Conversion logic
    pass

# Add to AutoConverter
```

## ‚ö° Performance Optimizations

### ONNX Conversion Benefits

- **Faster Inference**: 2-5x speedup compared to original frameworks
- **Smaller Memory**: Optimized model representation
- **Cross-platform**: Run anywhere with ONNX Runtime
- **Hardware Acceleration**: GPU, NPU, specialized chips

### Memory Management

```python
# Configure memory limits
manager = ModelManager(
    max_models_in_ram=5,      # Limit loaded models
    max_models_in_disk=20,    # Limit cached models
    auto_convert_to_onnx=True # Use optimized format
)

# Models t·ª± ƒë·ªông load/unload theo LRU policy
```

### Caching Strategy

1. **RAM Cache**: Keep frequently used models in memory
2. **Disk Cache**: Store downloaded models locally
3. **LRU Eviction**: Remove oldest unused models when limits reached
4. **Smart Cleanup**: Prioritize models not in RAM for disk cleanup

## üõ†Ô∏è Configuration

### Environment Variables

```bash
# Model server settings
export MAX_MODELS_IN_RAM=5
export MAX_MODELS_IN_DISK=10
export MODELS_DIRECTORY="/path/to/models"
export AUTO_CONVERT_TO_ONNX=true
export DEFAULT_DEVICE="cuda:0"
```

### Manager Configuration

```python
manager = ModelManager(
    max_models_in_ram=5,
    max_models_in_disk=10,
    models_directory="./models",
    auto_convert_to_onnx=True
)
```

## üìä Monitoring & Logging

### Model Status

```python
# List all models
models = manager.list_models()
for model in models:
    print(f"{model['model_id']}: {model['is_loaded']}")

# Get detailed info
info = manager.get_model_info("model-id")
print(f"Framework: {info['framework']}")
print(f"Device: {info['device']}")
print(f"Last used: {info['lasted_used']}")
```

### Performance Metrics

- Model load/unload times
- Inference latency
- Memory usage
- Cache hit rates
- Conversion success rates

## üîÑ Migration from Old System

### Step-by-Step Migration

1. **Install Dependencies**:
   ```bash
   pip install onnxruntime transformers torch tensorflow
   ```

2. **Update Imports**:
   ```python
   # Old
   from server.api_router import get_pipe
   
   # New
   from server.model_abstractions import ModelManager
   ```

3. **Replace Logic**:
   ```python
   # Old
   pipe = get_pipe(model_id)
   result = pipe(inputs)
   
   # New
   result = manager.predict(model_id, inputs, device)
   ```

4. **Update Configuration**:
   - Move model configs to new format
   - Set memory limits
   - Enable ONNX conversion

### Backward Compatibility

- API endpoints remain the same
- Response format unchanged
- Gradual migration possible

## üîç Troubleshooting

### Common Issues

1. **Model Not Found**: Check if model is registered
2. **Memory Issues**: Reduce `max_models_in_ram`
3. **Conversion Fails**: Check framework compatibility
4. **Slow Loading**: Enable ONNX conversion
5. **Device Errors**: Verify CUDA availability

### Debug Mode

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Detailed logs for debugging
manager = ModelManager(debug=True)
```

## üöß Future Roadmap

### Planned Features

- [ ] **Multi-GPU Support**: Distribute models across GPUs
- [ ] **Model Quantization**: INT8/FP16 optimization
- [ ] **Batch Inference**: Process multiple requests together
- [ ] **Model Versioning**: Support multiple versions
- [ ] **A/B Testing**: Compare model performance
- [ ] **Distributed Inference**: Across multiple nodes
- [ ] **Model Monitoring**: Performance tracking
- [ ] **Auto Scaling**: Based on load

### Framework Expansion

- [ ] **XGBoost/LightGBM**: Gradient boosting models
- [ ] **MLflow**: Model registry integration
- [ ] **TensorRT**: NVIDIA optimization
- [ ] **CoreML**: Apple devices
- [ ] **TensorFlow Lite**: Mobile deployment

## üìù Contributing

1. Fork the repository
2. Create feature branch
3. Add new framework wrapper
4. Write tests
5. Update documentation
6. Submit pull request

### Guidelines

- Follow existing patterns
- Add comprehensive tests
- Document new features
- Maintain backward compatibility

---

**T√≥m l·∫°i**: H·ªá th·ªëng Model Abstractions m·ªõi cung c·∫•p interface th·ªëng nh·∫•t, t·ªëi ∆∞u h√≥a performance th√¥ng qua ONNX, v√† qu·∫£n l√Ω memory th√¥ng minh - gi√∫p scale t·ª´ development ƒë·∫øn production m·ªôt c√°ch d·ªÖ d√†ng. 