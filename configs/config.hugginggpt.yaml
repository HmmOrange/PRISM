num_candidate_models: 5
max_description_length: 100
logit_bias:
  parse_task: 0.1
  choose_model: 5
tprompt:
  parse_task: >-
    #1 Task Planning Stage: The AI assistant can parse user input to several tasks: [{"task": task, "id": task_id, "dep": dependency_task_id, "args": {"text": text or <GENERATED>-dep_id, "image": image_url or <GENERATED>-dep_id, "audio": audio_url or <GENERATED>-dep_id}}]. The special tag "<GENERATED>-dep_id" refer to the one generated text/image/audio in the dependency task (Please consider whether the dependency task generates resources of this type.) and "dep_id" must be in "dep" list. The "dep" field denotes the ids of the previous prerequisite tasks which generate a new resource that the current task relies on. The "args" field must in ["text", "image", "audio", "video"], nothing else. The task MUST be selected from the following options: "token-classification", "text-classification", "zero-shot-classification", "translation", "summarization", "question-answering", "text-generation", "sentence-similarity", "tabular-classification", "tabular-regression", "object-detection", "image-classification", "image-to-text", "automatic-speech-recognition", "audio-classification", "video-classification". There may be multiple tasks of the same type. Think step by step about all the tasks needed to resolve the user's request. Parse out as few tasks as possible while ensuring that the user request can be resolved. Pay attention to the dependencies and order among tasks. If the user input can't be parsed, you need to reply empty JSON []. 
  choose_model: >-
    #2 Model Selection Stage: Given the user request and the parsed tasks, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The assistant should focus more on the description of the model and find the model that has the most potential to solve requests and tasks. Also, prefer models with local inference endpoints for speed and stability.
  response_results: >-
    #4 Response Generation Stage (Prediction-Only): At this stage, the AI assistant must output only the final prediction extracted directly from the executed tasks and inference results. The assistant is strictly forbidden from generating or inventing new content beyond what is present in the process logs. The response must not contain explanations, workflow descriptions, or any extra text. The output must always be wrapped inside <prediction> and </prediction> tags. If multiple items are required, the assistant must use exactly the separators specified by the task (such as commas, semicolons, or newlines). If no valid prediction can be extracted from the logs, the assistant must return an empty prediction tag <prediction></prediction>.

demos_or_presteps:
  parse_task: templates/demos/demo_parse_task.json
  choose_model: templates/demos/demo_choose_model.json
  response_results: templates/demos/demo_response_results.json 
prompt:
  parse_task: The chat log [ {{context}} ] may contain the resources I mentioned. Now I input { {{input}} } and data { {{data}} }. Pay attention to the input and output types of tasks and the dependencies between tasks.
  choose_model: >-
    Please choose the most suitable model from {{metas}} for the task {{task}}. The output must be in a strict JSON format: {"id": "id", "reason": "your detail reasons for the choice"}.
  response_results: >-
    You are an AI assistant producing the final answer only by extracting it from the execution logs and inference results. Input: {{input}}. You must output only the final prediction that can be directly found in the logs. Do not generate, infer, or imagine any content beyond what the logs contain. Do not provide explanations, reasoning, or any additional text. Do not include markdown, tables, or IDs. Ensure the prediction is precise and exactly follows the taskâ€™s expected format. If multiple items are required, use only the separators specified by the task. If no valid prediction can be extracted, return an empty prediction tag. The final answer must always be enclosed in <prediction>...</prediction>.