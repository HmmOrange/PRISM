{"id": "google/vit-base-patch16-224", "pipeline_tag": "image-classification", "tags": ["transformers", "pytorch", "tf", "jax", "safetensors", "vit", "image-classification", "vision", "dataset:imagenet-1k", "dataset:imagenet-21k", "arxiv:2010.11929", "arxiv:2006.03677", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-1k\n- imagenet-21k\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n---\n\n# Vision Transformer (base-sized model) \n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. \n\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/transformers/model_doc/vit.html#).\n\n## Training data\n\nThe ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes, and fine-tuned on [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/), a dataset consisting of 1 million images and 1k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). \n\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n\n### Pretraining\n\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Training resolution is 224.\n\n## Evaluation results\n\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```", "downloads": 3794431, "likes": 829, "meta": {"datasets": ["imagenet-1k", "imagenet-21k"], "license": "apache-2.0", "tags": ["vision", "image-classification"], "widget": [{"src": "https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg", "example_title": "Tiger"}, {"src": "https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg", "example_title": "Teapot"}, {"src": "https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg", "example_title": "Palace"}]}, "inference_type": "huggingface"}
{"id": "Ahmed9275/Vit-Cifar100", "pipeline_tag": "image-classification", "tags": ["transformers", "pytorch", "tensorboard", "vit", "image-classification", "generated_from_trainer", "dataset:cifar100", "license:apache-2.0", "model-index", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\ntags:\n- image-classification\n- generated_from_trainer\ndatasets:\n- cifar100\nmetrics:\n- accuracy\nmodel-index:\n- name: vit-base-beans-demo-v5\n  results:\n  - task:\n      type: image-classification\n      name: Image Classification\n    dataset:\n      name: Cifar100\n      type: cifar100\n      args: cifar100\n    metrics:\n    - type: accuracy\n      value: 0.8985\n      name: Accuracy\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# vit-base-beans-demo-v5\n\nThis model is a fine-tuned version of [google/vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k) on the Cifar100 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4420\n- Accuracy: 0.8985\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0002\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Accuracy |\n|:-------------:|:-----:|:-----:|:---------------:|:--------:|\n| 1.08          | 1.0   | 3125  | 0.6196          | 0.8262   |\n| 0.3816        | 2.0   | 6250  | 0.5322          | 0.8555   |\n| 0.1619        | 3.0   | 9375  | 0.4817          | 0.8765   |\n| 0.0443        | 4.0   | 12500 | 0.4420          | 0.8985   |\n\n\n### Framework versions\n\n- Transformers 4.19.2\n- Pytorch 1.11.0+cu113\n- Datasets 2.2.1\n- Tokenizers 0.12.1\n", "downloads": 669, "likes": 4, "meta": {"datasets": ["cifar100"], "license": "apache-2.0", "metrics": ["accuracy"], "tags": ["image-classification", "generated_from_trainer"], "model-index": [{"name": "vit-base-beans-demo-v5", "results": [{"task": {"type": "image-classification", "name": "Image Classification"}, "dataset": {"name": "Cifar100", "type": "cifar100", "args": "cifar100"}, "metrics": [{"type": "accuracy", "value": 0.8985, "name": "Accuracy", "verified": false}]}]}]}, "inference_type": "local"}
{"id": "nateraw/vit-base-patch16-224-cifar10", "pipeline_tag": "image-classification", "tags": ["transformers", "pytorch", "vit", "image-classification", "vision", "dataset:cifar10", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\ntags:\n- image-classification\n- vision\n- pytorch\nlicense: apache-2.0\ndatasets:\n- cifar10\nmetrics:\n- accuracy\nthumbnail: https://avatars3.githubusercontent.com/u/32437151?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4\n---\n\n# Vision Transformer Fine Tuned on CIFAR10\n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) and **fine-tuned on CIFAR10** at resolution 224x224.\n\nCheck out the code at my [my Github repo](https://github.com/nateraw/huggingface-vit-finetune).\n\n## Usage\n\n```python\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog10.png'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('nateraw/vit-base-patch16-224-cifar10')\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-base-patch16-224-cifar10')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\npreds = outputs.logits.argmax(dim=1)\n\nclasses = [\n    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n]\nclasses[preds[0]]\n```\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. \nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\nNote that this model does not provide any fine-tuned heads, as these were zero'd by Google researchers. However, the model does include the pre-trained pooler, which can be used for downstream tasks (such as image classification).\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n", "downloads": 4207, "likes": 10, "meta": {"datasets": ["cifar10"], "license": "apache-2.0", "metrics": ["accuracy"], "tags": ["image-classification", "vision", "pytorch"], "thumbnail": "https://avatars3.githubusercontent.com/u/32437151?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4"}, "inference_type": "local"}
{"id": "farleyknight/mnist-digit-classification-2022-09-04", "pipeline_tag": "image-classification", "tags": ["transformers", "pytorch", "vit", "image-classification", "vision", "generated_from_trainer", "dataset:mnist", "license:apache-2.0", "model-index", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\ntags:\n- image-classification\n- vision\n- generated_from_trainer\ndatasets:\n- mnist\nmetrics:\n- accuracy\nmodel-index:\n- name: mnist-digit-classification-2022-09-04\n  results:\n  - task:\n      type: image-classification\n      name: Image Classification\n    dataset:\n      name: mnist\n      type: mnist\n      config: mnist\n      split: train\n      args: mnist\n    metrics:\n    - type: accuracy\n      value: 0.9923333333333333\n      name: Accuracy\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# mnist-digit-classification-2022-09-04\n\nThis model is a fine-tuned version of [google/vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k) on the mnist dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0319\n- Accuracy: 0.9923\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5.0\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.22.0.dev0\n- Pytorch 1.12.1+cu102\n- Datasets 2.4.0\n- Tokenizers 0.12.1\n", "downloads": 726, "likes": 0, "meta": {"datasets": ["mnist"], "license": "apache-2.0", "metrics": ["accuracy"], "tags": ["image-classification", "vision", "generated_from_trainer"], "model-index": [{"name": "mnist-digit-classification-2022-09-04", "results": [{"task": {"type": "image-classification", "name": "Image Classification"}, "dataset": {"name": "mnist", "type": "mnist", "config": "mnist", "split": "train", "args": "mnist"}, "metrics": [{"type": "accuracy", "value": 0.9923333333333333, "name": "Accuracy", "verified": false}]}]}]}, "inference_type": "local"}
{"id": "prithivMLmods/Mnist-Digits-SigLIP2", "pipeline_tag": "image-classification", "tags": ["transformers", "safetensors", "siglip", "image-classification", "Digits", "Mnist", "SigLIP2", "0-t0-9", "Number-Classification", "en", "dataset:ylecun/mnist", "base_model:google/siglip2-base-patch16-224", "base_model:finetune:google/siglip2-base-patch16-224", "doi:10.57967/hf/5279", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\ndatasets:\n- ylecun/mnist\nlanguage:\n- en\nbase_model:\n- google/siglip2-base-patch16-224\npipeline_tag: image-classification\nlibrary_name: transformers\ntags:\n- Digits\n- Mnist\n- SigLIP2\n- 0-t0-9\n- Number-Classification\n---\n\n![fQPjrpOKabPgt_9vCH4Qj.png](https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/rB4X4q0YZkX0WJW6fZ83F.png)\n![ssdsdsdfsdfcsdfc.png](https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/cyqhEw4goojpJ2shwDdEb.png)\n\n# **Mnist-Digits-SigLIP2**  \n\n> **Mnist-Digits-SigLIP2** is an image classification model fine-tuned from **google/siglip2-base-patch16-224** to classify handwritten digits (0-9) using the **SiglipForImageClassification** architecture. It is trained on the MNIST dataset for accurate digit recognition.  \n\n```py\nClassification Report:\n              precision    recall  f1-score   support\n\n           0     0.9988    0.9959    0.9974      5923\n           1     0.9987    0.9918    0.9952      6742\n           2     0.9918    0.9943    0.9930      5958\n           3     0.9975    0.9938    0.9957      6131\n           4     0.9892    0.9882    0.9887      5842\n           5     0.9859    0.9937    0.9898      5421\n           6     0.9936    0.9939    0.9937      5918\n           7     0.9856    0.9943    0.9899      6265\n           8     0.9932    0.9921    0.9926      5851\n           9     0.9926    0.9897    0.9912      5949\n\n    accuracy                         0.9928     60000\n   macro avg     0.9927    0.9928    0.9927     60000\nweighted avg     0.9928    0.9928    0.9928     60000\n```\n\n![download (2).png](https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/qUaioZfL840_BrRhReCqd.png)\n\n### **Classes:**  \n- **Class 0:** \"0\"  \n- **Class 1:** \"1\"  \n- **Class 2:** \"2\"  \n- **Class 3:** \"3\"  \n- **Class 4:** \"4\"  \n- **Class 5:** \"5\"  \n- **Class 6:** \"6\"  \n- **Class 7:** \"7\"  \n- **Class 8:** \"8\"  \n- **Class 9:** \"9\"  \n\n---\n\n# **Run with TransformersðŸ¤—**  \n\n```python\n!pip install -q transformers torch pillow gradio\n```  \n\n```python\nimport gradio as gr\nfrom transformers import AutoImageProcessor, SiglipForImageClassification\nfrom transformers.image_utils import load_image\nfrom PIL import Image\nimport torch\n\n# Load model and processor\nmodel_name = \"prithivMLmods/Mnist-Digits-SigLIP2\"\nmodel = SiglipForImageClassification.from_pretrained(model_name)\nprocessor = AutoImageProcessor.from_pretrained(model_name)\n\ndef classify_digit(image):\n    \"\"\"Predicts the digit in the given handwritten digit image.\"\"\"\n    image = Image.fromarray(image).convert(\"RGB\")\n    inputs = processor(images=image, return_tensors=\"pt\")\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        probs = torch.nn.functional.softmax(logits, dim=1).squeeze().tolist()\n    \n    labels = {\n        \"0\": \"0\", \"1\": \"1\", \"2\": \"2\", \"3\": \"3\", \"4\": \"4\",\n        \"5\": \"5\", \"6\": \"6\", \"7\": \"7\", \"8\": \"8\", \"9\": \"9\"\n    }\n    predictions = {labels[str(i)]: round(probs[i], 3) for i in range(len(probs))}\n    \n    return predictions\n\n# Create Gradio interface\niface = gr.Interface(\n    fn=classify_digit,\n    inputs=gr.Image(type=\"numpy\"),\n    outputs=gr.Label(label=\"Prediction Scores\"),\n    title=\"MNIST Digit Classification ðŸ”¢\",\n    description=\"Upload a handwritten digit image (0-9) to recognize it using MNIST-Digits-SigLIP2.\"\n)\n\n# Launch the app\nif __name__ == \"__main__\":\n    iface.launch()\n```\n\n---\n\n# **Sample Inference**  \n\n![Screenshot 2025-03-28 at 23-23-02 MNIST Digit Classification ðŸ”¢.png](https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/o0YinTlr6or3V_wOJMCf3.png)\n![Screenshot 2025-03-28 at 23-25-22 MNIST Digit Classification ðŸ”¢.png](https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/LP4upkfHfUa3wdRSSS9tp.png)\n![Screenshot 2025-03-28 at 23-25-52 MNIST Digit Classification ðŸ”¢.png](https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/XJ0AmEg0Com-KN32jtGDu.png)\n![Screenshot 2025-03-28 at 23-26-52 MNIST Digit Classification ðŸ”¢.png](https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/rboO-rw7BxK7S8vJMF-To.png)\n\n# **Intended Use:**  \n\nThe **Mnist-Digits-SigLIP2** model is designed for handwritten digit recognition. Potential applications include:  \n\n- **Optical Character Recognition (OCR):** Digit recognition for various documents.  \n- **Banking & Finance:** Automated check processing.  \n- **Education & Learning:** AI-powered handwriting assessment.  \n- **Embedded Systems:** Handwriting input in smart devices. ", "downloads": 2011, "likes": 2, "meta": {"base_model": ["google/siglip2-base-patch16-224"], "datasets": ["ylecun/mnist"], "language": ["en"], "library_name": "transformers", "license": "apache-2.0", "pipeline_tag": "image-classification", "tags": ["Digits", "Mnist", "SigLIP2", "0-t0-9", "Number-Classification"]}, "inference_type": "local"}
{"id": "nateraw/food", "pipeline_tag": "image-classification", "tags": ["transformers", "pytorch", "tensorboard", "vit", "image-classification", "generated_from_trainer", "dataset:food101", "license:apache-2.0", "model-index", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\n- image-classification\n- pytorch\ndatasets:\n- food101\nmetrics:\n- accuracy\nmodel-index:\n- name: food101_outputs\n  results:\n  - task:\n      type: image-classification\n      name: Image Classification\n    dataset:\n      name: food-101\n      type: food101\n      args: default\n    metrics:\n    - type: accuracy\n      value: 0.8912871287128713\n      name: Accuracy\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# nateraw/food\n\nThis model is a fine-tuned version of [google/vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k) on the nateraw/food101 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4501\n- Accuracy: 0.8913\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0002\n- train_batch_size: 128\n- eval_batch_size: 128\n- seed: 1337\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|\n| 0.8271        | 1.0   | 592  | 0.6070          | 0.8562   |\n| 0.4376        | 2.0   | 1184 | 0.4947          | 0.8691   |\n| 0.2089        | 3.0   | 1776 | 0.4876          | 0.8747   |\n| 0.0882        | 4.0   | 2368 | 0.4639          | 0.8857   |\n| 0.0452        | 5.0   | 2960 | 0.4501          | 0.8913   |\n\n\n### Framework versions\n\n- Transformers 4.9.0.dev0\n- Pytorch 1.9.0+cu102\n- Datasets 1.9.1.dev0\n- Tokenizers 0.10.3\n", "downloads": 4041, "likes": 65, "meta": {"datasets": ["food101"], "license": "apache-2.0", "metrics": ["accuracy"], "tags": ["generated_from_trainer", "image-classification", "pytorch"], "model-index": [{"name": "food101_outputs", "results": [{"task": {"type": "image-classification", "name": "Image Classification"}, "dataset": {"name": "food-101", "type": "food101", "args": "default"}, "metrics": [{"type": "accuracy", "value": 0.8912871287128713, "name": "Accuracy", "verified": false}]}]}]}, "inference_type": "huggingface"}
{"id": "wesleyacheng/dog-breeds-multiclass-image-classification-with-vit", "pipeline_tag": "image-classification", "tags": ["transformers", "pytorch", "safetensors", "vit", "image-classification", "arxiv:2010.11929", "license:mit", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: mit\nmetrics:\n- accuracy\n- f1\npipeline_tag: image-classification\nwidget:\n- src: https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Welchcorgipembroke.JPG/1200px-Welchcorgipembroke.JPG\n  example_title: Pembroke Corgi\n- src: https://upload.wikimedia.org/wikipedia/commons/d/df/Shihtzu_%28cropped%29.jpg\n  example_title: Shih Tzu\n- src: https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\n  example_title: Beagle\n---\n\nModel made by notebook first posted in my [Kaggle](https://www.kaggle.com/wesleyacheng/dog-breeds-multiclass-image-classification-w-vit).\n\n# Model Motivation\n\nRecently, someone asked me if you can classify dog images into their respective dog breeds instead just differentiating from cats vs dogs like my last [notebook](https://www.kaggle.com/code/wesleyacheng/cat-vs-dog-image-classification-with-cnns). I say **YES**!\n\nDue to the complexity of the problem, we will be using the most advanced computer vision architecture released in the [2020 Google paper](https://arxiv.org/pdf/2010.11929v2.pdf), the [**Vision Transformer**](https://paperswithcode.com/methods/category/vision-transformer).\n\nThe difference between the **Vision Transformer** and the traditional **Convolutional Neural Network (CNN)** is how it treats an image. In **Vision Transformers**, we take the input as a patch of the original image, say 16 x 16, and feed in into the Transformer as a sequence with positional embeddings and self-attention, while in the **Convolutional Neural Network (CNN)**, we use the same patch of original image as an input, but use convolutions and pooling layers as inductive biases. What this means is that **Vision Transformer** can use it's judgement to attend any particular patch of the image in a *global* fashion using it's self-attention mechanism without having us to guide the neural network like a **CNN** with *local* centering/cropping/bounding box our images to help its convolutions. \n\nThis allows the **Vision Transformer** architecture to be more flexible and scalable in nature, allowing us to create [foundation models](https://blogs.nvidia.com/blog/2023/03/13/what-are-foundation-models) in computer vision, similar to the NLP foundational models like [BERT](https://paperswithcode.com/method/bert) and [GPT](https://paperswithcode.com/method/gpt), with pre-training self-supervised/supervised on massive amount of image data that would generalize to different computer vision tasks such as *image classification, recognition, segmentation, etc.* This cross-pollination helps us move closer towards the goal of Artificial General Intelligence.\n\nOne thing about **Vision Transformers** are it has weaker inductive biases compared to **Convolutional Neural Networks** that enables it's scalability and flexibility. This feature/bug depending on who you ask will require most well-performing pre-trained models to require more data despite having less parameters compared to it's CNN counterparts.\n\nLuckily, in this model, we will use a **Vision Transformer** from [Google hosted at HuggingFace](https://huggingface.co/google/vit-base-patch16-224-in21k) pre-trained on the [ImageNet-21k dataset](https://paperswithcode.com/paper/imagenet-21k-pretraining-for-the-masses) (14 million images, 21k classes) with 16x16 patches, 224x224 resolution to bypass that data limitation. We will be fine-tuning this model to our \"small\" dog breeds dataset of around 20 thousand images from the [Stanford Dogs dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) imported by Jessica Li into [Kaggle](https://www.kaggle.com/datasets/jessicali9530/stanford-dogs-dataset) to classify dog images into 120 types of dog breeds!\n\n# Model Description\nThis model is finetuned using the [Google Vision Transformer (vit-base-patch16-224-in21k)](https://huggingface.co/google/vit-base-patch16-224-in21k) on the [Stanford Dogs dataset in Kaggle](https://www.kaggle.com/datasets/jessicali9530/stanford-dogs-dataset) to classify dog images into 120 types of dog breeds.\n\n# Intended Uses & Limitations\nYou can use this finetuned model to classify images of dogs only and dog breeds that are in the dataset.\n\n# How to Use\n```python\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\nimport PIL\nimport requests\n\nurl = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\nimage = PIL.Image.open(requests.get(url, stream=True).raw)\n\nimage_processor = AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\nmodel = AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 120 Stanford dog breeds classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\n# Model Training Metrics\n| Epoch | Top-1 Accuracy |  Top-3 Accuracy | Top-5 Accuracy | Macro F1 |\n|-------|----------------|-----------------|----------------|----------|\n| 1     | 79.8%          | 95.1%           | 97.5%          | 77.2%    |\n| 2     | 83.8%          | 96.7%           | 98.2%          | 81.9%    |\n| 3     | 84.8%          | 96.7%           | 98.3%          | 83.4%    |\n\n# Model Evaluation Metrics\n| Top-1 Accuracy | Top-3 Accuracy  | Top-5 Accuracy | Macro F1 |\n|----------------|-----------------|----------------|----------|\n| 84.0%          | 97.1%           | 98.7%          | 83.0%    |", "downloads": 522, "likes": 4, "meta": {"license": "mit", "metrics": ["accuracy", "f1"], "pipeline_tag": "image-classification", "widget": [{"src": "https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Welchcorgipembroke.JPG/1200px-Welchcorgipembroke.JPG", "example_title": "Pembroke Corgi"}, {"src": "https://upload.wikimedia.org/wikipedia/commons/d/df/Shihtzu_%28cropped%29.jpg", "example_title": "Shih Tzu"}, {"src": "https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg", "example_title": "Beagle"}]}, "inference_type": "huggingface"}
{"id": "playrobin/furniture-styles", "pipeline_tag": "image-classification", "tags": ["transformers", "pytorch", "tensorboard", "safetensors", "vit", "image-classification", "huggingpics", "model-index", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "", "downloads": 30, "likes": 7, "meta": {"metrics": ["accuracy"], "tags": ["image-classification", "pytorch", "huggingpics"]}, "inference_type": "huggingface"}
{"id": "dima806/facial_emotions_image_detection", "pipeline_tag": "image-classification", "tags": ["transformers", "pytorch", "safetensors", "vit", "image-classification", "base_model:google/vit-base-patch16-224-in21k", "base_model:finetune:google/vit-base-patch16-224-in21k", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\nmetrics:\n- accuracy\n- f1\nbase_model:\n- google/vit-base-patch16-224-in21k\n---\nReturns facial emotion with about 91% accuracy based on facial human image.\n\nSee https://www.kaggle.com/code/dima806/facial-emotions-image-detection-vit for more details.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6449300e3adf50d864095b90/dr6xp-8bjXk0TqXfJaBDn.png)\n\n```\nClassification report:\n\n              precision    recall  f1-score   support\n\n         sad     0.8394    0.8632    0.8511      3596\n     disgust     0.9909    1.0000    0.9954      3596\n       angry     0.9022    0.9035    0.9028      3595\n     neutral     0.8752    0.8626    0.8689      3595\n        fear     0.8788    0.8532    0.8658      3596\n    surprise     0.9476    0.9449    0.9463      3596\n       happy     0.9302    0.9372    0.9336      3596\n\n    accuracy                         0.9092     25170\n   macro avg     0.9092    0.9092    0.9091     25170\nweighted avg     0.9092    0.9092    0.9091     25170\n```", "downloads": 15651, "likes": 90, "meta": {"base_model": ["google/vit-base-patch16-224-in21k"], "license": "apache-2.0", "metrics": ["accuracy", "f1"]}, "inference_type": "huggingface"}
{"id": "semihdervis/cat-emotion-classifier", "pipeline_tag": "image-classification", "tags": ["transformers", "safetensors", "vit", "image-classification", "generated_from_trainer", "dataset:imagefolder", "base_model:google/vit-base-patch16-224-in21k", "base_model:finetune:google/vit-base-patch16-224-in21k", "license:apache-2.0", "model-index", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlibrary_name: transformers\nlicense: apache-2.0\nbase_model: google/vit-base-patch16-224-in21k\ntags:\n- image-classification\n- generated_from_trainer\ndatasets:\n- imagefolder\nmetrics:\n- accuracy\nmodel-index:\n- name: vit-base-cat-emotions\n  results:\n  - task:\n      type: image-classification\n      name: Image Classification\n    dataset:\n      name: custom dataset\n      type: imagefolder\n      config: default\n      split: validation\n      args: default\n    metrics:\n    - type: accuracy\n      value: 0.6352941176470588\n      name: Accuracy\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# vit-base-cat-emotions\n\nYou can try out the model live [here](https://cat-emotion-classifier.streamlit.app/), and check out the [GitHub repository](https://github.com/semihdervis/cat-emotion-classifier) for more details.\n\nThis model is a fine-tuned version of [google/vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k) on the custom dataset dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.0160\n- Accuracy: 0.6353\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0002\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|\n| 0.3361        | 3.125 | 100  | 1.0125          | 0.6548   |\n| 0.0723        | 6.25  | 200  | 0.9043          | 0.7381   |\n| 0.0321        | 9.375 | 300  | 0.9268          | 0.7143   |\n\n\n### Framework versions\n\n- Transformers 4.44.1\n- Pytorch 2.2.2+cu118\n- Datasets 2.20.0\n- Tokenizers 0.19.1\n", "downloads": 91, "likes": 2, "meta": {"base_model": "google/vit-base-patch16-224-in21k", "datasets": ["imagefolder"], "library_name": "transformers", "license": "apache-2.0", "metrics": ["accuracy"], "tags": ["image-classification", "generated_from_trainer"], "model-index": [{"name": "vit-base-cat-emotions", "results": [{"task": {"type": "image-classification", "name": "Image Classification"}, "dataset": {"name": "custom dataset", "type": "imagefolder", "config": "default", "split": "validation", "args": "default"}, "metrics": [{"type": "accuracy", "value": 0.6352941176470588, "name": "Accuracy", "verified": false}]}]}]}, "inference_type": "huggingface"}
{"id": "jazzmacedo/fruits-and-vegetables-detector-36", "pipeline_tag": "image-classification", "tags": ["transformers", "pytorch", "safetensors", "resnet", "image-classification", "vision", "generated_from_trainer", "en", "dataset:imagefolder", "base_model:microsoft/resnet-50", "base_model:finetune:microsoft/resnet-50", "license:apache-2.0", "model-index", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\n- generated_from_trainer\ndatasets:\n- imagefolder\npipeline_tag: image-classification\nbase_model: microsoft/resnet-50\nmodel-index:\n- name: fruits-and-vegetables-detector-36\n  results:\n  - task:\n      type: image-classification\n      name: Image Classification\n    dataset:\n      name: imagefolder\n      type: imagefolder\n      config: default\n      split: train\n      args: default\n    metrics:\n    - type: accuracy\n      value: 0.9721\n      name: Accuracy\n---\n\n# fruits-and-vegetables-detector-36\n\nThis model is a fine-tuned version of [microsoft/resnet-50](https://huggingface.co/microsoft/resnet-50).\n\nIt achieves the following results on the evaluation set:\n\n- Loss: 0.0014\n- Accuracy: 0.9721\n\n## Model description\n\nThis Model is a exploration test using the base model resnet-50 from microsoft.\n\n## Intended uses & limitations\n\nThis Model was trained with a very small dataset\n[kritikseth/fruit-and-vegetable-image-recognition](https://www.kaggle.com/datasets/kritikseth/fruit-and-vegetable-image-recognition) that contains only 36 labels\n\n### How to use\n\nHere is how to use this model to classify an image:\n\n```python\nimport cv2\nimport torch\nimport torchvision.transforms as transforms\nfrom transformers import AutoModelForImageClassification\nfrom PIL import Image\n\n# Load the saved model and tokenizer\nmodel = AutoModelForImageClassification.from_pretrained(\"jazzmacedo/fruits-and-vegetables-detector-36\")\n\n# Get the list of labels from the model's configuration\nlabels = list(model.config.id2label.values())\n\n# Define the preprocessing transformation\npreprocess = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nimage_path = \"path/to/your/image.jpg\"\nimage = cv2.imread(image_path)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\npil_image = Image.fromarray(image)  # Convert NumPy array to PIL image\ninput_tensor = preprocess(pil_image).unsqueeze(0)\n\n# Run the image through the model\noutputs = model(input_tensor)\n\n# Get the predicted label index\npredicted_idx = torch.argmax(outputs.logits, dim=1).item()\n\n# Get the predicted label text\npredicted_label = labels[predicted_idx]\n\n# Print the predicted label\nprint(\"Detected label:\", predicted_label)\n```\n\n## Training and evaluation data\n\nDataset Source: https://www.kaggle.com/datasets/kritikseth/fruit-and-vegetable-image-recognition\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n- learning_rate: 0.001\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n", "downloads": 221, "likes": 3, "meta": {"base_model": "microsoft/resnet-50", "datasets": ["imagefolder"], "language": ["en"], "license": "apache-2.0", "pipeline_tag": "image-classification", "tags": ["vision", "image-classification", "generated_from_trainer"], "model-index": [{"name": "fruits-and-vegetables-detector-36", "results": [{"task": {"type": "image-classification", "name": "Image Classification"}, "dataset": {"name": "imagefolder", "type": "imagefolder", "config": "default", "split": "train", "args": "default"}, "metrics": [{"type": "accuracy", "value": 0.9721, "name": "Accuracy", "verified": false}]}]}]}, "inference_type": "huggingface"}
{"id": "dima806/67_cat_breeds_image_detection", "pipeline_tag": "image-classification", "tags": ["transformers", "pytorch", "safetensors", "vit", "image-classification", "base_model:google/vit-base-patch16-224-in21k", "base_model:finetune:google/vit-base-patch16-224-in21k", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\nmetrics:\n- accuracy\nbase_model:\n- google/vit-base-patch16-224-in21k\n---\nSee https://www.kaggle.com/code/dima806/67-cat-breed-image-detection-vit for more details.", "downloads": 136, "likes": 2, "meta": {"base_model": ["google/vit-base-patch16-224-in21k"], "license": "apache-2.0", "metrics": ["accuracy"]}, "inference_type": "local"}
{"id": "dima806/man_woman_face_image_detection", "pipeline_tag": "image-classification", "tags": ["transformers", "pytorch", "safetensors", "vit", "image-classification", "base_model:google/vit-base-patch16-224-in21k", "base_model:finetune:google/vit-base-patch16-224-in21k", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\nmetrics:\n- accuracy\n- f1\nbase_model:\n- google/vit-base-patch16-224-in21k\n---\nReturns with about 98.7% accuracy whether the face belongs to man or woman based on face image.\n\nSee https://www.kaggle.com/code/dima806/man-woman-face-image-detection-vit for more details.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6449300e3adf50d864095b90/t9MgehgAAZEJAXOebhfjO.png)\n\n```\nClassification report:\n\n              precision    recall  f1-score   support\n\n         man     0.9885    0.9857    0.9871     51062\n       woman     0.9857    0.9885    0.9871     51062\n\n    accuracy                         0.9871    102124\n   macro avg     0.9871    0.9871    0.9871    102124\nweighted avg     0.9871    0.9871    0.9871    102124\n```", "downloads": 64375, "likes": 10, "meta": {"base_model": ["google/vit-base-patch16-224-in21k"], "license": "apache-2.0", "metrics": ["accuracy", "f1"]}, "inference_type": "local"}
{"id": "Anwarkh1/Skin_Cancer-Image_Classification", "pipeline_tag": "image-classification", "tags": ["transformers", "safetensors", "vit", "image-classification", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\n---\n# Skin Cancer Image Classification Model\n\n## Introduction\n\nThis model is designed for the classification of skin cancer images into various categories including benign keratosis-like lesions, basal cell carcinoma, actinic keratoses, vascular lesions, melanocytic nevi, melanoma, and dermatofibroma.\n\n## Model Overview\n\n- Model Architecture: Vision Transformer (ViT)\n- Pre-trained Model: Google's ViT with 16x16 patch size and trained on ImageNet21k dataset\n- Modified Classification Head: The classification head has been replaced to adapt the model to the skin cancer classification task.\n\n## Dataset\n\n- Dataset Name: Skin Cancer Dataset\n- Source: [Marmal88's Skin Cancer Dataset on Hugging Face](https://huggingface.co/datasets/marmal88/skin_cancer)\n- Classes: Benign keratosis-like lesions, Basal cell carcinoma, Actinic keratoses, Vascular lesions, Melanocytic nevi, Melanoma, Dermatofibroma\n\n## Training\n\n- Optimizer: Adam optimizer with a learning rate of 1e-4\n- Loss Function: Cross-Entropy Loss\n- Batch Size: 32\n- Number of Epochs: 5\n\n## Evaluation Metrics\n\n- Train Loss: Average loss over the training dataset\n- Train Accuracy: Accuracy over the training dataset\n- Validation Loss: Average loss over the validation dataset\n- Validation Accuracy: Accuracy over the validation dataset\n\n## Results\n\n- Epoch 1/5, Train Loss: 0.7168, Train Accuracy: 0.7586, Val Loss: 0.4994, Val Accuracy: 0.8355\n- Epoch 2/5, Train Loss: 0.4550, Train Accuracy: 0.8466, Val Loss: 0.3237, Val Accuracy: 0.8973\n- Epoch 3/5, Train Loss: 0.2959, Train Accuracy: 0.9028, Val Loss: 0.1790, Val Accuracy: 0.9530\n- Epoch 4/5, Train Loss: 0.1595, Train Accuracy: 0.9482, Val Loss: 0.1498, Val Accuracy: 0.9555\n- Epoch 5/5, Train Loss: 0.1208, Train Accuracy: 0.9614, Val Loss: 0.1000, Val Accuracy: 0.9695\n## Conclusion\n\nThe model demonstrates good performance in classifying skin cancer images into various categories. Further fine-tuning or experimentation may improve performance on this task.\n\n", "downloads": 1644, "likes": 26, "meta": {"license": "apache-2.0"}, "inference_type": "huggingface"}
{"id": "linkanjarad/mobilenet_v2_1.0_224-plant-disease-identification", "pipeline_tag": "image-classification", "tags": ["transformers", "pytorch", "tensorboard", "mobilenet_v2", "image-classification", "generated_from_trainer", "dataset:image_folder", "license:other", "model-index", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: other\ntags:\n- generated_from_trainer\ndatasets:\n- image_folder\nmetrics:\n- accuracy\nmodel-index:\n- name: mobilenet_v2_1.0_224-plant-disease-identification\n  results:\n  - task:\n      type: image-classification\n      name: Image Classification\n    dataset:\n      name: New Plant Diseases Dataset\n      type: image_folder\n      config: default\n      split: train\n      args: default\n    metrics:\n    - type: accuracy\n      value: 0.9541\n      name: Accuracy\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# mobilenet_v2_1.0_224-plant-disease-identification\n\nThis model is a fine-tuned version of [google/mobilenet_v2_1.0_224](https://huggingface.co/google/mobilenet_v2_1.0_224) on the [Kaggle version](https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset) of the [Plant Village dataset](https://github.com/spMohanty/PlantVillage-Dataset).\nIt achieves the following results on the evaluation set:\n- Cross Entropy Loss: 0.15\n- Accuracy: 0.9541\n\n## Intended uses & limitations\n\nFor identifying common diseases in crops and assessing plant health. Not to be used as a replacement for an actual diagnosis from experts.\n\n## Training and evaluation data\n\nThe plant village dataset consists of 38 classes of diseases in common crops (including healthy/normal crops).\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-5\n- train_batch_size: 256\n- eval_batch_size: 256\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.2\n- num_epochs: 6\n\n### Framework versions\n\n- Transformers 4.27.3\n- Pytorch 1.13.0\n- Datasets 2.1.0\n- Tokenizers 0.13.2\n", "downloads": 2089, "likes": 30, "meta": {"datasets": ["image_folder"], "license": "other", "metrics": ["accuracy"], "tags": ["generated_from_trainer"], "model-index": [{"name": "mobilenet_v2_1.0_224-plant-disease-identification", "results": [{"task": {"type": "image-classification", "name": "Image Classification"}, "dataset": {"name": "New Plant Diseases Dataset", "type": "image_folder", "config": "default", "split": "train", "args": "default"}, "metrics": [{"type": "accuracy", "value": 0.9541, "name": "Accuracy", "verified": false}]}]}]}, "inference_type": "huggingface"}
{"id": "umm-maybe/AI-image-detector", "pipeline_tag": "image-classification", "tags": ["transformers", "pytorch", "autotrain", "vision", "image-classification", "license:cc-by-4.0", "co2_eq_emissions", "endpoints_compatible", "region:us"], "description": "---\ntags:\n- autotrain\n- vision\n- image-classification\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\nco2_eq_emissions:\n  emissions: 7.940487247386902\nlicense: cc-by-4.0\n---\n\n*__NOTE__: Unless you are trying to detect imagery generated using older models such as VQGAN+CLIP, please use the [updated version](https://huggingface.co/Organika/sdxl-detector) of this detector instead.*\n\nThis model is a proof-of-concept demonstration of using a ViT model to predict whether an artistic image was generated using AI.\n\nIt was created in October 2022, and as such, the training data did not include any samples generated by Midjourney 5, SDXL, or DALLE-3. It still may be able to correctly identify samples from these more recent models due to being trained on outputs of their predecessors.\n\nFurthermore the intended scope of this tool is artistic images; that is to say, it is not a deepfake photo detector, and general computer imagery (webcams, screenshots, etc.) may throw it off.\n\nIn general, this tool can only serve as one of many potential indicators that an image was AI-generated. Images scoring as very probably artificial (e.g. 90% or higher) could be referred to a human expert for further investigation, if needed.\n\nFor more information please see the blog post describing this project at:\nhttps://medium.com/@matthewmaybe/can-an-ai-learn-to-identify-ai-art-545d9d6af226\n\n# Model Trained Using AutoTrain\n\n- Problem type: Binary Classification\n- Model ID: 1519658722\n- CO2 Emissions (in grams): 7.9405\n\n## Validation Metrics\n\n- Loss: 0.163\n- Accuracy: 0.942\n- Precision: 0.938\n- Recall: 0.978\n- AUC: 0.980\n- F1: 0.958\n\n# License Notice\n\nThis work is licensed under a [Creative Commons Attribution-NoDerivatives 4.0 International License](https://creativecommons.org/licenses/by-nd/4.0/).\n\nYou may distribute and make this model available to others as part of your own web page, app, or service so long as you provide attribution. However, use of this model within text-to-image systems to evade AI image detection would be considered a \"derivative work\" and as such prohibited by the license terms.", "downloads": 4547, "likes": 70, "meta": {"license": "cc-by-4.0", "tags": ["autotrain", "vision", "image-classification"], "widget": [{"src": "https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg", "example_title": "Tiger"}, {"src": "https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg", "example_title": "Teapot"}, {"src": "https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg", "example_title": "Palace"}], "co2_eq_emissions": {"emissions": 7.940487247386902}}, "inference_type": "huggingface"}
{"id": "dima806/fairface_age_image_detection", "pipeline_tag": "image-classification", "tags": ["transformers", "safetensors", "vit", "image-classification", "dataset:nateraw/fairface", "base_model:google/vit-base-patch16-224-in21k", "base_model:finetune:google/vit-base-patch16-224-in21k", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\nmetrics:\n- accuracy\n- f1\nbase_model:\n- google/vit-base-patch16-224-in21k\npipeline_tag: image-classification\nlibrary_name: transformers\ndatasets:\n- nateraw/fairface\n---\nDetects age group with about 59% accuracy based on an image.\n\nSee https://www.kaggle.com/code/dima806/age-group-image-classification-vit for details.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6449300e3adf50d864095b90/gvzsgTtWDOE4vxwugZF4P.png)\n\n```\nClassification report:\n\n              precision    recall  f1-score   support\n\n         0-2     0.7803    0.7500    0.7649       180\n         3-9     0.7998    0.7998    0.7998      1249\n       10-19     0.5361    0.4236    0.4733      1086\n       20-29     0.6402    0.7221    0.6787      3026\n       30-39     0.4935    0.5083    0.5008      2099\n       40-49     0.4848    0.4386    0.4606      1238\n       50-59     0.5000    0.4814    0.4905       725\n       60-69     0.4497    0.4685    0.4589       286\nmore than 70     0.6897    0.1802    0.2857       111\n\n    accuracy                         0.5892     10000\n   macro avg     0.5971    0.5303    0.5459     10000\nweighted avg     0.5863    0.5892    0.5844     10000\n```", "downloads": 84061432, "likes": 30, "meta": {"base_model": ["google/vit-base-patch16-224-in21k"], "datasets": ["nateraw/fairface"], "library_name": "transformers", "license": "apache-2.0", "metrics": ["accuracy", "f1"], "pipeline_tag": "image-classification"}, "inference_type": "huggingface"}
{"id": "ilsilfverskiold/classify-news-category-iptc", "pipeline_tag": "text-classification", "tags": ["transformers", "tensorboard", "safetensors", "bert", "text-classification", "generated_from_trainer", "base_model:KB/bert-base-swedish-cased", "base_model:finetune:KB/bert-base-swedish-cased", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nbase_model: KB/bert-base-swedish-cased\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\n- f1\n- precision\n- recall\nmodel-index:\n- name: news_category_classification\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# News Category Classification for IPTC NewsCodes\n\nThis model is a fine-tuned version of [KB/bert-base-swedish-cased](https://huggingface.co/KB/bert-base-swedish-cased) on a private dataset.\n\nBuilt from a limited set of English, Swedish and Norwegian titles to classify news content within 16 categories as specified by the IPTC NewsCodes. \n\nThe model has been fine-tuned on a dataset that is greatly skewed, but has been slightly augmented to stabilize it.\n\n## Model description\n\nThe model is intended to categorize Norwegian, Swedish and English news content within the specified 16 categories but is a test model for demonstration purposes. \nIt needs more data within several categories to provide 100% value but it will outperform Claude Haiku and GPT-3.5 on this use case.\n\n## Intended uses & limitations\n\nUse it to categorize news texts. Only set the category if the value is at least 60% for the label, otherwise the model is uncertain.\n\n# Test examples \n\n**Input:** Mann siktet for drapsforsÃ¸k pÃ¥ Slovakias statsministeren\n\n**Output:** politics\n\n**Input:** Tre dÃ¸de i kioskbrann i Tyskland\n\n**Output:** disaster, accident, and emergency incident\n\n**Input:** Kultfilm fÃ¥r Netflix-oppfÃ¸lger. Kultfilmen Â«Happy GilmoreÂ» fra 1996 fÃ¥r en oppfÃ¸lger pÃ¥ Netflix. Det rÃ¸per strÃ¸mmetjenesten selv pÃ¥ X, tidligere Twitter. â€“Happy Gilmore er tilbake!\n\n**Output:** arts, culture, entertainment and media\n\n# Performance\n\nIt achieves the following results on the evaluation set:\n- Loss: 0.8030\n- Accuracy: 0.7431\n- F1: 0.7474\n- Precision: 0.7695\n- Recall: 0.7431\n\nSee the performance (accuracy) for each label below:\n- Arts, culture, entertainment and media: 0.6842\n- Conflict, war and peace: 0.7351\n- Crime, law and justice: 0.8918\n- Disaster, accident, and emergency incident: 0.8699\n- Economy, business, and finance: 0.6893\n- Environment: 0.4483\n- Health: 0.7222\n- Human interest: 0.3182\n- Labour: 0.5\n- Lifestyle and leisure: 0.5556\n- Politics: 0.7909\n- Science and technology: 0.4583\n- Society: 0.3538\n- Sport: 0.9615\n- Weather: 1.0\n- Religion: 0.0\n\n## Training and evaluation data\n\nTrained with the trainer, setting a learning rate of 2e-05 and batch size of 16 for 3 epochs.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch  | Step | Validation Loss | Accuracy | F1     | Precision | Recall | Accuracy Label Arts, culture, entertainment and media | Accuracy Label Conflict, war and peace | Accuracy Label Crime, law and justice | Accuracy Label Disaster, accident, and emergency incident | Accuracy Label Economy, business, and finance | Accuracy Label Environment | Accuracy Label Health | Accuracy Label Human interest | Accuracy Label Labour | Accuracy Label Lifestyle and leisure | Accuracy Label Politics | Accuracy Label Religion | Accuracy Label Science and technology | Accuracy Label Society | Accuracy Label Sport | Accuracy Label Weather |\n|:-------------:|:------:|:----:|:---------------:|:--------:|:------:|:---------:|:------:|:-----------------------------------------------------:|:--------------------------------------:|:-------------------------------------:|:---------------------------------------------------------:|:---------------------------------------------:|:--------------------------:|:---------------------:|:-----------------------------:|:---------------------:|:------------------------------------:|:-----------------------:|:-----------------------:|:-------------------------------------:|:----------------------:|:--------------------:|:----------------------:|\n| 1.9761        | 0.2907 | 200  | 1.4046          | 0.6462   | 0.6164 | 0.6057    | 0.6462 | 0.3158                                                | 0.8315                                 | 0.7629                                | 0.7055                                                    | 0.5437                                        | 0.0                        | 0.5                   | 0.0                           | 0.0                   | 0.3333                               | 0.4843                  | 0.0                     | 0.0833                                | 0.0                    | 0.9615               | 0.0                    |\n| 1.2153        | 0.5814 | 400  | 1.0225          | 0.6894   | 0.6868 | 0.7652    | 0.6894 | 0.7895                                                | 0.6554                                 | 0.8196                                | 0.8562                                                    | 0.6408                                        | 0.2414                     | 0.8333                | 0.1364                        | 0.0                   | 0.6667                               | 0.8467                  | 0.0                     | 0.375                                 | 0.0154                 | 0.9615               | 1.0                    |\n| 0.954         | 0.8721 | 600  | 0.8858          | 0.7231   | 0.7138 | 0.7309    | 0.7231 | 0.7368                                                | 0.7795                                 | 0.8918                                | 0.8699                                                    | 0.6214                                        | 0.3448                     | 0.8889                | 0.1818                        | 1.0                   | 0.5556                               | 0.6899                  | 0.0                     | 0.25                                  | 0.0462                 | 0.9615               | 1.0                    |\n| 0.6662        | 1.1628 | 800  | 0.9381          | 0.6881   | 0.7009 | 0.7618    | 0.6881 | 0.7895                                                | 0.6126                                 | 0.8454                                | 0.8630                                                    | 0.6505                                        | 0.4483                     | 0.7222                | 0.2273                        | 1.0                   | 0.4444                               | 0.8293                  | 0.0                     | 0.5417                                | 0.2308                 | 0.9615               | 1.0                    |\n| 0.5554        | 1.4535 | 1000 | 0.8791          | 0.7025   | 0.7124 | 0.7628    | 0.7025 | 0.7368                                                | 0.6478                                 | 0.9021                                | 0.8562                                                    | 0.6602                                        | 0.3103                     | 0.7778                | 0.3636                        | 0.5                   | 0.5556                               | 0.8084                  | 0.0                     | 0.5                                   | 0.1846                 | 0.9615               | 1.0                    |\n| 0.4396        | 1.7442 | 1200 | 0.8275          | 0.7175   | 0.7280 | 0.7686    | 0.7175 | 0.7895                                                | 0.6631                                 | 0.8196                                | 0.8836                                                    | 0.6893                                        | 0.3793                     | 0.8333                | 0.4091                        | 0.5                   | 0.5556                               | 0.8362                  | 0.0                     | 0.4167                                | 0.3692                 | 0.9615               | 1.0                    |\n| 0.383         | 2.0349 | 1400 | 0.7929          | 0.745    | 0.7501 | 0.7653    | 0.745  | 0.6842                                                | 0.7841                                 | 0.8866                                | 0.8767                                                    | 0.7087                                        | 0.4483                     | 0.7778                | 0.4091                        | 0.5                   | 0.5556                               | 0.6899                  | 0.0                     | 0.4167                                | 0.2923                 | 0.9615               | 0.0                    |\n| 0.3418        | 2.3256 | 1600 | 0.8042          | 0.7438   | 0.7440 | 0.7686    | 0.7438 | 0.7895                                                | 0.7351                                 | 0.9072                                | 0.8493                                                    | 0.7864                                        | 0.4483                     | 0.7778                | 0.3182                        | 0.5                   | 0.5556                               | 0.7909                  | 0.0                     | 0.4167                                | 0.1846                 | 0.9615               | 0.0                    |\n| 0.248         | 2.6163 | 1800 | 0.8387          | 0.7275   | 0.7325 | 0.7610    | 0.7275 | 0.6842                                                | 0.6891                                 | 0.8814                                | 0.8699                                                    | 0.7573                                        | 0.4138                     | 0.8333                | 0.4091                        | 0.5                   | 0.5556                               | 0.8014                  | 0.0                     | 0.4167                                | 0.2769                 | 0.9615               | 0.0                    |\n| 0.2525        | 2.9070 | 2000 | 0.8137          | 0.735    | 0.7413 | 0.7697    | 0.735  | 0.6842                                                | 0.7106                                 | 0.8763                                | 0.8699                                                    | 0.6796                                        | 0.4483                     | 0.7222                | 0.3636                        | 0.5                   | 0.5556                               | 0.8153                  | 0.0                     | 0.4583                                | 0.3385                 | 0.9615               | 0.0                    |\n\n\n### Framework versions\n\n- Transformers 4.40.2\n- Pytorch 2.2.1+cu121\n- Datasets 2.19.1\n- Tokenizers 0.19.1\n", "downloads": 512433, "likes": 1, "meta": {"base_model": "KB/bert-base-swedish-cased", "metrics": ["accuracy", "f1", "precision", "recall"], "tags": ["generated_from_trainer"], "model-index": [{"name": "news_category_classification", "results": []}]}, "inference_type": "local"}
{"id": "iiiorg/piiranha-v1-detect-personal-information", "pipeline_tag": "token-classification", "tags": ["transformers", "safetensors", "deberta-v2", "token-classification", "generated_from_trainer", "pii", "privacy", "personaldata", "redaction", "piidetection", "en", "it", "fr", "de", "nl", "es", "dataset:ai4privacy/pii-masking-400k", "base_model:microsoft/mdeberta-v3-base", "base_model:finetune:microsoft/mdeberta-v3-base", "license:cc-by-nc-nd-4.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlibrary_name: transformers\nlicense: cc-by-nc-nd-4.0\nbase_model: microsoft/mdeberta-v3-base\ntags:\n- generated_from_trainer\n- pii\n- privacy\n- personaldata\n- redaction\n- piidetection\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\ndatasets:\n- ai4privacy/pii-masking-400k\nlanguage:\n- en\n- it\n- fr\n- de\n- nl\n- es\npipeline_tag: token-classification\nmodel-index:\n- name: piiranha-1\n  results: []\n---\n\n# Piiranha-v1: Protect your personal information!\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/williamgao1729/piiranha-quickstart/blob/main/piiranha_quickstart%20(1).ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\nPiiranha (cc-by-nc-nd-4.0 license) is trained to **detect 17 types** of Personally Identifiable Information (PII) across six languages. It successfully **catches 98.27% of PII** tokens, with an overall classification **accuracy of 99.44%**.\nPiiranha is especially accurate at detecting passwords, emails (100%), phone numbers, and usernames.\n\nPerformance on PII vs. Non PII classification task:\n- **Precision: 98.48%** (98.48% of tokens classified as PII are actually PII)\n- **Recall: 98.27%** (correctly identifies 98.27% of PII tokens)\n- **Specificity: 99.84%** (correctly identifies 99.84% of Non PII tokens)\n\n<img src=\"https://cloud-3i4ld6u5y-hack-club-bot.vercel.app/0home.png\" alt=\"Akash Network logo\" width=\"250\"/>\n\nPiiranha was trained on H100 GPUs generously sponsored by the [Akash Network](https://akash.network)\n\n## Model Description\nPiiranha is a fine-tuned version of [microsoft/mdeberta-v3-base](https://huggingface.co/microsoft/mdeberta-v3-base).\nThe context length is 256 Deberta tokens. If your text is longer than that, just split it up.\n\nSupported languages: English, Spanish, French, German, Italian, Dutch\n\nSupported PII types: Account Number, Building Number, City, Credit Card Number, Date of Birth, Driver's License, Email, First Name, Last Name, ID Card, Password, Social Security Number, Street Address, Tax Number, Phone Number, Username, Zipcode. \n\nIt achieves the following results on a test set of ~73,000 sentences containing PII:\n- Accuracy: 99.44%\n- Loss: 0.0173\n- Precision: 93.16%\n- Recall: 93.08%\n- F1: 93.12%\n\nNote that the above metrics factor in the eighteen possible categories (17 PII and 1 Non PII), so the metrics are lower than the metrics for just PII vs. Non PII (binary classification).\n\n## Performance by PII type\nReported performance metrics are lower than the overall accuracy of 99.44% due to class imbalance (most tokens are not PII).\nHowever, the model is more useful than the below results suggest, due to the intent behind PII detection. The model sometimes misclassifies one PII type for another, but at the end of the day, it still recognizes the token as PII.\nFor instance, the model often confuses first names for last names, but that's fine because it still flags the name as PII.\n\n| Entity              | Precision | Recall | F1-Score | Support |\n|---------------------|-----------|--------|----------|---------|\n| ACCOUNTNUM          | 0.84      | 0.87   | 0.85     | 3575    |\n| BUILDINGNUM         | 0.92      | 0.90   | 0.91     | 3252    |\n| CITY                | 0.95      | 0.97   | 0.96     | 7270    |\n| CREDITCARDNUMBER    | 0.94      | 0.96   | 0.95     | 2308    |\n| DATEOFBIRTH         | 0.93      | 0.85   | 0.89     | 3389    |\n| DRIVERLICENSENUM    | 0.96      | 0.96   | 0.96     | 2244    |\n| EMAIL               | 1.00      | 1.00   | 1.00     | 6892    |\n| GIVENNAME           | 0.87      | 0.93   | 0.90     | 12150   |\n| IDCARDNUM           | 0.89      | 0.94   | 0.91     | 3700    |\n| PASSWORD            | 0.98      | 0.98   | 0.98     | 2387    |\n| SOCIALNUM           | 0.93      | 0.94   | 0.93     | 2709    |\n| STREET              | 0.97      | 0.95   | 0.96     | 3331    |\n| SURNAME             | 0.89      | 0.78   | 0.83     | 8267    |\n| TAXNUM              | 0.97      | 0.89   | 0.93     | 2322    |\n| TELEPHONENUM        | 0.99      | 1.00   | 0.99     | 5039    |\n| USERNAME            | 0.98      | 0.98   | 0.98     | 7680    |\n| ZIPCODE             | 0.94      | 0.97   | 0.95     | 3191    |\n| **micro avg**       | 0.93      | 0.93   | 0.93     | 79706   |\n| **macro avg**       | 0.94      | 0.93   | 0.93     | 79706   |\n| **weighted avg**    | 0.93      | 0.93   | 0.93     | 79706   |\n\n## Intended uses & limitations\n\nPiiranha can be used to assist with redacting PII from texts. Use at your own risk. We do not accept responsibility for any incorrect model predictions.\n## Training and evaluation data\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 128\n- eval_batch_size: 128\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.05\n- num_epochs: 5\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch  | Step | Validation Loss | Precision | Recall | F1     | Accuracy |\n|:-------------:|:------:|:----:|:---------------:|:---------:|:------:|:------:|:--------:|\n| 0.2984        | 0.0983 | 250  | 0.1005          | 0.5446    | 0.6111 | 0.5759 | 0.9702   |\n| 0.0568        | 0.1965 | 500  | 0.0464          | 0.7895    | 0.8459 | 0.8167 | 0.9849   |\n| 0.0441        | 0.2948 | 750  | 0.0400          | 0.8346    | 0.8669 | 0.8504 | 0.9869   |\n| 0.0368        | 0.3931 | 1000 | 0.0320          | 0.8531    | 0.8784 | 0.8656 | 0.9891   |\n| 0.0323        | 0.4914 | 1250 | 0.0293          | 0.8779    | 0.8889 | 0.8834 | 0.9903   |\n| 0.0287        | 0.5896 | 1500 | 0.0269          | 0.8919    | 0.8836 | 0.8877 | 0.9907   |\n| 0.0282        | 0.6879 | 1750 | 0.0276          | 0.8724    | 0.9012 | 0.8866 | 0.9903   |\n| 0.0268        | 0.7862 | 2000 | 0.0254          | 0.8890    | 0.9041 | 0.8965 | 0.9914   |\n| 0.0264        | 0.8844 | 2250 | 0.0236          | 0.8886    | 0.9040 | 0.8962 | 0.9915   |\n| 0.0243        | 0.9827 | 2500 | 0.0232          | 0.8998    | 0.9033 | 0.9015 | 0.9917   |\n| 0.0213        | 1.0810 | 2750 | 0.0237          | 0.9115    | 0.9040 | 0.9077 | 0.9923   |\n| 0.0213        | 1.1792 | 3000 | 0.0222          | 0.9123    | 0.9143 | 0.9133 | 0.9925   |\n| 0.0217        | 1.2775 | 3250 | 0.0222          | 0.8999    | 0.9169 | 0.9083 | 0.9924   |\n| 0.0209        | 1.3758 | 3500 | 0.0212          | 0.9111    | 0.9133 | 0.9122 | 0.9928   |\n| 0.0204        | 1.4741 | 3750 | 0.0206          | 0.9054    | 0.9203 | 0.9128 | 0.9926   |\n| 0.0183        | 1.5723 | 4000 | 0.0212          | 0.9126    | 0.9160 | 0.9143 | 0.9927   |\n| 0.0191        | 1.6706 | 4250 | 0.0192          | 0.9122    | 0.9192 | 0.9157 | 0.9929   |\n| 0.0185        | 1.7689 | 4500 | 0.0195          | 0.9200    | 0.9191 | 0.9196 | 0.9932   |\n| 0.018         | 1.8671 | 4750 | 0.0188          | 0.9136    | 0.9215 | 0.9176 | 0.9933   |\n| 0.0183        | 1.9654 | 5000 | 0.0191          | 0.9179    | 0.9212 | 0.9196 | 0.9934   |\n| 0.0147        | 2.0637 | 5250 | 0.0188          | 0.9246    | 0.9242 | 0.9244 | 0.9937   |\n| 0.0149        | 2.1619 | 5500 | 0.0184          | 0.9188    | 0.9254 | 0.9221 | 0.9937   |\n| 0.0143        | 2.2602 | 5750 | 0.0193          | 0.9187    | 0.9224 | 0.9205 | 0.9932   |\n| 0.014         | 2.3585 | 6000 | 0.0190          | 0.9246    | 0.9280 | 0.9263 | 0.9936   |\n| 0.0146        | 2.4568 | 6250 | 0.0190          | 0.9225    | 0.9277 | 0.9251 | 0.9936   |\n| 0.0148        | 2.5550 | 6500 | 0.0175          | 0.9297    | 0.9306 | 0.9301 | 0.9942   |\n| 0.0136        | 2.6533 | 6750 | 0.0172          | 0.9191    | 0.9329 | 0.9259 | 0.9938   |\n| 0.0137        | 2.7516 | 7000 | 0.0166          | 0.9299    | 0.9312 | 0.9306 | 0.9942   |\n| 0.014         | 2.8498 | 7250 | 0.0167          | 0.9285    | 0.9313 | 0.9299 | 0.9942   |\n| 0.0128        | 2.9481 | 7500 | 0.0166          | 0.9271    | 0.9326 | 0.9298 | 0.9943   |\n| 0.0113        | 3.0464 | 7750 | 0.0171          | 0.9286    | 0.9347 | 0.9316 | 0.9946   |\n| 0.0103        | 3.1447 | 8000 | 0.0172          | 0.9284    | 0.9383 | 0.9334 | 0.9945   |\n| 0.0104        | 3.2429 | 8250 | 0.0169          | 0.9312    | 0.9406 | 0.9359 | 0.9947   |\n| 0.0094        | 3.3412 | 8500 | 0.0166          | 0.9368    | 0.9359 | 0.9364 | 0.9948   |\n| 0.01          | 3.4395 | 8750 | 0.0166          | 0.9289    | 0.9387 | 0.9337 | 0.9944   |\n| 0.0099        | 3.5377 | 9000 | 0.0162          | 0.9335    | 0.9332 | 0.9334 | 0.9947   |\n| 0.0099        | 3.6360 | 9250 | 0.0160          | 0.9321    | 0.9380 | 0.9350 | 0.9947   |\n| 0.01          | 3.7343 | 9500 | 0.0168          | 0.9306    | 0.9389 | 0.9347 | 0.9947   |\n| 0.0101        | 3.8325 | 9750 | 0.0159          | 0.9339    | 0.9350 | 0.9344 | 0.9947   |\n\n### Contact\nwilliam (at) integrinet [dot] org\n \n### Framework versions\n\n- Transformers 4.44.2\n- Pytorch 2.4.1+cu121\n- Datasets 3.0.0\n- Tokenizers 0.19.1", "downloads": 120542, "likes": 199, "meta": {"base_model": "microsoft/mdeberta-v3-base", "datasets": ["ai4privacy/pii-masking-400k"], "language": ["en", "it", "fr", "de", "nl", "es"], "library_name": "transformers", "license": "cc-by-nc-nd-4.0", "metrics": ["precision", "recall", "f1", "accuracy"], "pipeline_tag": "token-classification", "tags": ["generated_from_trainer", "pii", "privacy", "personaldata", "redaction", "piidetection"], "model-index": [{"name": "piiranha-1", "results": []}]}, "inference_type": "huggingface"}
{"id": "dslim/bert-base-NER", "pipeline_tag": "token-classification", "tags": ["transformers", "pytorch", "tf", "jax", "onnx", "safetensors", "bert", "token-classification", "en", "dataset:conll2003", "arxiv:1810.04805", "license:mit", "model-index", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage: en\ndatasets:\n- conll2003\nlicense: mit\nmodel-index:\n- name: dslim/bert-base-NER\n  results:\n  - task:\n      type: token-classification\n      name: Token Classification\n    dataset:\n      name: conll2003\n      type: conll2003\n      config: conll2003\n      split: test\n    metrics:\n    - type: accuracy\n      value: 0.9118041001560013\n      name: Accuracy\n      verified: true\n    - type: precision\n      value: 0.9211550382257732\n      name: Precision\n      verified: true\n    - type: recall\n      value: 0.9306415698281261\n      name: Recall\n      verified: true\n    - type: f1\n      value: 0.9258740048459675\n      name: F1\n      verified: true\n    - type: loss\n      value: 0.48325642943382263\n      name: loss\n      verified: true\n---\n# bert-base-NER\n\nIf my open source models have been useful to you, please consider supporting me in building small, useful AI models for everyone (and help me afford med school / help out my parents financially). Thanks!\n\n<a href=\"https://www.buymeacoffee.com/dslim\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/arial-yellow.png\" alt=\"Buy Me A Coffee\" style=\"height: 60px !important;width: 217px !important;\" ></a>\n\n## Model description\n\n**bert-base-NER** is a fine-tuned BERT model that is ready to use for **Named Entity Recognition** and achieves **state-of-the-art performance** for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). \n\nSpecifically, this model is a *bert-base-cased* model that was fine-tuned on the English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \n\nIf you'd like to use a larger BERT-large model fine-tuned on the same dataset, a [**bert-large-NER**](https://huggingface.co/dslim/bert-large-NER/) version is also available. \n\n### Available NER models \n| Model Name | Description | Parameters |\n|-------------------|-------------|------------------|\n| [distilbert-NER](https://huggingface.co/dslim/distilbert-NER) **(NEW!)** | Fine-tuned DistilBERT - a smaller, faster, lighter version of BERT | 66M |\n| [bert-large-NER](https://huggingface.co/dslim/bert-large-NER/) | Fine-tuned bert-large-cased - larger model with slightly better performance | 340M |\n| [bert-base-NER](https://huggingface.co/dslim/bert-base-NER)-([uncased](https://huggingface.co/dslim/bert-base-NER-uncased)) | Fine-tuned bert-base, available in both cased and uncased versions | 110M |\n\n\n## Intended uses & limitations\n\n#### How to use\n\nYou can use this model with Transformers *pipeline* for NER.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"My name is Wolfgang and I live in Berlin\"\n\nner_results = nlp(example)\nprint(ner_results)\n```\n\n#### Limitations and bias\n\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases. \n\n## Training data\n\nThis model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \n\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\n\nAbbreviation|Description\n-|-\nO|Outside of a named entity\nB-MISC |Beginning of a miscellaneous entity right after another miscellaneous entity\nI-MISC | Miscellaneous entity\nB-PER |Beginning of a personâ€™s name right after another personâ€™s name\nI-PER |Personâ€™s name\nB-ORG |Beginning of an organization right after another organization\nI-ORG |organization\nB-LOC |Beginning of a location right after another location\nI-LOC |Location\n\n\n### CoNLL-2003 English Dataset Statistics\nThis dataset was derived from the Reuters corpus which consists of Reuters news stories. You can read more about how this dataset was created in the CoNLL-2003 paper. \n#### # of training examples per entity type\nDataset|LOC|MISC|ORG|PER\n-|-|-|-|-\nTrain|7140|3438|6321|6600\nDev|1837|922|1341|1842\nTest|1668|702|1661|1617\n#### # of articles/sentences/tokens per dataset\nDataset |Articles |Sentences |Tokens\n-|-|-|-\nTrain |946 |14,987 |203,621\nDev |216 |3,466 |51,362\nTest |231 |3,684 |46,435\n\n## Training procedure\n\nThis model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task. \n\n## Eval results\nmetric|dev|test\n-|-|-\nf1 |95.1 |91.3\nprecision |95.0 |90.7\nrecall |95.3 |91.9\n\nThe test metrics are a little lower than the official Google BERT results which encoded document context & experimented with CRF. More on replicating the original results [here](https://github.com/google-research/bert/issues/223).\n\n### BibTeX entry and citation info\n\n```\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n```\n@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\n    title = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\n    author = \"Tjong Kim Sang, Erik F.  and\n      De Meulder, Fien\",\n    booktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\n    year = \"2003\",\n    url = \"https://www.aclweb.org/anthology/W03-0419\",\n    pages = \"142--147\",\n}\n```\n", "downloads": 2027784, "likes": 615, "meta": {"datasets": ["conll2003"], "language": "en", "license": "mit", "model-index": [{"name": "dslim/bert-base-NER", "results": [{"task": {"type": "token-classification", "name": "Token Classification"}, "dataset": {"name": "conll2003", "type": "conll2003", "config": "conll2003", "split": "test"}, "metrics": [{"type": "accuracy", "value": 0.9118041001560013, "name": "Accuracy", "verified": false}, {"type": "precision", "value": 0.9211550382257732, "name": "Precision", "verified": false}, {"type": "recall", "value": 0.9306415698281261, "name": "Recall", "verified": false}, {"type": "f1", "value": 0.9258740048459675, "name": "F1", "verified": false}, {"type": "loss", "value": 0.48325642943382263, "name": "loss", "verified": false}]}]}]}, "inference_type": "huggingface"}
{"id": "blaze999/Medical-NER", "pipeline_tag": "token-classification", "tags": ["transformers", "safetensors", "deberta-v2", "token-classification", "generated_from_trainer", "medical", "base_model:microsoft/deberta-v3-base", "base_model:finetune:microsoft/deberta-v3-base", "license:mit", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: mit\nbase_model: microsoft/deberta-v3-base\ntags:\n- generated_from_trainer\n- medical\nwidget:\n- text: 63 year old woman with history of CAD presented to ER\n  example_title: Example-1\n- text: 63 year old woman diagnosed with CAD\n  example_title: Example-2\n- text: A 48 year-old female presented with vaginal bleeding and abnormal Pap smears.\n    Upon diagnosis of invasive non-keratinizing SCC of the cervix, she underwent a\n    radical hysterectomy with salpingo-oophorectomy which demonstrated positive spread\n    to the pelvic lymph nodes and the parametrium. Pathological examination revealed\n    that the tumour also extensively involved the lower uterine segment.\n  example_title: example 3\npipeline_tag: token-classification\nmodel-index:\n- name: deberta-med-ner-2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n\n\n\n# deberta-med-ner-2\n\nThis model is a fine-tuned version of [DeBERTa](https://huggingface.co/microsoft/deberta-v3-base) on the PubMED Dataset.\n\n## Model description\n\nMedical NER Model finetuned on BERT to recognize 41 Medical entities.\n\n\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 16\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n\n\n## Usage\nThe easiest way is to load the inference api from huggingface and second method is through the pipeline object offered by transformers library.\n```python\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\npipe = pipeline(\"token-classification\", model=\"Clinical-AI-Apollo/Medical-NER\", aggregation_strategy='simple')\nresult = pipe('45 year old woman diagnosed with CAD')\n\n\n\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"Clinical-AI-Apollo/Medical-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Clinical-AI-Apollo/Medical-NER\")\n```\n\n### Author\n\nAuthor: [Saketh Mattupalli](https://huggingface.co/blaze999)\n\n### Framework versions\n\n- Transformers 4.37.0\n- Pytorch 2.1.2\n- Datasets 2.1.0\n- Tokenizers 0.15.1", "downloads": 13087, "likes": 218, "meta": {"base_model": "microsoft/deberta-v3-base", "license": "mit", "pipeline_tag": "token-classification", "tags": ["generated_from_trainer", "medical"], "widget": [{"text": "63 year old woman with history of CAD presented to ER", "example_title": "Example-1"}, {"text": "63 year old woman diagnosed with CAD", "example_title": "Example-2"}, {"text": "A 48 year-old female presented with vaginal bleeding and abnormal Pap smears. Upon diagnosis of invasive non-keratinizing SCC of the cervix, she underwent a radical hysterectomy with salpingo-oophorectomy which demonstrated positive spread to the pelvic lymph nodes and the parametrium. Pathological examination revealed that the tumour also extensively involved the lower uterine segment.", "example_title": "example 3"}], "model-index": [{"name": "deberta-med-ner-2", "results": []}]}, "inference_type": "huggingface"}
{"id": "w11wo/indonesian-roberta-base-posp-tagger", "pipeline_tag": "token-classification", "tags": ["transformers", "pytorch", "tf", "tensorboard", "safetensors", "roberta", "token-classification", "generated_from_trainer", "ind", "dataset:indonlu", "base_model:flax-community/indonesian-roberta-base", "base_model:finetune:flax-community/indonesian-roberta-base", "license:mit", "model-index", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: mit\nbase_model: flax-community/indonesian-roberta-base\ntags:\n- generated_from_trainer\ndatasets:\n- indonlu\nlanguage:\n- ind\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: indonesian-roberta-base-posp-tagger\n  results:\n  - task:\n      type: token-classification\n      name: Token Classification\n    dataset:\n      name: indonlu\n      type: indonlu\n      config: posp\n      split: test\n      args: posp\n    metrics:\n    - type: precision\n      value: 0.9625100240577386\n      name: Precision\n    - type: recall\n      value: 0.9625100240577386\n      name: Recall\n    - type: f1\n      value: 0.9625100240577386\n      name: F1\n    - type: accuracy\n      value: 0.9625100240577386\n      name: Accuracy\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# indonesian-roberta-base-posp-tagger\n\nThis model is a fine-tuned version of [flax-community/indonesian-roberta-base](https://huggingface.co/flax-community/indonesian-roberta-base) on the indonlu dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1395\n- Precision: 0.9625\n- Recall: 0.9625\n- F1: 0.9625\n- Accuracy: 0.9625\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Precision | Recall | F1     | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:---------:|:------:|:------:|:--------:|\n| No log        | 1.0   | 420  | 0.2254          | 0.9313    | 0.9313 | 0.9313 | 0.9313   |\n| 0.4398        | 2.0   | 840  | 0.1617          | 0.9499    | 0.9499 | 0.9499 | 0.9499   |\n| 0.1566        | 3.0   | 1260 | 0.1431          | 0.9569    | 0.9569 | 0.9569 | 0.9569   |\n| 0.103         | 4.0   | 1680 | 0.1412          | 0.9605    | 0.9605 | 0.9605 | 0.9605   |\n| 0.0723        | 5.0   | 2100 | 0.1408          | 0.9635    | 0.9635 | 0.9635 | 0.9635   |\n| 0.051         | 6.0   | 2520 | 0.1408          | 0.9642    | 0.9642 | 0.9642 | 0.9642   |\n| 0.051         | 7.0   | 2940 | 0.1510          | 0.9635    | 0.9635 | 0.9635 | 0.9635   |\n| 0.0368        | 8.0   | 3360 | 0.1653          | 0.9645    | 0.9645 | 0.9645 | 0.9645   |\n| 0.0277        | 9.0   | 3780 | 0.1664          | 0.9644    | 0.9644 | 0.9644 | 0.9644   |\n| 0.0231        | 10.0  | 4200 | 0.1668          | 0.9646    | 0.9646 | 0.9646 | 0.9646   |\n\n\n### Framework versions\n\n- Transformers 4.37.2\n- Pytorch 2.2.0+cu118\n- Datasets 2.16.1\n- Tokenizers 0.15.1\n", "downloads": 2234927, "likes": 7, "meta": {"base_model": "flax-community/indonesian-roberta-base", "datasets": ["indonlu"], "language": ["ind"], "license": "mit", "metrics": ["precision", "recall", "f1", "accuracy"], "tags": ["generated_from_trainer"], "model-index": [{"name": "indonesian-roberta-base-posp-tagger", "results": [{"task": {"type": "token-classification", "name": "Token Classification"}, "dataset": {"name": "indonlu", "type": "indonlu", "config": "posp", "split": "test", "args": "posp"}, "metrics": [{"type": "precision", "value": 0.9625100240577386, "name": "Precision", "verified": false}, {"type": "recall", "value": 0.9625100240577386, "name": "Recall", "verified": false}, {"type": "f1", "value": 0.9625100240577386, "name": "F1", "verified": false}, {"type": "accuracy", "value": 0.9625100240577386, "name": "Accuracy", "verified": false}]}]}]}, "inference_type": "local"}
{"id": "SamLowe/roberta-base-go_emotions", "pipeline_tag": "text-classification", "tags": ["transformers", "pytorch", "safetensors", "roberta", "text-classification", "emotions", "multi-class-classification", "multi-label-classification", "en", "dataset:go_emotions", "doi:10.57967/hf/3548", "license:mit", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage: en\ntags:\n- text-classification\n- pytorch\n- roberta\n- emotions\n- multi-class-classification\n- multi-label-classification\ndatasets:\n- go_emotions\nlicense: mit\nwidget:\n- text: I am not having a great day.\n---\n\n#### Overview\n\nModel trained from [roberta-base](https://huggingface.co/roberta-base) on the [go_emotions](https://huggingface.co/datasets/go_emotions) dataset for multi-label classification.\n\n##### ONNX version also available\n\nA version of this model in ONNX format (including an INT8 quantized ONNX version) is now available at [https://huggingface.co/SamLowe/roberta-base-go_emotions-onnx](https://huggingface.co/SamLowe/roberta-base-go_emotions-onnx). These are faster for inference, esp for smaller batch sizes, massively reduce the size of the dependencies required for inference, make inference of the model more multi-platform, and in the case of the quantized version reduce the model file/download size by 75% whilst retaining almost all the accuracy if you only need inference.\n\n#### Dataset used for the model\n\n[go_emotions](https://huggingface.co/datasets/go_emotions) is based on Reddit data and has 28 labels. It is a multi-label dataset where one or multiple labels may apply for any given input text, hence this model is a multi-label classification model with 28 'probability' float outputs for any given input text. Typically a threshold of 0.5 is applied to the probabilities for the prediction for each label.\n\n#### How the model was created\n\nThe model was trained using `AutoModelForSequenceClassification.from_pretrained` with `problem_type=\"multi_label_classification\"` for 3 epochs with a learning rate of 2e-5 and weight decay of 0.01.\n\n#### Inference\n\nThere are multiple ways to use this model in Huggingface Transformers. Possibly the simplest is using a pipeline:\n\n```python\nfrom transformers import pipeline\n\nclassifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n\nsentences = [\"I am not having a great day\"]\n\nmodel_outputs = classifier(sentences)\nprint(model_outputs[0])\n# produces a list of dicts for each of the labels\n```\n\n#### Evaluation / metrics\n\nEvaluation of the model is available at\n\n- https://github.com/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb)\n\n##### Summary\n\nAs provided in the above notebook, evaluation of the multi-label output (of the 28 dim output via a threshold of 0.5 to binarize each) using the dataset test split gives:\n\n- Accuracy: 0.474\n- Precision: 0.575\n- Recall: 0.396\n- F1: 0.450\n\nBut the metrics are more meaningful when measured per label given the multi-label nature (each label is effectively an independent binary classification) and the fact that there is drastically different representations of the labels in the dataset.\n\nWith a threshold of 0.5 applied to binarize the model outputs, as per the above notebook, the metrics per label are:\n\n|                | accuracy | precision | recall | f1    | mcc   | support | threshold |\n| -------------- | -------- | --------- | ------ | ----- | ----- | ------- | --------- |\n| admiration     | 0.946    | 0.725     | 0.675  | 0.699 | 0.670 | 504     | 0.5       |\n| amusement      | 0.982    | 0.790     | 0.871  | 0.829 | 0.821 | 264     | 0.5       |\n| anger          | 0.970    | 0.652     | 0.379  | 0.479 | 0.483 | 198     | 0.5       |\n| annoyance      | 0.940    | 0.472     | 0.159  | 0.238 | 0.250 | 320     | 0.5       |\n| approval       | 0.942    | 0.609     | 0.302  | 0.404 | 0.403 | 351     | 0.5       |\n| caring         | 0.973    | 0.448     | 0.319  | 0.372 | 0.364 | 135     | 0.5       |\n| confusion      | 0.972    | 0.500     | 0.431  | 0.463 | 0.450 | 153     | 0.5       |\n| curiosity      | 0.950    | 0.537     | 0.356  | 0.428 | 0.412 | 284     | 0.5       |\n| desire         | 0.987    | 0.630     | 0.410  | 0.496 | 0.502 | 83      | 0.5       |\n| disappointment | 0.974    | 0.625     | 0.199  | 0.302 | 0.343 | 151     | 0.5       |\n| disapproval    | 0.950    | 0.494     | 0.307  | 0.379 | 0.365 | 267     | 0.5       |\n| disgust        | 0.982    | 0.707     | 0.333  | 0.453 | 0.478 | 123     | 0.5       |\n| embarrassment  | 0.994    | 0.750     | 0.243  | 0.367 | 0.425 | 37      | 0.5       |\n| excitement     | 0.983    | 0.603     | 0.340  | 0.435 | 0.445 | 103     | 0.5       |\n| fear           | 0.992    | 0.758     | 0.603  | 0.671 | 0.672 | 78      | 0.5       |\n| gratitude      | 0.990    | 0.960     | 0.881  | 0.919 | 0.914 | 352     | 0.5       |\n| grief          | 0.999    | 0.000     | 0.000  | 0.000 | 0.000 | 6       | 0.5       |\n| joy            | 0.978    | 0.647     | 0.559  | 0.600 | 0.590 | 161     | 0.5       |\n| love           | 0.982    | 0.773     | 0.832  | 0.802 | 0.793 | 238     | 0.5       |\n| nervousness    | 0.996    | 0.600     | 0.130  | 0.214 | 0.278 | 23      | 0.5       |\n| optimism       | 0.972    | 0.667     | 0.376  | 0.481 | 0.488 | 186     | 0.5       |\n| pride          | 0.997    | 0.000     | 0.000  | 0.000 | 0.000 | 16      | 0.5       |\n| realization    | 0.974    | 0.541     | 0.138  | 0.220 | 0.264 | 145     | 0.5       |\n| relief         | 0.998    | 0.000     | 0.000  | 0.000 | 0.000 | 11      | 0.5       |\n| remorse        | 0.991    | 0.553     | 0.750  | 0.636 | 0.640 | 56      | 0.5       |\n| sadness        | 0.977    | 0.621     | 0.494  | 0.550 | 0.542 | 156     | 0.5       |\n| surprise       | 0.981    | 0.750     | 0.404  | 0.525 | 0.542 | 141     | 0.5       |\n| neutral        | 0.782    | 0.694     | 0.604  | 0.646 | 0.492 | 1787    | 0.5       |\n\nOptimizing the threshold per label for the one that gives the optimum F1 metrics gives slightly better metrics - sacrificing some precision for a greater gain in recall, hence to the benefit of F1 (how this was done is shown in the above notebook):\n\n|                | accuracy | precision | recall | f1    | mcc   | support | threshold |\n| -------------- | -------- | --------- | ------ | ----- | ----- | ------- | --------- |\n| admiration     | 0.940    | 0.651     | 0.776  | 0.708 | 0.678 | 504     | 0.25      |\n| amusement      | 0.982    | 0.781     | 0.890  | 0.832 | 0.825 | 264     | 0.45      |\n| anger          | 0.959    | 0.454     | 0.601  | 0.517 | 0.502 | 198     | 0.15      |\n| annoyance      | 0.864    | 0.243     | 0.619  | 0.349 | 0.328 | 320     | 0.10      |\n| approval       | 0.926    | 0.432     | 0.442  | 0.437 | 0.397 | 351     | 0.30      |\n| caring         | 0.972    | 0.426     | 0.385  | 0.405 | 0.391 | 135     | 0.40      |\n| confusion      | 0.974    | 0.548     | 0.412  | 0.470 | 0.462 | 153     | 0.55      |\n| curiosity      | 0.943    | 0.473     | 0.711  | 0.568 | 0.552 | 284     | 0.25      |\n| desire         | 0.985    | 0.518     | 0.530  | 0.524 | 0.516 | 83      | 0.25      |\n| disappointment | 0.974    | 0.562     | 0.298  | 0.390 | 0.398 | 151     | 0.40      |\n| disapproval    | 0.941    | 0.414     | 0.468  | 0.439 | 0.409 | 267     | 0.30      |\n| disgust        | 0.978    | 0.523     | 0.463  | 0.491 | 0.481 | 123     | 0.20      |\n| embarrassment  | 0.994    | 0.567     | 0.459  | 0.507 | 0.507 | 37      | 0.10      |\n| excitement     | 0.981    | 0.500     | 0.417  | 0.455 | 0.447 | 103     | 0.35      |\n| fear           | 0.991    | 0.712     | 0.667  | 0.689 | 0.685 | 78      | 0.40      |\n| gratitude      | 0.990    | 0.957     | 0.889  | 0.922 | 0.917 | 352     | 0.45      |\n| grief          | 0.999    | 0.333     | 0.333  | 0.333 | 0.333 | 6       | 0.05      |\n| joy            | 0.978    | 0.623     | 0.646  | 0.634 | 0.623 | 161     | 0.40      |\n| love           | 0.982    | 0.740     | 0.899  | 0.812 | 0.807 | 238     | 0.25      |\n| nervousness    | 0.996    | 0.571     | 0.348  | 0.432 | 0.444 | 23      | 0.25      |\n| optimism       | 0.971    | 0.580     | 0.565  | 0.572 | 0.557 | 186     | 0.20      |\n| pride          | 0.998    | 0.875     | 0.438  | 0.583 | 0.618 | 16      | 0.10      |\n| realization    | 0.961    | 0.270     | 0.262  | 0.266 | 0.246 | 145     | 0.15      |\n| relief         | 0.992    | 0.152     | 0.636  | 0.246 | 0.309 | 11      | 0.05      |\n| remorse        | 0.991    | 0.541     | 0.946  | 0.688 | 0.712 | 56      | 0.10      |\n| sadness        | 0.977    | 0.599     | 0.583  | 0.591 | 0.579 | 156     | 0.40      |\n| surprise       | 0.977    | 0.543     | 0.674  | 0.601 | 0.593 | 141     | 0.15      |\n| neutral        | 0.758    | 0.598     | 0.810  | 0.688 | 0.513 | 1787    | 0.25      |\n\nThis improves the overall metrics:\n\n- Precision: 0.542\n- Recall: 0.577\n- F1: 0.541\n\nOr if calculated weighted by the relative size of the support of each label:\n\n- Precision: 0.572\n- Recall: 0.677\n- F1: 0.611\n\n#### Commentary on the dataset\n\nSome labels (E.g. gratitude) when considered independently perform very strongly with F1 exceeding 0.9, whilst others (E.g. relief) perform very poorly.\n\nThis is a challenging dataset. Labels such as relief do have much fewer examples in the training data (less than 100 out of the 40k+, and only 11 in the test split).\n\nBut there is also some ambiguity and/or labelling errors visible in the training data of go_emotions that is suspected to constrain the performance. Data cleaning on the dataset to reduce some of the mistakes, ambiguity, conflicts and duplication in the labelling would produce a higher performing model.", "downloads": 425885, "likes": 586, "meta": {"datasets": ["go_emotions"], "language": "en", "license": "mit", "tags": ["text-classification", "pytorch", "roberta", "emotions", "multi-class-classification", "multi-label-classification"], "widget": [{"text": "I am not having a great day."}]}, "inference_type": "huggingface"}
{"id": "rjac/bert-20news-classification", "pipeline_tag": "text-classification", "tags": ["transformers", "tf", "distilbert", "text-classification", "generated_from_keras_callback", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: bert-20news-classification\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# bert-20news-classification\n\nThis model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Train Loss: 0.0479\n- Train Accuracy: 0.9922\n- Validation Loss: 0.2769\n- Validation Accuracy: 0.9284\n- Epoch: 9\n\n## Model description\n\nThis model is a fine-tuned version of the DistilBERT model for sequence classification tasks. It was trained using Hugging Face's transformers and TensorFlow. The model expects input sequences to be tokenized according to the DistilBERT's tokenizer.\n\nThe model was trained specifically for classifying text into 20 different categories derived from the 20 Newsgroups dataset. These categories include various topics such as 'alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'.\n\n\n## Intended uses & limitations\nThis model is intended for classifying text into the above mentioned 20 categories. It can be used for categorizing text data from similar domains or topics.\n\n## Training and evaluation data\nthe model was trained on 90% of the data from the 20 Newsgroups dataset, with the remaining 10% used for validation.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: {'name': 'Adam', 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'jit_compile': True, 'is_legacy_optimizer': False, 'learning_rate': {'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 2e-05, 'decay_steps': 2120, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}}, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False}\n- training_precision: float32\n\n### Training results\n\n| Train Loss | Train Accuracy | Validation Loss | Validation Accuracy | Epoch |\n|:----------:|:--------------:|:---------------:|:-------------------:|:-----:|\n| 1.8498     | 0.5829         | 0.9285          | 0.8012              | 0     |\n| 0.6611     | 0.8406         | 0.4800          | 0.8807              | 1     |\n| 0.3563     | 0.9128         | 0.3829          | 0.9002              | 2     |\n| 0.2276     | 0.9475         | 0.3593          | 0.9072              | 3     |\n| 0.1544     | 0.9659         | 0.3205          | 0.9214              | 4     |\n| 0.1094     | 0.9779         | 0.3007          | 0.9214              | 5     |\n| 0.0825     | 0.9846         | 0.2821          | 0.9258              | 6     |\n| 0.0634     | 0.9895         | 0.2754          | 0.9337              | 7     |\n| 0.0533     | 0.9916         | 0.2707          | 0.9337              | 8     |\n| 0.0479     | 0.9922         | 0.2769          | 0.9284              | 9     |\n\n\n### Framework versions\n\n- Transformers 4.28.0\n- TensorFlow 2.12.0\n- Datasets 2.12.0\n- Tokenizers 0.13.3\n", "downloads": 38, "likes": 0, "meta": {"license": "apache-2.0", "tags": ["generated_from_keras_callback"], "model-index": [{"name": "bert-20news-classification", "results": []}]}, "inference_type": "local"}
{"id": "mshenoda/roberta-spam", "pipeline_tag": "text-classification", "tags": ["transformers", "pytorch", "safetensors", "roberta", "text-classification", "en", "dataset:mshenoda/spam-messages", "arxiv:1907.11692", "license:mit", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: mit\ndatasets:\n- mshenoda/spam-messages\npipeline_tag: text-classification\nwidget:\n- text: U have a secret admirer. REVEAL who thinks U R So special. Call 09065174042.\n    To opt out Reply REVEAL STOP. 1.50 per msg recd.\n  example_title: spam example 1\n- text: Hey so this sat are we going for the intro pilates only? Or the kickboxing\n    too?\n  example_title: ham example 1\n- text: Great News! Call FREEFONE 08006344447 to claim your guaranteed $1000 CASH\n    or $2000 gift. Speak to a live operator NOW!\n  example_title: spam example 2\n- text: Dude im no longer a pisces. Im an aquarius now.\n  example_title: ham example 2\nlanguage:\n- en\n---\n# RoBERTa based Spam Message Detection\nSpam messages frequently carry malicious links or phishing attempts posing significant threats to both organizations and their users. By choosing our RoBERTa-based spam message detection system, organizations can greatly enhance their security infrastructure. Our system effectively detects and filters out spam messages, adding an extra layer of security that safeguards organizations against potential financial losses, legal consequences, and reputational harm.\n\n## Found this model useful:\nYour feedback is important and would help keep this relevent. \n\n## Metrics\nLoss    |  Accuracy(0.9906)      |  Precision(0.9971) / Recall(0.9934)     |    Confusion Matrix          \n:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------: \n![](plots/train_validation_loss.jpg \"Train / Validation Loss\") Train / Validation | ![](plots/validation_accuracy.jpg \"Validation Accuracy\") Validation | ![](plots/validation_precision_recall.jpg \"Validation Precision / Recall\")  Validation | ![](plots/confusion_matrix.png \"confusion_matrix\")  Testing Set\n\n## Model Output\n- 0 is ham\n- 1 is spam\n\n## Dataset\n\nhttps://huggingface.co/datasets/mshenoda/spam-messages\n\nThe dataset is composed of messages labeled by ham or spam, merged from three data sources:\n\n1. SMS Spam Collection https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset\n2. Telegram Spam Ham https://huggingface.co/datasets/thehamkercat/telegram-spam-ham/tree/main\n3. Enron Spam:  https://huggingface.co/datasets/SetFit/enron_spam/tree/main (only used message column and labels)\n\nThe prepare script for enron is available at https://github.com/mshenoda/roberta-spam/tree/main/data/enron.\nThe data is split 80% train 10% validation, and 10% test sets; the scripts used to split and merge of the three data sources are available at: https://github.com/mshenoda/roberta-spam/tree/main/data/utils.\n\n### Dataset Class Distribution\n\nTraining  80%  |  Validation  10%   |  Testing  10%          \n:-------------------------:|:-------------------------:|:-------------------------: \n![](plots/train_set_distribution.jpg \"Train / Validation Loss\") Class Distribution | ![](plots/val_set_distribution.jpg \"Class Distribution\") Class Distribution | ![](plots/test_set_distribution.jpg \"Class Distribution\")  Class Distribution\n\n\n## Architecture\nThe model is fine tuned RoBERTa \n\nroberta-base: https://huggingface.co/roberta-base\n\npaper: https://arxiv.org/abs/1907.11692 \n\n## Code\n\nhttps://github.com/mshenoda/roberta-spam", "downloads": 22824, "likes": 30, "meta": {"datasets": ["mshenoda/spam-messages"], "language": ["en"], "license": "mit", "pipeline_tag": "text-classification", "widget": [{"text": "U have a secret admirer. REVEAL who thinks U R So special. Call 09065174042. To opt out Reply REVEAL STOP. 1.50 per msg recd.", "example_title": "spam example 1"}, {"text": "Hey so this sat are we going for the intro pilates only? Or the kickboxing too?", "example_title": "ham example 1"}, {"text": "Great News! Call FREEFONE 08006344447 to claim your guaranteed $1000 CASH or $2000 gift. Speak to a live operator NOW!", "example_title": "spam example 2"}, {"text": "Dude im no longer a pisces. Im an aquarius now.", "example_title": "ham example 2"}]}, "inference_type": "huggingface"}
{"id": "jy46604790/Fake-News-Bert-Detect", "pipeline_tag": "text-classification", "tags": ["transformers", "pytorch", "roberta", "text-classification", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\n---\n\n\n\n# Fake News Recognition\n\n## Overview\n\nThis model is trained by over 40,000 news from different medias based on the 'roberta-base'. It can give result by simply entering the text of the news less than 500 words(the excess will be truncated automatically).\n\nLABEL_0: Fake news\n\nLABEL_1: Real news\n\n## Qucik Tutorial\n\n### Download The Model\n\n```python\nfrom transformers import pipeline\nMODEL = \"jy46604790/Fake-News-Bert-Detect\"\nclf = pipeline(\"text-classification\", model=MODEL, tokenizer=MODEL)\n```\n\n### Feed Data\n\n```python\ntext = \"Indonesian police have recaptured a U.S. citizen who escaped a week ago from an overcrowded prison on the holiday island of Bali, the jail s second breakout of foreign inmates this year.  Cristian Beasley from California was rearrested on Sunday, Badung Police chief Yudith Satria Hananta said, without providing further details.  Beasley was a suspect in crimes related to narcotics but had not been sentenced when he escaped from Kerobokan prison in Bali last week. The 32-year-old is believed to have cut through bars in the ceiling of his cell before scaling a perimeter wall of the prison in an area being refurbished. The Kerobokan prison, about 10 km (six miles) from the main tourist beaches in the Kuta area, often holds foreigners facing drug-related charges. Representatives of Beasley could not immediately be reached for comment. In June, an Australian, a Bulgarian, an Indian and a Malaysian tunneled to freedom about 12 meters (13 yards) under Kerobokan prison s walls. The Indian and the Bulgarian were caught soon after in neighboring East Timor, but Australian Shaun Edward Davidson and Malaysian Tee Kok King remain at large. Davidson has taunted authorities by saying he was enjoying life in various parts of the world, in purported posts on Facebook.  Kerobokan has housed a number of well-known foreign drug convicts, including Australian Schappelle Corby, whose 12-1/2-year sentence for marijuana smuggling got huge media attention.\"\n```\n\n### Result\n\n```python\nresult = clf(text)\nresult\n```\n\noutput:[{'label': 'LABEL_1', 'score': 0.9994995594024658}]", "downloads": 9476, "likes": 25, "meta": {"license": "apache-2.0"}, "inference_type": "huggingface"}
{"id": "ProsusAI/finbert", "pipeline_tag": "text-classification", "tags": ["transformers", "pytorch", "tf", "jax", "bert", "text-classification", "financial-sentiment-analysis", "sentiment-analysis", "en", "arxiv:1908.10063", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage: en\ntags:\n- financial-sentiment-analysis\n- sentiment-analysis\nwidget:\n- text: Stocks rallied and the British pound gained.\n---\n\nFinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. [Financial PhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts) by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper [FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063) and our related [blog post](https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on Medium.\n\nThe model will give softmax outputs for three labels: positive, negative or neutral.\n\n---\n\nAbout Prosus\n\nProsus is a global consumer internet group and one of the largest technology investors in the world. Operating and investing globally in markets with long-term growth potential, Prosus builds leading consumer internet companies that empower people and enrich communities. For more information, please visit www.prosus.com.\n\nContact information\n\nPlease contact Dogu Araci dogu.araci[at]prosus[dot]com and Zulkuf Genc zulkuf.genc[at]prosus[dot]com about any FinBERT related issues and questions.\n", "downloads": 1203694, "likes": 910, "meta": {"language": "en", "tags": ["financial-sentiment-analysis", "sentiment-analysis"], "widget": [{"text": "Stocks rallied and the British pound gained."}]}, "inference_type": "huggingface"}
{"id": "j-hartmann/emotion-english-distilroberta-base", "pipeline_tag": "text-classification", "tags": ["transformers", "pytorch", "tf", "roberta", "text-classification", "distilroberta", "sentiment", "emotion", "twitter", "reddit", "en", "arxiv:2210.00434", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage: en\ntags:\n- distilroberta\n- sentiment\n- emotion\n- twitter\n- reddit\nwidget:\n- text: Oh wow. I didn't know that.\n- text: This movie always makes me cry..\n- text: Oh Happy Day\n---\n\n# Emotion English DistilRoBERTa-base\n\n# Description â„¹\n\nWith this model, you can classify emotions in English text data. The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emotions, plus a neutral class:\n\n1) anger ðŸ¤¬\n2) disgust ðŸ¤¢\n3) fear ðŸ˜¨\n4) joy ðŸ˜€\n5) neutral ðŸ˜\n6) sadness ðŸ˜­\n7) surprise ðŸ˜²\n\nThe model is a fine-tuned checkpoint of [DistilRoBERTa-base](https://huggingface.co/distilroberta-base). For a 'non-distilled' emotion model, please refer to the model card of the [RoBERTa-large](https://huggingface.co/j-hartmann/emotion-english-roberta-large) version.\n\n# Application ðŸš€\n\na) Run emotion model with 3 lines of code on single text example using Hugging Face's pipeline command on Google Colab:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/j-hartmann/emotion-english-distilroberta-base/blob/main/simple_emotion_pipeline.ipynb)\n\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\nclassifier(\"I love this!\")\n```\n\n```python\nOutput:\n[[{'label': 'anger', 'score': 0.004419783595949411},\n  {'label': 'disgust', 'score': 0.0016119900392368436},\n  {'label': 'fear', 'score': 0.0004138521908316761},\n  {'label': 'joy', 'score': 0.9771687984466553},\n  {'label': 'neutral', 'score': 0.005764586851000786},\n  {'label': 'sadness', 'score': 0.002092392183840275},\n  {'label': 'surprise', 'score': 0.008528684265911579}]]\n```\n\nb) Run emotion model on multiple examples and full datasets (e.g., .csv files) on Google Colab:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/j-hartmann/emotion-english-distilroberta-base/blob/main/emotion_prediction_example.ipynb)\n\n# Contact ðŸ’»\n\nPlease reach out to [jochen.hartmann@tum.de](mailto:jochen.hartmann@tum.de) if you have any questions or feedback.\n\nThanks to Samuel Domdey and [chrsiebert](https://huggingface.co/siebert) for their support in making this model available.\n\n# Reference âœ…\n\nFor attribution, please cite the following reference if you use this model. A working paper will be available soon.\n\n```\nJochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.\n```\n\nBibTex citation:\n\n```\n@misc{hartmann2022emotionenglish,\n  author={Hartmann, Jochen},\n  title={Emotion English DistilRoBERTa-base},\n  year={2022},\n  howpublished = {\\url{https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/}},\n}\n```\n\n# Appendix ðŸ“š\n\nPlease find an overview of the datasets used for training below. All datasets contain English text. The table summarizes which emotions are available in each of the datasets. The datasets represent a diverse collection of text types. Specifically, they contain emotion labels for texts from Twitter, Reddit, student self-reports, and utterances from TV dialogues. As MELD (Multimodal EmotionLines Dataset) extends the popular EmotionLines dataset, EmotionLines itself is not included here. \n\n|Name|anger|disgust|fear|joy|neutral|sadness|surprise|\n|---|---|---|---|---|---|---|---|\n|Crowdflower (2016)|Yes|-|-|Yes|Yes|Yes|Yes|\n|Emotion Dataset, Elvis et al. (2018)|Yes|-|Yes|Yes|-|Yes|Yes|\n|GoEmotions, Demszky et al. (2020)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|ISEAR, Vikash (2018)|Yes|Yes|Yes|Yes|-|Yes|-|\n|MELD, Poria et al. (2019)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|SemEval-2018, EI-reg, Mohammad et al. (2018) |Yes|-|Yes|Yes|-|Yes|-|\n\nThe model is trained on a balanced subset from the datasets listed above (2,811 observations per emotion, i.e., nearly 20k observations in total). 80% of this balanced subset is used for training and 20% for evaluation. The evaluation accuracy is 66% (vs. the random-chance baseline of 1/7 = 14%).\n\n# Scientific Applications ðŸ“–\n\nBelow you can find a list of papers using \"Emotion English DistilRoBERTa-base\". If you would like your paper to be added to the list, please send me an email.\n\n- Butt, S., Sharma, S., Sharma, R., Sidorov, G., & Gelbukh, A. (2022). What goes on inside rumour and non-rumour tweets and their reactions: A Psycholinguistic Analyses. Computers in Human Behavior, 107345.\n- Kuang, Z., Zong, S., Zhang, J., Chen, J., & Liu, H. (2022). Music-to-Text Synaesthesia: Generating Descriptive Text from Music Recordings. arXiv preprint arXiv:2210.00434.\n- Rozado, D., Hughes, R., & Halberstadt, J. (2022). Longitudinal analysis of sentiment and emotion in news media headlines using automated labelling with Transformer language models. Plos one, 17(10), e0276367.", "downloads": 635346, "likes": 427, "meta": {"language": "en", "tags": ["distilroberta", "sentiment", "emotion", "twitter", "reddit"], "widget": [{"text": "Oh wow. I didn't know that."}, {"text": "This movie always makes me cry.."}, {"text": "Oh Happy Day"}]}, "inference_type": "huggingface"}
{"id": "cardiffnlp/tweet-topic-21-multi", "pipeline_tag": "text-classification", "tags": ["transformers", "pytorch", "tf", "roberta", "text-classification", "en", "dataset:cardiffnlp/tweet_topic_multi", "arxiv:2209.09824", "license:mit", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage: en\nwidget:\n- text: It is great to see athletes promoting awareness for climate change.\ndatasets:\n- cardiffnlp/tweet_topic_multi\nlicense: mit\nmetrics:\n- f1\n- accuracy\npipeline_tag: text-classification\n---\n\n# tweet-topic-21-multi\n\nThis model is based on a [TimeLMs](https://github.com/cardiffnlp/timelms) language model trained on ~124M tweets from January 2018 to December 2021 (see [here](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m)), and finetuned for multi-label topic classification on a corpus of 11,267 [tweets](https://huggingface.co/datasets/cardiffnlp/tweet_topic_multi). This model is suitable for English. \n\n - Reference Paper: [TweetTopic](https://arxiv.org/abs/2209.09824) (COLING 2022). \n\n<b>Labels</b>: \n\n\n| <span style=\"font-weight:normal\">0: arts_&_culture</span>           | <span style=\"font-weight:normal\">5: fashion_&_style</span>   | <span style=\"font-weight:normal\">10: learning_&_educational</span>  | <span style=\"font-weight:normal\">15: science_&_technology</span>  |\n|-----------------------------|---------------------|----------------------------|--------------------------|\n| 1: business_&_entrepreneurs | 6: film_tv_&_video  | 11: music                  | 16: sports               |\n| 2: celebrity_&_pop_culture  | 7: fitness_&_health | 12: news_&_social_concern  | 17: travel_&_adventure   |\n| 3: diaries_&_daily_life     | 8: food_&_dining    | 13: other_hobbies          | 18: youth_&_student_life |\n| 4: family                   | 9: gaming           | 14: relationships          |                          |\n\n\n## Full classification example\n\n```python\nfrom transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import expit\n\n    \nMODEL = f\"cardiffnlp/tweet-topic-21-multi\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nclass_mapping = model.config.id2label\n\ntext = \"It is great to see athletes promoting awareness for climate change.\"\ntokens = tokenizer(text, return_tensors='pt')\noutput = model(**tokens)\n\nscores = output[0][0].detach().numpy()\nscores = expit(scores)\npredictions = (scores >= 0.5) * 1\n\n\n# TF\n#tf_model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n#class_mapping = tf_model.config.id2label\n#text = \"It is great to see athletes promoting awareness for climate change.\"\n#tokens = tokenizer(text, return_tensors='tf')\n#output = tf_model(**tokens)\n#scores = output[0][0]\n#scores = expit(scores)\n#predictions = (scores >= 0.5) * 1\n\n# Map to classes\nfor i in range(len(predictions)):\n  if predictions[i]:\n    print(class_mapping[i])\n\n```\nOutput: \n\n```\nnews_&_social_concern\nsports\n```\n\n### BibTeX entry and citation info\n\nPlease cite the [reference paper](https://aclanthology.org/2022.coling-1.299/) if you use this model.\n\n```bibtex\n@inproceedings{antypas-etal-2022-twitter,\n    title = \"{T}witter Topic Classification\",\n    author = \"Antypas, Dimosthenis  and\n      Ushio, Asahi  and\n      Camacho-Collados, Jose  and\n      Silva, Vitor  and\n      Neves, Leonardo  and\n      Barbieri, Francesco\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.299\",\n    pages = \"3386--3400\"\n}\n```", "downloads": 168352, "likes": 67, "meta": {"datasets": ["cardiffnlp/tweet_topic_multi"], "language": "en", "license": "mit", "metrics": ["f1", "accuracy"], "pipeline_tag": "text-classification", "widget": [{"text": "It is great to see athletes promoting awareness for climate change."}]}, "inference_type": "huggingface"}
{"id": "s-nlp/roberta_toxicity_classifier", "pipeline_tag": "text-classification", "tags": ["transformers", "pytorch", "roberta", "text-classification", "toxic comments classification", "en", "dataset:google/jigsaw_toxicity_pred", "arxiv:1907.11692", "base_model:FacebookAI/roberta-large", "base_model:finetune:FacebookAI/roberta-large", "license:openrail++", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage:\n- en\ntags:\n- toxic comments classification\nlicenses:\n- cc-by-nc-sa\nlicense: openrail++\nbase_model:\n- FacebookAI/roberta-large\ndatasets:\n- google/jigsaw_toxicity_pred\n---\n\n## Toxicity Classification Model\n\nThis model is trained for toxicity classification task. The dataset used for training is the merge of the English parts of the three datasets by **Jigsaw** ([Jigsaw 2018](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge), [Jigsaw 2019](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification), [Jigsaw 2020](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification)), containing around 2 million examples. We split it into two parts and fine-tune a RoBERTa model ([RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)) on it. The classifiers perform closely on the test set of the first Jigsaw competition, reaching the **AUC-ROC** of 0.98 and **F1-score** of 0.76.\n\n## How to use\n```python\nimport torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\n\ntokenizer = RobertaTokenizer.from_pretrained('s-nlp/roberta_toxicity_classifier')\nmodel = RobertaForSequenceClassification.from_pretrained('s-nlp/roberta_toxicity_classifier')\n\nbatch = tokenizer.encode(\"You are amazing!\", return_tensors=\"pt\")\n\noutput = model(batch)\n# idx 0 for neutral, idx 1 for toxic\n```\n\n## Citation\n\nTo acknowledge our work, please, use the corresponding citation:\n\n```\n@inproceedings{logacheva-etal-2022-paradetox,\n    title = \"{P}ara{D}etox: Detoxification with Parallel Data\",\n    author = \"Logacheva, Varvara  and\n      Dementieva, Daryna  and\n      Ustyantsev, Sergey  and\n      Moskovskiy, Daniil  and\n      Dale, David  and\n      Krotova, Irina  and\n      Semenov, Nikita  and\n      Panchenko, Alexander\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.469\",\n    pages = \"6804--6818\",\n    abstract = \"We present a novel pipeline for the collection of parallel data for the detoxification task. We collect non-toxic paraphrases for over 10,000 English toxic sentences. We also show that this pipeline can be used to distill a large existing corpus of paraphrases to get toxic-neutral sentence pairs. We release two parallel corpora which can be used for the training of detoxification models. To the best of our knowledge, these are the first parallel datasets for this task.We describe our pipeline in detail to make it fast to set up for a new language or domain, thus contributing to faster and easier development of new parallel resources.We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches. We conduct both automatic and manual evaluations. All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin. This suggests that our novel datasets can boost the performance of detoxification systems.\",\n}\n```\n\n## Licensing Information\n\nThis model is licensed under the OpenRAIL++ License, which supports the development of various technologiesâ€”both industrial and academicâ€”that serve the public good.", "downloads": 208166, "likes": 60, "meta": {"base_model": ["FacebookAI/roberta-large"], "datasets": ["google/jigsaw_toxicity_pred"], "language": ["en"], "license": "openrail++", "tags": ["toxic comments classification"], "licenses": ["cc-by-nc-sa"]}, "inference_type": "huggingface"}
{"id": "Falconsai/intent_classification", "pipeline_tag": "text-classification", "tags": ["transformers", "pytorch", "coreml", "safetensors", "distilbert", "text-classification", "en", "arxiv:1910.01108", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\npipeline_tag: text-classification\nlanguage:\n- en\nwidget:\n- text: I ordered from you 2 weeks ago and its stil not here.\n- text: I need to bring in my daughter for a checkup.\n---\n# Model Card: Fine-Tuned DistilBERT for User Intent Classification\n\n## Model Description\n\nThe **Fine-Tuned DistilBERT** is a variant of the BERT transformer model,\ndistilled for efficient performance while maintaining high accuracy.\nIt has been adapted and fine-tuned for the specific task of classifying user intent in text data.\n\nThe model, named \"distilbert-base-uncased,\" is pre-trained on a substantial amount of text data,\nwhich allows it to capture semantic nuances and contextual information present in natural language text.\nIt has been fine-tuned with meticulous attention to hyperparameter settings, including batch size and learning rate, to ensure optimal model performance for the user intent classification task.\n\nDuring the fine-tuning process, a batch size of 8 for efficient computation and learning was chosen.\nAdditionally, a learning rate (2e-5) was selected to strike a balance between rapid convergence and steady optimization,\nensuring the model not only learns quickly but also steadily refines its capabilities throughout training.\n\nThis model has been trained on a rather small dataset of under 50k, 100 epochs, specifically designed for user intent classification.\nThe dataset consists of text samples, each labeled with different user intents, such as \"information seeking,\" \"question asking,\" or \"opinion expressing.\" The diversity within the dataset allowed the model to learn to identify user intent accurately. This dataset was carefully curated from a variety of sources.\n\nThe goal of this meticulous training process is to equip the model with the ability to classify user intent in text data effectively, making it ready to contribute to a wide range of applications involving user interaction analysis and personalization.\n\n## Intended Uses & Limitations\n\n### Intended Uses\n- **User Intent Classification**: The primary intended use of this model is to classify user intent in text data. It is well-suited for applications that involve understanding user intentions, such as chatbots, virtual assistants, and recommendation systems.\n\n### How to Use\nTo use this model for user intent classification, you can follow these steps:\n\n```markdown\nfrom transformers import pipeline\n\nclassifier = pipeline(\"text-classification\", model=\"Falconsai/intent_classification\")\ntext = \"Your text to classify here.\"\nresult = classifier(text)\n```\n\n### Limitations\n- **Specialized Task Fine-Tuning**: While the model excels at user intent classification, its performance may vary when applied to other natural language processing tasks. Users interested in employing this model for different tasks should explore fine-tuned versions available in the model hub for optimal results.\n\n## Training Data\n\nThe model's training data includes a proprietary dataset designed for user intent classification. This dataset comprises a diverse collection of text samples, categorized into various user intent classes. The training process aimed to equip the model with the ability to classify user intent effectively.\n\n### Training Stats\n- Evaluation Loss:  0.011744413524866104\n- Evaluation Accuracy: 0.9986976744186047\n- Evaluation Runtime: 3.1136\n- Evaluation Samples per Second: 1726.29\n- Evaluation Steps per Second: 215.826\n\n## Responsible Usage\n\nIt is essential to use this model responsibly and ethically, adhering to content guidelines and applicable regulations when implementing it in real-world applications, particularly those involving potentially sensitive content.\n\n## References\n\n- [Hugging Face Model Hub](https://huggingface.co/models)\n- [DistilBERT Paper](https://arxiv.org/abs/1910.01108)\n\n**Disclaimer:** The model's performance may be influenced by the quality and representativeness of the data it was fine-tuned on. Users are encouraged to assess the model's suitability for their specific applications and datasets.", "downloads": 1163, "likes": 44, "meta": {"language": ["en"], "license": "apache-2.0", "pipeline_tag": "text-classification", "widget": [{"text": "I ordered from you 2 weeks ago and its stil not here."}, {"text": "I need to bring in my daughter for a checkup."}]}, "inference_type": "huggingface"}
{"id": "unitary/toxic-bert", "pipeline_tag": "text-classification", "tags": ["transformers", "pytorch", "jax", "safetensors", "bert", "text-classification", "arxiv:1703.04009", "arxiv:1905.12516", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\n---\n\n      \n<div align=\"center\">    \n\n**âš ï¸ Disclaimer:**\nThe huggingface models currently give different results to the detoxify library (see issue [here](https://github.com/unitaryai/detoxify/issues/15)). For the most up to date models we recommend using the models from https://github.com/unitaryai/detoxify\n\n# ðŸ™Š Detoxify\n##  Toxic Comment Classification with âš¡ Pytorch Lightning and ðŸ¤— Transformers   \n\n![CI testing](https://github.com/unitaryai/detoxify/workflows/CI%20testing/badge.svg)\n![Lint](https://github.com/unitaryai/detoxify/workflows/Lint/badge.svg)\n\n</div>\n \n![Examples image](examples.png)\n\n## Description   \n\nTrained models & code to predict toxic comments on 3 Jigsaw challenges: Toxic comment classification, UnintendedÂ Bias in Toxic comments, Multilingual toxic comment classification.\n\nBuilt by [Laura Hanu](https://laurahanu.github.io/) at [Unitary](https://www.unitary.ai/), where we are working to stop harmful content online by interpreting visual content in context. \n\nDependencies:\n- For inference:\n  - ðŸ¤— Transformers\n  - âš¡ Pytorch lightning \n- For training will also need:\n  - Kaggle API (to download data)\n\n\n| Challenge | Year | Goal | Original Data Source | Detoxify Model Name | Top Kaggle Leaderboard Score | Detoxify Score\n|-|-|-|-|-|-|-|\n| [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) | 2018 |  build a multi-headed model thatâ€™s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate. | Wikipedia Comments | `original` | 0.98856 | 0.98636\n| [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) | 2019 | build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. | Civil Comments | `unbiased` | 0.94734 | 0.93639\n| [Jigsaw Multilingual Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification) | 2020 | build effective multilingual models | Wikipedia Comments + Civil Comments | `multilingual` | 0.9536 | 0.91655*\n\n*Score not directly comparable since it is obtained on the validation set provided and not on the test set. To update when the test labels are made available. \n\nIt is also noteworthy to mention that the top leadearboard scores have been achieved using model ensembles. The purpose of this library was to build something user-friendly and straightforward to use.\n\n## Limitations and ethical considerations\n\nIf words that are associated with swearing, insults or profanity are present in a comment, it is likely that it will be classified as toxic, regardless of the tone or the intent of the author e.g. humorous/self-deprecating. This could present some biases towards already vulnerable minority groups.\n\nThe intended use of this library is for research purposes, fine-tuning on carefully constructed datasets that reflect real world demographics  and/or to aid content moderators in flagging out harmful content quicker.\n\nSome useful resources about the risk of different biases in toxicity or hate speech detection are:\n- [The Risk of Racial Bias in Hate Speech Detection](https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf)\n- [Automated Hate Speech Detection and the Problem of Offensive Language](https://arxiv.org/pdf/1703.04009.pdf%201.pdf)\n- [Racial Bias in Hate Speech and Abusive Language Detection Datasets](https://arxiv.org/pdf/1905.12516.pdf)\n\n## Quick prediction\n\n\nThe `multilingual` model has been trained on 7 different languages so it should only be tested on: `english`, `french`, `spanish`, `italian`, `portuguese`, `turkish` or `russian`.\n\n```bash\n# install detoxify  \n\npip install detoxify\n\n```\n```python\n\nfrom detoxify import Detoxify\n\n# each model takes in either a string or a list of strings\n\nresults = Detoxify('original').predict('example text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','Ã¶rnek metin','Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ñ‚ÐµÐºÑÑ‚Ð°'])\n\n# optional to display results nicely (will need to pip install pandas)\n\nimport pandas as pd\n\nprint(pd.DataFrame(results, index=input_text).round(5))\n\n```\nFor more details check the Prediction section.\n\n\n## Labels\nAll challenges have a toxicity label. The toxicity labels represent the aggregate ratings of up to 10 annotators according the following schema:\n- **Very Toxic** (a very hateful, aggressive, or disrespectful comment that is very likely to make you leave a discussion or give up on sharing your perspective)\n- **Toxic** (a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective)\n- **Hard to Say**\n- **Not Toxic**\n\nMore information about the labelling schema can be found [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).\n\n### Toxic Comment Classification Challenge\nThis challenge includes the following labels:\n\n- `toxic`\n- `severe_toxic`\n- `obscene`\n- `threat`\n- `insult`\n- `identity_hate`\n\n### Jigsaw Unintended Bias in Toxicity Classification\nThis challenge has 2 types of labels: the main toxicity labels and some additional identity labels that represent the identities mentioned in the comments. \n\nOnly identities with more than 500 examples in the test set (combined public and private) are included during training as additional labels and in the evaluation calculation.\n\n- `toxicity`\n- `severe_toxicity`\n- `obscene`\n- `threat`\n- `insult`\n- `identity_attack`\n- `sexual_explicit`\n\nIdentity labels used:\n- `male`\n- `female`\n- `homosexual_gay_or_lesbian`\n- `christian`\n- `jewish`\n- `muslim`\n- `black`\n- `white`\n- `psychiatric_or_mental_illness`\n\nA complete list of all the identity labels available can be found [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).\n\n\n### Jigsaw Multilingual Toxic Comment Classification\n\nSince this challenge combines the data from the previous 2 challenges, it includes all labels from above, however the final evaluation is only on:\n\n- `toxicity`\n\n## How to run   \n\nFirst, install dependencies   \n```bash\n# clone project   \n\ngit clone https://github.com/unitaryai/detoxify\n\n# create virtual env\n\npython3 -m venv toxic-env\nsource toxic-env/bin/activate\n\n# install project   \n\npip install -e detoxify\ncd detoxify\n\n# for training\npip install -r requirements.txt\n\n ```   \n\n## Prediction\n\nTrained models summary:\n\n|Model name| Transformer type| Data from\n|:--:|:--:|:--:|\n|`original`| `bert-base-uncased` | Toxic Comment Classification Challenge\n|`unbiased`| `roberta-base`| Unintended Bias in Toxicity Classification\n|`multilingual`| `xlm-roberta-base`| Multilingual Toxic Comment Classification\n\nFor a quick prediction can run the example script on a comment directly or from a txt containing a list of comments. \n```bash\n\n# load model via torch.hub\n\npython run_prediction.py --input 'example' --model_name original\n\n# load model from from checkpoint path\n\npython run_prediction.py --input 'example' --from_ckpt_path model_path\n\n# save results to a .csv file\n\npython run_prediction.py --input test_set.txt --model_name original --save_to results.csv\n\n# to see usage\n\npython run_prediction.py --help\n\n```\n\nCheckpoints can be downloaded from the latest release or via the Pytorch hub API with the following names:\n- `toxic_bert`\n- `unbiased_toxic_roberta`\n- `multilingual_toxic_xlm_r`\n```bash\nmodel = torch.hub.load('unitaryai/detoxify','toxic_bert')\n```\n\nImporting detoxify in python:\n\n```python\n\nfrom detoxify import Detoxify\n\nresults = Detoxify('original').predict('some text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','Ã¶rnek metin','Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ñ‚ÐµÐºÑÑ‚Ð°'])\n\n# to display results nicely\n\nimport pandas as pd\n\nprint(pd.DataFrame(results,index=input_text).round(5))\n\n```\n\n\n## Training\n\n If you do not already have a Kaggle account: \n - you need to create one to be able to download the data\n \n - go to My Account and click on Create New API Token - this will download a kaggle.json file\n\n - make sure this file is located in ~/.kaggle\n\n ```bash\n\n# create data directory\n\nmkdir jigsaw_data\ncd jigsaw_data\n\n# download data\n\nkaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n\nkaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification\n\nkaggle competitions download -c jigsaw-multilingual-toxic-comment-classification\n\n```\n## Start Training\n ### Toxic Comment Classification Challenge\n\n ```bash\n\npython create_val_set.py\n\npython train.py --config configs/Toxic_comment_classification_BERT.json\n``` \n ### Unintended Bias in Toxicicity Challenge\n\n```bash\n\npython train.py --config configs/Unintended_bias_toxic_comment_classification_RoBERTa.json\n\n```\n ### Multilingual Toxic Comment Classification\n\n This is trained in 2 stages. First, train on all available data, and second, train only on the translated versions of the first challenge. \n \n The [translated data](https://www.kaggle.com/miklgr500/jigsaw-train-multilingual-coments-google-api) can be downloaded from Kaggle in french, spanish, italian, portuguese, turkish, and russian (the languages available in the test set).\n\n ```bash\n\n# stage 1\n\npython train.py --config configs/Multilingual_toxic_comment_classification_XLMR.json\n\n# stage 2\n\npython train.py --config configs/Multilingual_toxic_comment_classification_XLMR_stage2.json\n\n```\n### Monitor progress with tensorboard\n\n ```bash\n\ntensorboard --logdir=./saved\n\n```\n## Model Evaluation\n\n### Toxic Comment Classification Challenge\n\nThis challenge is evaluated on the mean AUC score of all the labels.\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n```\n### Unintended Bias in Toxicicity Challenge\n\nThis challenge is evaluated on a novel bias metric that combines different AUC scores to balance overall performance. More information on this metric [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation).\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n# to get the final bias metric\npython model_eval/compute_bias_metric.py\n\n```\n### Multilingual Toxic Comment Classification\n\nThis challenge is evaluated on the AUC score of the main toxic label.\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n```\n\n### Citation   \n```\n@misc{Detoxify,\n  title={Detoxify},\n  author={Hanu, Laura and {Unitary team}},\n  howpublished={Github. https://github.com/unitaryai/detoxify},\n  year={2020}\n}\n```", "downloads": 160222, "likes": 186, "meta": {"license": "apache-2.0"}, "inference_type": "huggingface"}
{"id": "vineetsharma/customer-support-intent-albert", "pipeline_tag": "text-classification", "tags": ["transformers", "pytorch", "safetensors", "albert", "text-classification", "generated_from_trainer", "base_model:albert/albert-base-v2", "base_model:finetune:albert/albert-base-v2", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\nbase_model: albert-base-v2\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nwidget:\n- text: please help me change several items of an order\n  example_title: example 1\n- text: i need the invoice of the last order\n  example_title: example 2\n- text: can you please change the shipping address\n  example_title: example 3\nmodel-index:\n- name: customer-support-intent-albert\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# customer-support-intent-albert\n\nThis model is a fine-tuned version of [albert-base-v2](https://huggingface.co/albert-base-v2) for intent classification on the [bitext/Bitext-customer-support-llm-chatbot-training-dataset](https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset) dataset.\n\nIt achieves the following results on the evaluation set:\n- Loss: 0.0154\n- Accuracy: 0.9988\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|\n| 1.1993        | 1.0   | 409  | 0.0969          | 0.9927   |\n| 0.0304        | 2.0   | 818  | 0.0247          | 0.9951   |\n| 0.0087        | 3.0   | 1227 | 0.0169          | 0.9963   |\n\n\n### Framework versions\n\n- Transformers 4.33.1\n- Pytorch 2.0.1\n- Datasets 2.14.5\n- Tokenizers 0.13.3\n", "downloads": 165, "likes": 21, "meta": {"base_model": "albert-base-v2", "license": "apache-2.0", "metrics": ["accuracy"], "tags": ["generated_from_trainer"], "widget": [{"text": "please help me change several items of an order", "example_title": "example 1"}, {"text": "i need the invoice of the last order", "example_title": "example 2"}, {"text": "can you please change the shipping address", "example_title": "example 3"}], "model-index": [{"name": "customer-support-intent-albert", "results": []}]}, "inference_type": "huggingface"}
{"id": "ElKulako/cryptobert", "pipeline_tag": "text-classification", "tags": ["transformers", "pytorch", "safetensors", "roberta", "text-classification", "cryptocurrency", "crypto", "BERT", "sentiment classification", "NLP", "bitcoin", "ethereum", "shib", "social media", "sentiment analysis", "cryptocurrency sentiment analysis", "en", "dataset:ElKulako/stocktwits-crypto", "license:mit", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\ndatasets:\n- ElKulako/stocktwits-crypto\nlanguage:\n- en\ntags:\n- cryptocurrency\n- crypto\n- BERT\n- sentiment classification\n- NLP\n- bitcoin\n- ethereum\n- shib\n- social media\n- sentiment analysis\n- cryptocurrency sentiment analysis\nlicense: mit\n---\n\nFor academic reference, cite the following paper: https://ieeexplore.ieee.org/document/10223689\n\n# CryptoBERT\nCryptoBERT is a pre-trained NLP model to analyse the language and sentiments of cryptocurrency-related social media posts and messages. It was built by further training the [vinai's bertweet-base](https://huggingface.co/vinai/bertweet-base) language model on the cryptocurrency domain, using a corpus of over 3.2M unique cryptocurrency-related social media posts. \n(A research paper with more details will follow soon.)\n## Classification Training\nThe model was trained on the following labels: \"Bearish\" : 0, \"Neutral\": 1, \"Bullish\": 2\n\nCryptoBERT's sentiment classification head was fine-tuned on a balanced dataset of 2M labelled StockTwits posts, sampled from [ElKulako/stocktwits-crypto](https://huggingface.co/datasets/ElKulako/stocktwits-crypto). \n\nCryptoBERT was trained with a max sequence length of 128. Technically, it can handle sequences of up to 514 tokens, however, going beyond 128 is not recommended.\n\n# Classification Example\n```python\nfrom transformers import TextClassificationPipeline, AutoModelForSequenceClassification, AutoTokenizer\nmodel_name = \"ElKulako/cryptobert\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 3)\npipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, max_length=64, truncation=True, padding = 'max_length')\n# post_1 & post_3 = bullish, post_2 = bearish\npost_1 = \" see y'all tomorrow and can't wait to see ada in the morning, i wonder what price it is going to be at. ðŸ˜ŽðŸ‚ðŸ¤ ðŸ’¯ðŸ˜´, bitcoin is looking good go for it and flash by that 45k. \"\npost_2 = \"  alright racers, itâ€™s a race to the bottom! good luck today and remember there are no losers (minus those who invested in currency nobody really uses) take your marks... are you ready? go!!\" \npost_3 = \" i'm never selling. the whole market can bottom out. i'll continue to hold this dumpster fire until the day i die if i need to.\" \ndf_posts = [post_1, post_2, post_3]\npreds = pipe(df_posts)\nprint(preds)\n\n\n```\n\n```\n[{'label': 'Bullish', 'score': 0.8734585642814636}, {'label': 'Bearish', 'score': 0.9889495372772217}, {'label': 'Bullish', 'score': 0.6595883965492249}]\n```\n\n## Training Corpus\nCryptoBERT was trained on 3.2M social media posts regarding various cryptocurrencies. Only non-duplicate posts of length above 4 words were considered. The following communities were used as sources for our corpora:\n\n\n(1) StockTwits - 1.875M posts about the top 100 cryptos by trading volume. Posts were collected from the 1st of November 2021 to the 16th of June 2022. [ElKulako/stocktwits-crypto](https://huggingface.co/datasets/ElKulako/stocktwits-crypto)\n\n(2) Telegram - 664K posts from top 5 telegram groups: [Binance](https://t.me/binanceexchange), [Bittrex](https://t.me/BittrexGlobalEnglish), [huobi global](https://t.me/huobiglobalofficial), [Kucoin](https://t.me/Kucoin_Exchange), [OKEx](https://t.me/OKExOfficial_English). \nData from 16.11.2020 to 30.01.2021. Courtesy of [Anton](https://www.kaggle.com/datasets/aagghh/crypto-telegram-groups).\n\n(3) Reddit - 172K comments from various crypto investing threads, collected from May 2021 to May 2022\n\n(4) Twitter - 496K posts with hashtags XBT, Bitcoin or BTC. Collected for May 2018. Courtesy of [Paul](https://www.kaggle.com/datasets/paul92s/bitcoin-tweets-14m).", "downloads": 215795, "likes": 166, "meta": {"datasets": ["ElKulako/stocktwits-crypto"], "language": ["en"], "license": "mit", "tags": ["cryptocurrency", "crypto", "BERT", "sentiment classification", "NLP", "bitcoin", "ethereum", "shib", "social media", "sentiment analysis", "cryptocurrency sentiment analysis"]}, "inference_type": "local"}
{"id": "thanhtlx/text_classification_2", "pipeline_tag": "text-classification", "tags": ["safetensors", "roberta", "region:us"], "description": "---\n{}\n---\n# Institution Name Normalization Model\n\n## ðŸ“ MÃ´ táº£\n\nÄÃ¢y lÃ  má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i vÄƒn báº£n Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ chuáº©n hÃ³a cÃ¡c tÃªn cÆ¡ quan (institutions) Ä‘Æ°á»£c viáº¿t theo nhiá»u cÃ¡ch khÃ¡c nhau trong cÃ¡c bÃ i bÃ¡o khoa há»c. \nMÃ´ hÃ¬nh nháº­n diá»‡n vÃ  Ã¡nh xáº¡ cÃ¡c tÃªn cÆ¡ quan (cÃ³ thá»ƒ khÃ¡c nhau vá» ngÃ´n ngá»¯, cÃ¡ch viáº¿t táº¯t, hoáº·c cÃ¡ch trÃ¬nh bÃ y) thÃ nh má»™t tÃªn chuáº©n hÃ³a duy nháº¥t. \n\nVÃ­ dá»¥:\n- \"VNU University of Engineering and Technology\" â†’ `uet`\n- \"University of Science, Vietnam National University Hanoi\" â†’ `hus`\n\nMÃ´ hÃ¬nh sá»­ dá»¥ng kiáº¿n trÃºc BERT vÃ  Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u chá»©a cÃ¡c tÃªn cÆ¡ quan tá»« cÃ¡c bÃ i bÃ¡o khoa há»c.\n\n## ðŸ“Œ Chi tiáº¿t cÃ¡c mÃ´ hÃ¬nh\n\n| TÃªn model                                | Kiáº¿n trÃºc | Chá»©c nÄƒng chÃ­nh                                    |\n|------------------------------------------|-----------|---------------------------------------------------|\n| `thanhtlx/text_classification_2` | RobertaForSequenceClassification      | Chuáº©n hÃ³a tÃªn cÆ¡ quan thÃ nh má»™t trong 12 nhÃ£n chuáº©n |\n\n## ðŸ“¥ Äáº§u vÃ o\n\n- **Äá»‹nh dáº¡ng**: TÃªn cÆ¡ quan dÆ°á»›i dáº¡ng chuá»—i vÄƒn báº£n (cÃ³ thá»ƒ báº±ng tiáº¿ng Anh, tiáº¿ng Viá»‡t, hoáº·c viáº¿t táº¯t)\n- **Kiá»ƒu dá»¯ liá»‡u**: Chuá»—i vÄƒn báº£n (`str`)\n- **Xá»­ lÃ½**: VÄƒn báº£n sáº½ Ä‘Æ°á»£c mÃ£ hÃ³a bá»Ÿi tokenizer cá»§a mÃ´ hÃ¬nh (tá»± Ä‘á»™ng cáº¯t ngáº¯n vÃ  Ä‘á»‡m náº¿u cáº§n)\n\n**VÃ­ dá»¥ Ä‘áº§u vÃ o**:\n- \"VNU University of Engineering and Technology\"\n- \"ÄH BÃ¡ch Khoa HÃ  Ná»™i\"\n- \"University of Science, Vietnam National University Hanoi\"\n\n## ðŸ“¤ Äáº§u ra\n\n- Má»™t chuá»—i kÃ½ hiá»‡u tÆ°Æ¡ng á»©ng vá»›i tÃªn cÆ¡ quan chuáº©n hÃ³a:\n  - `smp`: School of Medicine and Pharmacy\n  - `ueb`: University of Economics and Business\n  - `ussh`: University of Social Sciences and Humanities\n  - `hus`: University of Science\n  - `hsb`: Hanoi School of Business\n  - `law`: School of Law\n  - `ulis`: University of Languages and International Studies\n  - `vju`: Vietnam Japan University\n  - `uet`: University of Engineering and Technology\n  - `ued`: University of Education\n  - `is`: International School\n  - `iti`: Information Technology Institute\n  - `other`: Others\n\n**VÃ­ dá»¥**:\n- \"VNU University of Engineering and Technology\" â†’ `uet`\n- \"University of Science, Vietnam National University Hanoi\" â†’ `hus`\n\nðŸ§ª Sá»­ dá»¥ng mÃ´ hÃ¬nh\nDÆ°á»›i Ä‘Ã¢y lÃ  Ä‘oáº¡n mÃ£ máº«u Ä‘á»ƒ sá»­ dá»¥ng mÃ´ hÃ¬nh:\n```python\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\nimport tensorflow as tf\n\n# Táº£i tokenizer vÃ  mÃ´ hÃ¬nh\ntokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_2\")\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_2\")\n\n# TÃªn cÆ¡ quan cáº§n chuáº©n hÃ³a\ntext = \"VNU University of Engineering and Technology\"\n\n# MÃ£ hÃ³a vÄƒn báº£n\ninputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n\n# Dá»± Ä‘oÃ¡n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# Chuyá»ƒn logits thÃ nh xÃ¡c suáº¥t\nprobs = tf.nn.softmax(logits, axis=1)\n\n# Láº¥y nhÃ£n cÃ³ xÃ¡c suáº¥t cao nháº¥t\npredicted_label = tf.argmax(probs, axis=1).numpy()[0]\nprint(f\"NhÃ£n dá»± Ä‘oÃ¡n: {model.config.id2label[predicted_label]}\")\n```\n", "downloads": 88, "likes": 0, "meta": {}, "inference_type": "local"}
{"id": "papluca/xlm-roberta-base-language-detection", "pipeline_tag": "text-classification", "tags": ["transformers", "pytorch", "tf", "safetensors", "xlm-roberta", "text-classification", "generated_from_trainer", "multilingual", "ar", "bg", "de", "el", "en", "es", "fr", "hi", "it", "ja", "nl", "pl", "pt", "ru", "sw", "th", "tr", "ur", "vi", "zh", "dataset:papluca/language-identification", "arxiv:1911.02116", "base_model:FacebookAI/xlm-roberta-base", "base_model:finetune:FacebookAI/xlm-roberta-base", "doi:10.57967/hf/2064", "license:mit", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage:\n- multilingual\n- ar\n- bg\n- de\n- el\n- en\n- es\n- fr\n- hi\n- it\n- ja\n- nl\n- pl\n- pt\n- ru\n- sw\n- th\n- tr\n- ur\n- vi\n- zh\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets: papluca/language-identification\nmetrics:\n- accuracy\n- f1\nbase_model: xlm-roberta-base\nmodel-index:\n- name: xlm-roberta-base-language-detection\n  results: []\n---\n\n# xlm-roberta-base-language-detection\n\nThis model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) on the [Language Identification](https://huggingface.co/datasets/papluca/language-identification#additional-information) dataset.\n\n## Model description\n\nThis model is an XLM-RoBERTa transformer model with a classification head on top (i.e. a linear layer on top of the pooled output). \nFor additional information please refer to the [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) model card or to the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al.\n\n## Intended uses & limitations\n\nYou can directly use this model as a language detector, i.e. for sequence classification tasks. Currently, it supports the following 20 languages: \n\n`arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)`\n\n## Training and evaluation data\n\nThe model was fine-tuned on the [Language Identification](https://huggingface.co/datasets/papluca/language-identification#additional-information) dataset, which consists of text sequences in 20 languages. The training set contains 70k samples, while the validation and test sets 10k each. The average accuracy on the test set is **99.6%** (this matches the average macro/weighted F1-score being the test set perfectly balanced). A more detailed evaluation is provided by the following table.\n\n| Language | Precision | Recall | F1-score | support |\n|:--------:|:---------:|:------:|:--------:|:-------:|\n|ar        |0.998      |0.996   |0.997     |500      |\n|bg        |0.998      |0.964   |0.981     |500      |\n|de        |0.998      |0.996   |0.997     |500      |\n|el        |0.996      |1.000   |0.998     |500      |\n|en        |1.000      |1.000   |1.000     |500      |\n|es        |0.967      |1.000   |0.983     |500      |\n|fr        |1.000      |1.000   |1.000     |500      |\n|hi        |0.994      |0.992   |0.993     |500      |\n|it        |1.000      |0.992   |0.996     |500      |\n|ja        |0.996      |0.996   |0.996     |500      |\n|nl        |1.000      |1.000   |1.000     |500      |\n|pl        |1.000      |1.000   |1.000     |500      |\n|pt        |0.988      |1.000   |0.994     |500      |\n|ru        |1.000      |0.994   |0.997     |500      |\n|sw        |1.000      |1.000   |1.000     |500      |\n|th        |1.000      |0.998   |0.999     |500      |\n|tr        |0.994      |0.992   |0.993     |500      |\n|ur        |1.000      |1.000   |1.000     |500      |\n|vi        |0.992      |1.000   |0.996     |500      |\n|zh        |1.000      |1.000   |1.000     |500      |\n\n### Benchmarks\n\nAs a baseline to compare `xlm-roberta-base-language-detection` against, we have used the Python [langid](https://github.com/saffsd/langid.py) library. Since it comes pre-trained on 97 languages, we have used its `.set_languages()` method to constrain the language set to our 20 languages. The average accuracy of langid on the test set is **98.5%**. More details are provided by the table below.\n\n| Language | Precision | Recall | F1-score | support |\n|:--------:|:---------:|:------:|:--------:|:-------:|\n|ar        |0.990      |0.970   |0.980     |500      |\n|bg        |0.998      |0.964   |0.981     |500      |\n|de        |0.992      |0.944   |0.967     |500      |\n|el        |1.000      |0.998   |0.999     |500      |\n|en        |1.000      |1.000   |1.000     |500      |\n|es        |1.000      |0.968   |0.984     |500      |\n|fr        |0.996      |1.000   |0.998     |500      |\n|hi        |0.949      |0.976   |0.963     |500      |\n|it        |0.990      |0.980   |0.985     |500      |\n|ja        |0.927      |0.988   |0.956     |500      |\n|nl        |0.980      |1.000   |0.990     |500      |\n|pl        |0.986      |0.996   |0.991     |500      |\n|pt        |0.950      |0.996   |0.973     |500      |\n|ru        |0.996      |0.974   |0.985     |500      |\n|sw        |1.000      |1.000   |1.000     |500      |\n|th        |1.000      |0.996   |0.998     |500      |\n|tr        |0.990      |0.968   |0.979     |500      |\n|ur        |0.998      |0.996   |0.997     |500      |\n|vi        |0.971      |0.990   |0.980     |500      |\n|zh        |1.000      |1.000   |1.000     |500      |\n\n## How to get started with the model\n\nThe easiest way to use the model is via the high-level `pipeline` API:\n\n```python\nfrom transformers import pipeline\n\ntext = [\n    \"Brevity is the soul of wit.\",\n    \"Amor, ch'a nullo amato amar perdona.\"\n]\n\nmodel_ckpt = \"papluca/xlm-roberta-base-language-detection\"\npipe = pipeline(\"text-classification\", model=model_ckpt)\npipe(text, top_k=1, truncation=True)\n```\n\nOr one can proceed with the tokenizer and model separately:\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntext = [\n    \"Brevity is the soul of wit.\",\n    \"Amor, ch'a nullo amato amar perdona.\"\n]\n\nmodel_ckpt = \"papluca/xlm-roberta-base-language-detection\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt)\n\ninputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npreds = torch.softmax(logits, dim=-1)\n\n# Map raw predictions to languages\nid2lang = model.config.id2label\nvals, idxs = torch.max(preds, dim=1)\n{id2lang[k.item()]: v.item() for k, v in zip(idxs, vals)}\n```\n\n## Training procedure\n\nFine-tuning was done via the `Trainer` API. Here is the [Colab notebook](https://colab.research.google.com/drive/15LJTckS6gU3RQOmjLqxVNBmbsBdnUEvl?usp=sharing) with the training code.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 128\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n- mixed_precision_training: Native AMP\n\n### Training results\n\nThe validation results on the `valid` split of the Language Identification dataset are summarised here below.\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:------:|\n| 0.2492        | 1.0   | 1094 | 0.0149          | 0.9969   | 0.9969 |\n| 0.0101        | 2.0   | 2188 | 0.0103          | 0.9977   | 0.9977 |\n\nIn short, it achieves the following results on the validation set:\n- Loss: 0.0101\n- Accuracy: 0.9977\n- F1: 0.9977\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n", "downloads": 685973, "likes": 343, "meta": {"base_model": "xlm-roberta-base", "datasets": "papluca/language-identification", "language": ["multilingual", "ar", "bg", "de", "el", "en", "es", "fr", "hi", "it", "ja", "nl", "pl", "pt", "ru", "sw", "th", "tr", "ur", "vi", "zh"], "license": "mit", "metrics": ["accuracy", "f1"], "tags": ["generated_from_trainer"], "model-index": [{"name": "xlm-roberta-base-language-detection", "results": []}]}, "inference_type": "local"}
{"id": "MCG-NJU/videomae-base-finetuned-kinetics", "pipeline_tag": "video-classification", "tags": ["transformers", "pytorch", "safetensors", "videomae", "video-classification", "vision", "arxiv:2203.12602", "arxiv:2111.06377", "license:cc-by-nc-4.0", "endpoints_compatible", "region:us"], "description": "---\nlicense: cc-by-nc-4.0\ntags:\n- vision\n- video-classification\n---\n\n# VideoMAE (base-sized model, fine-tuned on Kinetics-400) \n\nVideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Tong et al. and first released in [this repository](https://github.com/MCG-NJU/VideoMAE). \n\nDisclaimer: The team releasing VideoMAE did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nVideoMAE is an extension of [Masked Autoencoders (MAE)](https://arxiv.org/abs/2111.06377) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches.\n\nVideos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nBy pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\n\n## Intended uses & limitations\n\nYou can use the raw model for video classification into one of the 400 possible Kinetics-400 labels.\n\n### How to use\n\nHere is how to use this model to classify a video:\n\n```python\nfrom transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\nimport numpy as np\nimport torch\n\nvideo = list(np.random.randn(16, 3, 224, 224))\n\nprocessor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\nmodel = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n\ninputs = processor(video, return_tensors=\"pt\")\n\nwith torch.no_grad():\n  outputs = model(**inputs)\n  logits = outputs.logits\n\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/transformers/main/model_doc/videomae.html#).\n\n## Training data\n\n(to do, feel free to open a PR)\n\n## Training procedure\n\n### Preprocessing\n\n(to do, feel free to open a PR)\n\n### Pretraining\n\n(to do, feel free to open a PR)\n\n## Evaluation results\n\nThis model obtains a top-1 accuracy of 80.9 and a top-5 accuracy of 94.7 on the test set of Kinetics-400.\n\n### BibTeX entry and citation info\n\n```bibtex\nmisc{https://doi.org/10.48550/arxiv.2203.12602,\n  doi = {10.48550/ARXIV.2203.12602},\n  url = {https://arxiv.org/abs/2203.12602},\n  author = {Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},\n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  title = {VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```", "downloads": 139223, "likes": 36, "meta": {"license": "cc-by-nc-4.0", "tags": ["vision", "video-classification"]}, "inference_type": "local"}
{"id": "MIT/ast-finetuned-audioset-10-10-0.4593", "pipeline_tag": "audio-classification", "tags": ["transformers", "pytorch", "safetensors", "audio-spectrogram-transformer", "audio-classification", "arxiv:2104.01778", "license:bsd-3-clause", "endpoints_compatible", "region:us"], "description": "---\nlicense: bsd-3-clause\ntags:\n- audio-classification\n---\n\n# Audio Spectrogram Transformer (fine-tuned on AudioSet) \n\nAudio Spectrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Gong et al. and first released in [this repository](https://github.com/YuanGongND/ast). \n\nDisclaimer: The team releasing Audio Spectrogram Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Audio Spectrogram Transformer is equivalent to [ViT](https://huggingface.co/docs/transformers/model_doc/vit), but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.\n\n## Usage\n\nYou can use the raw model for classifying audio into one of the AudioSet classes. See the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification.forward.example) for more info.", "downloads": 1696278, "likes": 317, "meta": {"license": "bsd-3-clause", "tags": ["audio-classification"]}, "inference_type": "local"}
{"id": "firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3", "pipeline_tag": "audio-classification", "tags": ["transformers", "safetensors", "whisper", "audio-classification", "generated_from_trainer", "base_model:openai/whisper-large-v3", "base_model:finetune:openai/whisper-large-v3", "license:apache-2.0", "endpoints_compatible", "region:us"], "description": "---\nlibrary_name: transformers\nlicense: apache-2.0\nbase_model: openai/whisper-large-v3\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\n- precision\n- recall\n- f1\nmodel-index:\n- name: speech-emotion-recognition-with-openai-whisper-large-v3\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n\n# ðŸŽ§ **Speech Emotion Recognition with Whisper**\nThis project leverages the **Whisper** model to recognize emotions in speech. The goal is to classify audio recordings into different emotional categories, such as **Happy**, **Sad**, **Surprised**, and etc.\n\n\n## ðŸ—‚ **Dataset**\nThe dataset used for training and evaluation is sourced from multiple datasets, including:\n- [RAVDESS](https://zenodo.org/records/1188976#.XsAXemgzaUk)\n- [SAVEE](https://www.kaggle.com/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee/data)\n- [TESS](https://tspace.library.utoronto.ca/handle/1807/24487)\n- [URDU](https://www.kaggle.com/datasets/bitlord/urdu-language-speech-dataset)\n\nThe dataset contains recordings labeled with various emotions. Below is the distribution of the emotions in the dataset:\n| **Emotion** | **Count** |\n|-------------|-----------|\n| sad         | 752       |\n| happy       | 752       |\n| angry       | 752       |\n| neutral     | 716       |\n| disgust     | 652       |\n| fearful     | 652       |\n| surprised   | 652       |\n| calm        | 192       |\n\nThis distribution reflects the balance of emotions in the dataset, with some emotions having more samples than others. Excluded the \"calm\" emotion during training due to its underrepresentation.\n\n\n## ðŸŽ¤ **Preprocessing**\n- **Audio Loading**: Using **Librosa** to load the audio files and convert them to numpy arrays.\n- **Feature Extraction**: The audio data is processed using the **Whisper Feature Extractor**, which standardizes and normalizes the audio features for input to the model.\n\n\n## ðŸ”§ **Model**\nThe model used is the **Whisper Large V3** model, fine-tuned for **audio classification** tasks:\n- **Model**: [openai/whisper-large-v3](https://huggingface.co/openai/whisper-large-v3) \n- **Output**: Emotion labels (`Angry', 'Disgust', 'Fearful', 'Happy', 'Neutral', 'Sad', 'Surprised'`)\n  \nI map the emotion labels to numeric IDs and use them for model training and evaluation.\n\n\n## âš™ï¸ **Training**\nThe model is trained with the following parameters:\n- **Learning Rate**: `5e-05`  \n- **Train Batch Size**: `2`\n- **Eval Batch Size**: `2`\n- **Random Seed**: `42`  \n- **Gradient Accumulation Steps**: `5`  \n- **Total Train Batch Size**: `10` (effective batch size after gradient accumulation)\n- **Optimizer**: **Adam** with parameters: `betas=(0.9, 0.999)` and `epsilon=1e-08`\n- **Learning Rate Scheduler**: `linear`\n- **Warmup Ratio for LR Scheduler**: `0.1`\n- **Number of Epochs**: `25`\n- **Mixed Precision Training**: Native AMP (Automatic Mixed Precision)\n  \nThese parameters ensure efficient model training and stability, especially when dealing with large datasets and deep models like **Whisper**.\nThe training utilizes **Wandb** for experiment tracking and monitoring.\n\n\n## ðŸ“Š **Metrics**\nThe following evaluation metrics were obtained after training the model:\n- **Loss**: `0.5008`\n- **Accuracy**: `0.9199`\n- **Precision**: `0.9230`\n- **Recall**: `0.9199`\n- **F1 Score**: `0.9198`\n  \nThese metrics demonstrate the model's performance on the speech emotion recognition task. The high values for accuracy, precision, recall, and F1 score indicate that the model is effectively identifying emotional states from speech data.\n\n\n## ðŸ§ª **Results**\nAfter training, the model is evaluated on the test dataset, and the results are monitored using **Wandb** in this [Link](https://wandb.ai/firdhoworking-sepuluh-nopember-institute-of-technology/speech-emotion-recognition-with-whisper?nw=nwuserfirdhoworking).\n| Training Loss | Epoch   | Step | Validation Loss | Accuracy | Precision | Recall | F1     |\n|:-------------:|:-------:|:----:|:---------------:|:--------:|:---------:|:------:|:------:|\n| 0.4948        | 0.9995  | 394  | 0.4911          | 0.8286   | 0.8449    | 0.8286 | 0.8302 |\n| 0.6271        | 1.9990  | 788  | 0.5307          | 0.8225   | 0.8559    | 0.8225 | 0.8277 |\n| 0.2364        | 2.9985  | 1182 | 0.5076          | 0.8692   | 0.8727    | 0.8692 | 0.8684 |\n| 0.0156        | 3.9980  | 1576 | 0.5669          | 0.8732   | 0.8868    | 0.8732 | 0.8745 |\n| 0.2305        | 5.0     | 1971 | 0.4578          | 0.9108   | 0.9142    | 0.9108 | 0.9114 |\n| 0.0112        | 5.9995  | 2365 | 0.4701          | 0.9108   | 0.9159    | 0.9108 | 0.9114 |\n| 0.0013        | 6.9990  | 2759 | 0.5232          | 0.9138   | 0.9204    | 0.9138 | 0.9137 |\n| 0.1894        | 7.9985  | 3153 | 0.5008          | 0.9199   | 0.9230    | 0.9199 | 0.9198 |\n| 0.0877        | 8.9980  | 3547 | 0.5517          | 0.9138   | 0.9152    | 0.9138 | 0.9138 |\n| 0.1471        | 10.0    | 3942 | 0.5856          | 0.8895   | 0.9002    | 0.8895 | 0.8915 |\n| 0.0026        | 10.9995 | 4336 | 0.8334          | 0.8773   | 0.8949    | 0.8773 | 0.8770 |\n\n\n## ðŸš€ **How to Use**\n```python\nfrom transformers import AutoModelForAudioClassification, AutoFeatureExtractor\nimport librosa\nimport torch\nimport numpy as np\n\nmodel_id = \"firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3\"\nmodel = AutoModelForAudioClassification.from_pretrained(model_id)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_id, do_normalize=True)\nid2label = model.config.id2label\n```\n```python\ndef preprocess_audio(audio_path, feature_extractor, max_duration=30.0):\n    audio_array, sampling_rate = librosa.load(audio_path, sr=feature_extractor.sampling_rate)\n    \n    max_length = int(feature_extractor.sampling_rate * max_duration)\n    if len(audio_array) > max_length:\n        audio_array = audio_array[:max_length]\n    else:\n        audio_array = np.pad(audio_array, (0, max_length - len(audio_array)))\n\n    inputs = feature_extractor(\n        audio_array,\n        sampling_rate=feature_extractor.sampling_rate,\n        max_length=max_length,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    return inputs\n```\n```python\ndef predict_emotion(audio_path, model, feature_extractor, id2label, max_duration=30.0):\n    inputs = preprocess_audio(audio_path, feature_extractor, max_duration)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_id = torch.argmax(logits, dim=-1).item()\n    predicted_label = id2label[predicted_id]\n    \n    return predicted_label\n```\n```python\naudio_path = \"/content/drive/MyDrive/Audio/Speech_URDU/Happy/SM5_F4_H058.wav\"\n\npredicted_emotion = predict_emotion(audio_path, model, feature_extractor, id2label)\nprint(f\"Predicted Emotion: {predicted_emotion}\")\n```\n\n## ðŸŽ¯ Framework versions\n- Transformers 4.44.2\n- Pytorch 2.4.1+cu121\n- Datasets 3.0.0\n- Tokenizers 0.19.1\n", "downloads": 71802, "likes": 56, "meta": {"base_model": "openai/whisper-large-v3", "library_name": "transformers", "license": "apache-2.0", "metrics": ["accuracy", "precision", "recall", "f1"], "tags": ["generated_from_trainer"], "model-index": [{"name": "speech-emotion-recognition-with-openai-whisper-large-v3", "results": []}]}, "inference_type": "local"}
{"id": "DunnBC22/wav2vec2-base-Speech_Emotion_Recognition", "pipeline_tag": "audio-classification", "tags": ["transformers", "pytorch", "tensorboard", "wav2vec2", "audio-classification", "generated_from_trainer", "en", "dataset:audiofolder", "license:apache-2.0", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- audiofolder\nmetrics:\n- accuracy\n- f1\n- recall\n- precision\nlanguage:\n- en\npipeline_tag: audio-classification\nmodel-index:\n- name: wav2vec2-base-Speech_Emotion_Recognition\n  results: []\n---\n\n# wav2vec2-base-Speech_Emotion_Recognition\n\nThis model is a fine-tuned version of [facebook/wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base).\n\nIt achieves the following results on the evaluation set:\n- Loss: 0.7264\n- Accuracy: 0.7539\n- F1\n  - Weighted: 0.7514\n  - Micro: 0.7539\n  - Macro: 0.7529\n- Recall\n  - Weighted: 0.7539\n  - Micro: 0.7539\n  - Macro: 0.7577\n- Precision\n  - Weighted: 0.7565\n  - Micro: 0.7539\n  - Macro: 0.7558\n\n## Model description\n\nThis model predicts the emotion of the person speaking in the audio sample.\n\nFor more information on how it was created, check out the following link:  https://github.com/DunnBC22/Vision_Audio_and_Multimodal_Projects/tree/main/Audio-Projects/Emotion%20Detection/Speech%20Emotion%20Detection\n\n## Intended uses & limitations\n\nThis model is intended to demonstrate my ability to solve a complex problem using technology.\n\n## Training and evaluation data\n\nDataset Source: https://www.kaggle.com/datasets/dmitrybabko/speech-emotion-recognition-en\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 128\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy | Weighted F1 | Micro F1 | Macro F1 | Weighted Recall | Micro Recall | Macro Recall | Weighted Precision | Micro Precision | Macro Precision |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:-----------:|:--------:|:--------:|:---------------:|:------------:|:------------:|:------------------:|:---------------:|:---------------:|\n| 1.5581        | 0.98  | 43   | 1.4046          | 0.4653   | 0.4080      | 0.4653   | 0.4174   | 0.4653          | 0.4653       | 0.4793       | 0.5008             | 0.4653          | 0.4974          |\n| 1.5581        | 1.98  | 86   | 1.1566          | 0.5997   | 0.5836      | 0.5997   | 0.5871   | 0.5997          | 0.5997       | 0.6093       | 0.6248             | 0.5997          | 0.6209          |\n| 1.5581        | 2.98  | 129  | 0.9733          | 0.6883   | 0.6845      | 0.6883   | 0.6860   | 0.6883          | 0.6883       | 0.6923       | 0.7012             | 0.6883          | 0.7009          |\n| 1.5581        | 3.98  | 172  | 0.8313          | 0.7399   | 0.7392      | 0.7399   | 0.7409   | 0.7399          | 0.7399       | 0.7417       | 0.7415             | 0.7399          | 0.7432          |\n| 1.5581        | 4.98  | 215  | 0.8708          | 0.7028   | 0.6963      | 0.7028   | 0.6970   | 0.7028          | 0.7028       | 0.7081       | 0.7148             | 0.7028          | 0.7114          |\n| 1.5581        | 5.98  | 258  | 0.7969          | 0.7297   | 0.7267      | 0.7297   | 0.7277   | 0.7297          | 0.7297       | 0.7333       | 0.7393             | 0.7297          | 0.7382          |\n| 1.5581        | 6.98  | 301  | 0.7349          | 0.7603   | 0.7613      | 0.7603   | 0.7631   | 0.7603          | 0.7603       | 0.7635       | 0.7699             | 0.7603          | 0.7702          |\n| 1.5581        | 7.98  | 344  | 0.7714          | 0.7469   | 0.7444      | 0.7469   | 0.7456   | 0.7469          | 0.7469       | 0.7485       | 0.7554             | 0.7469          | 0.7563          |\n| 1.5581        | 8.98  | 387  | 0.7183          | 0.7630   | 0.7615      | 0.7630   | 0.7631   | 0.7630          | 0.7630       | 0.7652       | 0.7626             | 0.7630          | 0.7637          |\n| 1.5581        | 9.98  | 430  | 0.7264          | 0.7539   | 0.7514      | 0.7539   | 0.7529   | 0.7539          | 0.7539       | 0.7577       | 0.7565             | 0.7539          | 0.7558          |\n\n\n### Framework versions\n\n- Transformers 4.26.1\n- Pytorch 2.0.0+cu118\n- Datasets 2.11.0\n- Tokenizers 0.13.3", "downloads": 1930, "likes": 13, "meta": {"datasets": ["audiofolder"], "language": ["en"], "license": "apache-2.0", "metrics": ["accuracy", "f1", "recall", "precision"], "pipeline_tag": "audio-classification", "tags": ["generated_from_trainer"], "model-index": [{"name": "wav2vec2-base-Speech_Emotion_Recognition", "results": []}]}, "inference_type": "local"}
{"id": "dima806/music_genres_classification", "pipeline_tag": "audio-classification", "tags": ["transformers", "pytorch", "safetensors", "wav2vec2", "audio-classification", "base_model:facebook/wav2vec2-base-960h", "base_model:finetune:facebook/wav2vec2-base-960h", "license:apache-2.0", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\nmetrics:\n- accuracy\n- roc_auc\nbase_model:\n- facebook/wav2vec2-base-960h\n---\n[Music genre](https://en.wikipedia.org/wiki/Music_genre) classification is a fundamental and versatile application in many various domains. Some possible use cases for music genre classification include:\n\n- music recommendation systems;\n- content organization and discovery;\n- radio broadcasting and programming;\n- music licensing and copyright management;\n- music analysis and research;\n- content tagging and metadata enrichment;\n- audio identification and copyright protection;\n- music production and creativity;\n- healthcare and therapy;\n- entertainment and gaming.\n\nThe model is trained based on publicly available dataset of labeled music data â€” [GTZAN Dataset](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification) â€” that contains 1000 sample 30-second audio files evenly split among 10 genres:\n\n- blues;\n- classical;\n- country;\n- disco;\n- hip-hop;\n- jazz;\n- metal;\n- pop;\n- reggae;\n- rock.\n\nThe final code is available as a [Kaggle notebook](https://www.kaggle.com/code/dima806/music-genre-classification-wav2vec2-base-960h).\nSee also [my Medium article](https://medium.com/data-and-beyond/building-a-free-advanced-music-genre-classification-pipeline-using-machine-learning-654b0de7cc3e)  for more details.", "downloads": 1635, "likes": 30, "meta": {"base_model": ["facebook/wav2vec2-base-960h"], "license": "apache-2.0", "metrics": ["accuracy", "roc_auc"]}, "inference_type": "local"}
{"id": "anton-l/wav2vec2-base-lang-id", "pipeline_tag": "audio-classification", "tags": ["transformers", "pytorch", "tensorboard", "wav2vec2", "audio-classification", "generated_from_trainer", "dataset:common_language", "license:apache-2.0", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\ntags:\n- audio-classification\n- generated_from_trainer\ndatasets:\n- common_language\nmetrics:\n- accuracy\nmodel-index:\n- name: wav2vec2-base-lang-id\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# wav2vec2-base-lang-id\n\nThis model is a fine-tuned version of [facebook/wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base) on the anton-l/common_language dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.9836\n- Accuracy: 0.7945\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 32\n- eval_batch_size: 4\n- seed: 0\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 128\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 10.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|\n| 2.9568        | 1.0   | 173  | 3.2866          | 0.1146   |\n| 1.9243        | 2.0   | 346  | 2.1241          | 0.3840   |\n| 1.2923        | 3.0   | 519  | 1.5498          | 0.5489   |\n| 0.8659        | 4.0   | 692  | 1.4953          | 0.6126   |\n| 0.5539        | 5.0   | 865  | 1.2431          | 0.6926   |\n| 0.4101        | 6.0   | 1038 | 1.1443          | 0.7232   |\n| 0.2945        | 7.0   | 1211 | 1.0870          | 0.7544   |\n| 0.1552        | 8.0   | 1384 | 1.1080          | 0.7661   |\n| 0.0968        | 9.0   | 1557 | 0.9836          | 0.7945   |\n| 0.0623        | 10.0  | 1730 | 1.0252          | 0.7993   |\n\n\n### Framework versions\n\n- Transformers 4.11.0.dev0\n- Pytorch 1.9.1+cu111\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n", "downloads": 1588, "likes": 8, "meta": {"datasets": ["common_language"], "license": "apache-2.0", "metrics": ["accuracy"], "tags": ["audio-classification", "generated_from_trainer"], "model-index": [{"name": "wav2vec2-base-lang-id", "results": []}]}, "inference_type": "local"}
{"id": "Helsinki-NLP/opus-mt-fr-en", "pipeline_tag": "translation", "tags": ["transformers", "pytorch", "tf", "jax", "marian", "text2text-generation", "translation", "fr", "en", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-fr-en\n\n* source languages: fr\n* target languages: en\n*  OPUS readme: [fr-en](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/fr-en/README.md)\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.zip)\n* test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.test.txt)\n* test set scores: [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/fr-en/opus-2020-02-26.eval.txt)\n\n## Benchmarks\n\n| testset               | BLEU  | chr-F |\n|-----------------------|-------|-------|\n| newsdiscussdev2015-enfr.fr.en \t| 33.1 \t| 0.580 |\n| newsdiscusstest2015-enfr.fr.en \t| 38.7 \t| 0.614 |\n| newssyscomb2009.fr.en \t| 30.3 \t| 0.569 |\n| news-test2008.fr.en \t| 26.2 \t| 0.542 |\n| newstest2009.fr.en \t| 30.2 \t| 0.570 |\n| newstest2010.fr.en \t| 32.2 \t| 0.590 |\n| newstest2011.fr.en \t| 33.0 \t| 0.597 |\n| newstest2012.fr.en \t| 32.8 \t| 0.591 |\n| newstest2013.fr.en \t| 33.9 \t| 0.591 |\n| newstest2014-fren.fr.en \t| 37.8 \t| 0.633 |\n| Tatoeba.fr.en \t| 57.5 \t| 0.720 |\n\n", "downloads": 931561, "likes": 47, "meta": {"license": "apache-2.0", "tags": ["translation"]}, "inference_type": "huggingface"}
{"id": "Helsinki-NLP/opus-mt-en-fr", "pipeline_tag": "translation", "tags": ["transformers", "pytorch", "tf", "jax", "marian", "text2text-generation", "translation", "en", "fr", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\npipeline_tag: translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-fr\n\n* source languages: en\n* target languages: fr\n*  OPUS readme: [en-fr](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/en-fr/README.md)\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-02-26.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.zip)\n* test set translations: [opus-2020-02-26.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.test.txt)\n* test set scores: [opus-2020-02-26.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-fr/opus-2020-02-26.eval.txt)\n\n## Benchmarks\n\n| testset               | BLEU  | chr-F |\n|-----------------------|-------|-------|\n| newsdiscussdev2015-enfr.en.fr \t| 33.8 \t| 0.602 |\n| newsdiscusstest2015-enfr.en.fr \t| 40.0 \t| 0.643 |\n| newssyscomb2009.en.fr \t| 29.8 \t| 0.584 |\n| news-test2008.en.fr \t| 27.5 \t| 0.554 |\n| newstest2009.en.fr \t| 29.4 \t| 0.577 |\n| newstest2010.en.fr \t| 32.7 \t| 0.596 |\n| newstest2011.en.fr \t| 34.3 \t| 0.611 |\n| newstest2012.en.fr \t| 31.8 \t| 0.592 |\n| newstest2013.en.fr \t| 33.2 \t| 0.589 |\n| Tatoeba.en.fr \t| 50.5 \t| 0.672 |", "downloads": 312177, "likes": 63, "meta": {"license": "apache-2.0", "pipeline_tag": "translation"}, "inference_type": "huggingface"}
{"id": "Helsinki-NLP/opus-mt-en-zh", "pipeline_tag": "translation", "tags": ["transformers", "pytorch", "tf", "jax", "rust", "marian", "text2text-generation", "translation", "en", "zh", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage:\n- en\n- zh\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### eng-zho\n\n* source group: English \n* target group: Chinese \n*  OPUS readme: [eng-zho](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-zho/README.md)\n\n*  model: transformer\n* source language(s): eng\n* target language(s): cjy_Hans cjy_Hant cmn cmn_Hans cmn_Hant gan lzh lzh_Hans nan wuu yue yue_Hans yue_Hant\n* model: transformer\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* a sentence initial language token is required in the form of `>>id<<` (id = valid target language ID)\n* download original weights: [opus-2020-07-17.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip)\n* test set translations: [opus-2020-07-17.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.test.txt)\n* test set scores: [opus-2020-07-17.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.eval.txt)\n\n## Benchmarks\n\n| testset               | BLEU  | chr-F |\n|-----------------------|-------|-------|\n| Tatoeba-test.eng.zho \t| 31.4 \t| 0.268 |\n\n\n### System Info: \n- hf_name: eng-zho\n\n- source_languages: eng\n\n- target_languages: zho\n\n- opus_readme_url: https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-zho/README.md\n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['en', 'zh']\n\n- src_constituents: {'eng'}\n\n- tgt_constituents: {'cmn_Hans', 'nan', 'nan_Hani', 'gan', 'yue', 'cmn_Kana', 'yue_Hani', 'wuu_Bopo', 'cmn_Latn', 'yue_Hira', 'cmn_Hani', 'cjy_Hans', 'cmn', 'lzh_Hang', 'lzh_Hira', 'cmn_Hant', 'lzh_Bopo', 'zho', 'zho_Hans', 'zho_Hant', 'lzh_Hani', 'yue_Hang', 'wuu', 'yue_Kana', 'wuu_Latn', 'yue_Bopo', 'cjy_Hant', 'yue_Hans', 'lzh', 'cmn_Hira', 'lzh_Yiii', 'lzh_Hans', 'cmn_Bopo', 'cmn_Hang', 'hak_Hani', 'cmn_Yiii', 'yue_Hant', 'lzh_Kana', 'wuu_Hani'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.zip\n\n- url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-zho/opus-2020-07-17.test.txt\n\n- src_alpha3: eng\n\n- tgt_alpha3: zho\n\n- short_pair: en-zh\n\n- chrF2_score: 0.268\n\n- bleu: 31.4\n\n- brevity_penalty: 0.8959999999999999\n\n- ref_len: 110468.0\n\n- src_name: English\n\n- tgt_name: Chinese\n\n- train_date: 2020-07-17\n\n- src_alpha2: en\n\n- tgt_alpha2: zh\n\n- prefer_old: False\n\n- long_pair: eng-zho\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41\n", "downloads": 77786, "likes": 375, "meta": {"language": ["en", "zh"], "license": "apache-2.0", "tags": ["translation"]}, "inference_type": "huggingface"}
{"id": "Helsinki-NLP/opus-mt-zh-en", "pipeline_tag": "translation", "tags": ["transformers", "pytorch", "tf", "rust", "marian", "text2text-generation", "translation", "zh", "en", "license:cc-by-4.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage:\n- zh\n- en\ntags:\n- translation\nlicense: cc-by-4.0\n---\n\n### zho-eng\n\n## Table of Contents\n- [Model Details](#model-details)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Citation Information](#citation-information)\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n\n## Model Details\n- **Model Description:**\n- **Developed by:** Language Technology Research Group at the University of Helsinki\n- **Model Type:** Translation\n- **Language(s):**  \n  - Source Language:  Chinese\n  - Target Language: English\n- **License:** CC-BY-4.0\n- **Resources for more information:**\n  - [GitHub Repo](https://github.com/Helsinki-NLP/OPUS-MT-train)\n\n\n## Uses\n\n#### Direct Use\n\nThis model can be used for translation and text-to-text generation.\n\n\n## Risks, Limitations and Biases\n\n**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).\n\nFurther details about the dataset for this model can be found in the OPUS readme: [zho-eng](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/zho-eng/README.md)\n\n## Training\n\n#### System Information \n* helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n* transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n* port_machine: brutasse\n* port_time: 2020-08-21-14:41\n* src_multilingual: False\n* tgt_multilingual: False\n\n#### Training Data\n##### Preprocessing\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* ref_len: 82826.0\n* dataset: [opus](https://github.com/Helsinki-NLP/Opus-MT)\n* download original weights: [opus-2020-07-17.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/zho-eng/opus-2020-07-17.zip)\n\n* test set translations: [opus-2020-07-17.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/zho-eng/opus-2020-07-17.test.txt)\n\n\n## Evaluation\n\n#### Results\n\n* test set scores: [opus-2020-07-17.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/zho-eng/opus-2020-07-17.eval.txt)\n\n* brevity_penalty: 0.948\n\n\n## Benchmarks\n\n| testset               | BLEU  | chr-F |\n|-----------------------|-------|-------|\n| Tatoeba-test.zho.eng \t| 36.1 \t| 0.548 |\n\n## Citation Information\n\n```bibtex\n@InProceedings{TiedemannThottingal:EAMT2020,\n  author = {J{\\\"o}rg Tiedemann and Santhosh Thottingal},\n  title = {{OPUS-MT} â€” {B}uilding open translation services for the {W}orld},\n  booktitle = {Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)},\n  year = {2020},\n  address = {Lisbon, Portugal}\n }\n```\n\n## How to Get Started With the Model\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n```\n\n\n", "downloads": 377559, "likes": 519, "meta": {"language": ["zh", "en"], "license": "cc-by-4.0", "tags": ["translation"]}, "inference_type": "huggingface"}
{"id": "Helsinki-NLP/opus-mt-en-es", "pipeline_tag": "translation", "tags": ["transformers", "pytorch", "tf", "jax", "marian", "text2text-generation", "translation", "en", "es", "license:apache-2.0", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage:\n- en\n- es\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### eng-spa\n\n* source group: English \n* target group: Spanish \n*  OPUS readme: [eng-spa](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa/README.md)\n\n*  model: transformer\n* source language(s): eng\n* target language(s): spa\n* model: transformer\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus-2020-08-18.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.zip)\n* test set translations: [opus-2020-08-18.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.test.txt)\n* test set scores: [opus-2020-08-18.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.eval.txt)\n\n## Benchmarks\n\n| testset               | BLEU  | chr-F |\n|-----------------------|-------|-------|\n| newssyscomb2009-engspa.eng.spa \t| 31.0 \t| 0.583 |\n| news-test2008-engspa.eng.spa \t| 29.7 \t| 0.564 |\n| newstest2009-engspa.eng.spa \t| 30.2 \t| 0.578 |\n| newstest2010-engspa.eng.spa \t| 36.9 \t| 0.620 |\n| newstest2011-engspa.eng.spa \t| 38.2 \t| 0.619 |\n| newstest2012-engspa.eng.spa \t| 39.0 \t| 0.625 |\n| newstest2013-engspa.eng.spa \t| 35.0 \t| 0.598 |\n| Tatoeba-test.eng.spa \t| 54.9 \t| 0.721 |\n\n\n### System Info: \n- hf_name: eng-spa\n\n- source_languages: eng\n\n- target_languages: spa\n\n- opus_readme_url: https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-spa/README.md\n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['en', 'es']\n\n- src_constituents: {'eng'}\n\n- tgt_constituents: {'spa'}\n\n- src_multilingual: False\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.zip\n\n- url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.test.txt\n\n- src_alpha3: eng\n\n- tgt_alpha3: spa\n\n- short_pair: en-es\n\n- chrF2_score: 0.721\n\n- bleu: 54.9\n\n- brevity_penalty: 0.978\n\n- ref_len: 77311.0\n\n- src_name: English\n\n- tgt_name: Spanish\n\n- train_date: 2020-08-18 00:00:00\n\n- src_alpha2: en\n\n- tgt_alpha2: es\n\n- prefer_old: False\n\n- long_pair: eng-spa\n\n- helsinki_git_sha: d2f0910c89026c34a44e331e785dec1e0faa7b82\n\n- transformers_git_sha: f7af09b4524b784d67ae8526f0e2fcc6f5ed0de9\n\n- port_machine: brutasse\n\n- port_time: 2020-08-24-18:20", "downloads": 277436, "likes": 117, "meta": {"language": ["en", "es"], "license": "apache-2.0", "tags": ["translation"]}, "inference_type": "huggingface"}
{"id": "utrobinmv/t5_translate_en_ru_zh_large_1024", "pipeline_tag": "translation", "tags": ["transformers", "safetensors", "t5", "text2text-generation", "translation", "ru", "zh", "en", "dataset:ccmatrix", "license:apache-2.0", "autotrain_compatible", "text-generation-inference", "endpoints_compatible", "region:us"], "description": "---\nlanguage:\n- ru\n- zh\n- en\ntags:\n- translation\nlicense: apache-2.0\ndatasets:\n- ccmatrix\nmetrics:\n- sacrebleu\nwidget:\n- example_title: translate zh-ru\n  text: 'translate to ru: å¼€å‘çš„ç›®çš„æ˜¯ä¸ºç”¨æˆ·æä¾›ä¸ªäººåŒæ­¥ç¿»è¯‘ã€‚\n\n    '\n- example_title: translate ru-en\n  text: 'translate to en: Ð¦ÐµÐ»ÑŒ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ â€” Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÐ¼ Ð»Ð¸Ñ‡Ð½Ð¾Ð³Ð¾ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾Ð³Ð¾\n    Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‡Ð¸ÐºÐ°.\n\n    '\n- example_title: translate en-ru\n  text: 'translate to ru: The purpose of the development is to provide users with\n    a personal synchronized interpreter.\n\n    '\n- example_title: translate en-zh\n  text: 'translate to zh: The purpose of the development is to provide users with\n    a personal synchronized interpreter.\n\n    '\n- example_title: translate zh-en\n  text: 'translate to en: å¼€å‘çš„ç›®çš„æ˜¯ä¸ºç”¨æˆ·æä¾›ä¸ªäººåŒæ­¥è§£é‡Šå™¨ã€‚\n\n    '\n- example_title: translate ru-zh\n  text: 'translate to zh: Ð¦ÐµÐ»ÑŒ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ â€” Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÐ¼ Ð»Ð¸Ñ‡Ð½Ð¾Ð³Ð¾ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾Ð³Ð¾\n    Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‡Ð¸ÐºÐ°.'\n---\n\n# T5 English, Russian and Chinese multilingual machine translation\n\nThis model represents a conventional T5 transformer in multitasking mode for translation into the required language, precisely configured for machine translation for pairs: ru-zh, zh-ru, en-zh, zh-en, en-ru, ru-en.\n\nThe model can perform direct translation between any pair of Russian, Chinese or English languages. For translation into the target language, the target language identifier is specified as a prefix 'translate to <lang>:'. In this case, the source language may not be specified, in addition, the source text may be multilingual.\n\nExample translate Russian to Chinese\n\n```python\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\ndevice = 'cuda' #or 'cpu' for translate on cpu\n\nmodel_name = 'utrobinmv/t5_translate_en_ru_zh_large_1024'\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\nmodel.to(device)\ntokenizer = T5Tokenizer.from_pretrained(model_name)\n\nprefix = 'translate to zh: '\nsrc_text = prefix + \"Ð¡ÑŠÐµÑˆÑŒ ÐµÑ‰Ñ‘ ÑÑ‚Ð¸Ñ… Ð¼ÑÐ³ÐºÐ¸Ñ… Ñ„Ñ€Ð°Ð½Ñ†ÑƒÐ·ÑÐºÐ¸Ñ… Ð±ÑƒÐ»Ð¾Ðº.\"\n\n# translate Russian to Chinese\ninput_ids = tokenizer(src_text, return_tensors=\"pt\")\n\ngenerated_tokens = model.generate(**input_ids.to(device))\n\nresult = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\nprint(result)\n# å†åƒè¿™äº›æ³•å›½çš„ç”œèœœçš„é¢åŒ…ã€‚\n```\n\n\n\nand Example translate Chinese to Russian\n\n```python\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\ndevice = 'cuda' #or 'cpu' for translate on cpu\n\nmodel_name = 'utrobinmv/t5_translate_en_ru_zh_large_1024'\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\nmodel.to(device)\ntokenizer = T5Tokenizer.from_pretrained(model_name)\n\nprefix = 'translate to ru: '\nsrc_text = prefix + \"å†åƒè¿™äº›æ³•å›½çš„ç”œèœœçš„é¢åŒ…ã€‚\"\n\n# translate Russian to Chinese\ninput_ids = tokenizer(src_text, return_tensors=\"pt\")\n\ngenerated_tokens = model.generate(**input_ids.to(device))\n\nresult = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\nprint(result)\n# Ð¡ÑŠÐµÑˆÑŒÑ‚Ðµ ÑÑ‚Ð¾Ñ‚ ÑÐ»Ð°Ð´ÐºÐ¸Ð¹ Ñ…Ð»ÐµÐ± Ð¸Ð· Ð¤Ñ€Ð°Ð½Ñ†Ð¸Ð¸.\n```\n\n\n\n##  \n\n\n\n## Languages covered\n\nRussian (ru_RU), Chinese (zh_CN), English (en_US)\n", "downloads": 1125, "likes": 86, "meta": {"datasets": ["ccmatrix"], "language": ["ru", "zh", "en"], "license": "apache-2.0", "metrics": ["sacrebleu"], "tags": ["translation"], "widget": [{"example_title": "translate zh-ru", "text": "translate to ru: å¼€å‘çš„ç›®çš„æ˜¯ä¸ºç”¨æˆ·æä¾›ä¸ªäººåŒæ­¥ç¿»è¯‘ã€‚\n"}, {"example_title": "translate ru-en", "text": "translate to en: Ð¦ÐµÐ»ÑŒ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ â€” Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÐ¼ Ð»Ð¸Ñ‡Ð½Ð¾Ð³Ð¾ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‡Ð¸ÐºÐ°.\n"}, {"example_title": "translate en-ru", "text": "translate to ru: The purpose of the development is to provide users with a personal synchronized interpreter.\n"}, {"example_title": "translate en-zh", "text": "translate to zh: The purpose of the development is to provide users with a personal synchronized interpreter.\n"}, {"example_title": "translate zh-en", "text": "translate to en: å¼€å‘çš„ç›®çš„æ˜¯ä¸ºç”¨æˆ·æä¾›ä¸ªäººåŒæ­¥è§£é‡Šå™¨ã€‚\n"}, {"example_title": "translate ru-zh", "text": "translate to zh: Ð¦ÐµÐ»ÑŒ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ â€” Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÐ¼ Ð»Ð¸Ñ‡Ð½Ð¾Ð³Ð¾ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‡Ð¸ÐºÐ°.\n"}]}, "inference_type": "huggingface"}
{"id": "google-t5/t5-large", "pipeline_tag": "translation", "tags": ["transformers", "pytorch", "tf", "jax", "safetensors", "t5", "text2text-generation", "summarization", "translation", "en", "fr", "ro", "de", "multilingual", "dataset:c4", "arxiv:1805.12471", "arxiv:1708.00055", "arxiv:1704.05426", "arxiv:1606.05250", "arxiv:1808.09121", "arxiv:1810.12885", "arxiv:1905.10044", "arxiv:1910.09700", "license:apache-2.0", "autotrain_compatible", "text-generation-inference", "endpoints_compatible", "region:us"], "description": "---\nlanguage:\n- en\n- fr\n- ro\n- de\n- multilingual\nlicense: apache-2.0\ntags:\n- summarization\n- translation\ndatasets:\n- c4\n---\n\n# Model Card for T5 Large\n\n![model image](https://camo.githubusercontent.com/623b4dea0b653f2ad3f36c71ebfe749a677ac0a1/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f343030362f312a44304a31674e51663876727255704b657944387750412e706e67)\n\n#  Table of Contents\n\n1. [Model Details](#model-details)\n2. [Uses](#uses)\n3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n4. [Training Details](#training-details)\n5. [Evaluation](#evaluation)\n6. [Environmental Impact](#environmental-impact)\n7. [Citation](#citation)\n8. [Model Card Authors](#model-card-authors)\n9. [How To Get Started With the Model](#how-to-get-started-with-the-model)\n\n# Model Details\n\n## Model Description\n\nThe developers of the Text-To-Text Transfer Transformer (T5) [write](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html): \n\n> With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.\n\nT5-Large is the checkpoint with 770 million parameters. \n\n- **Developed by:** Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. See [associated paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) and [GitHub repo](https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints)\n- **Model type:** Language model\n- **Language(s) (NLP):** English, French, Romanian, German\n- **License:** Apache 2.0\n- **Related Models:** [All T5 Checkpoints](https://huggingface.co/models?search=t5)\n- **Resources for more information:**\n  - [Research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)\n  - [Google's T5 Blog Post](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) \n  - [GitHub Repo](https://github.com/google-research/text-to-text-transfer-transformer)\n  - [Hugging Face T5 Docs](https://huggingface.co/docs/transformers/model_doc/t5)\n  \n# Uses\n\n## Direct Use and Downstream Use\n\nThe developers write in a [blog post](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) that the model: \n\n> Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.\n\nSee the [blog post](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for further details.\n\n## Out-of-Scope Use\n\nMore information needed.\n\n# Bias, Risks, and Limitations\n\nMore information needed.\n\n## Recommendations\n\nMore information needed.\n\n# Training Details\n\n## Training Data\n\nThe model is pre-trained on the [Colossal Clean Crawled Corpus (C4)](https://www.tensorflow.org/datasets/catalog/c4), which was developed and released in the context of the same [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) as T5.\n\nThe model was pre-trained on a on a **multi-task mixture of unsupervised (1.) and supervised tasks (2.)**.\nThereby, the following datasets were being used for (1.) and (2.):\n\n1. **Datasets used for Unsupervised denoising objective**:\n\n- [C4](https://huggingface.co/datasets/c4)\n- [Wiki-DPR](https://huggingface.co/datasets/wiki_dpr)\n\n\n2. **Datasets used for Supervised text-to-text language modeling objective**\n\n- Sentence acceptability judgment\n  - CoLA [Warstadt et al., 2018](https://arxiv.org/abs/1805.12471)\n- Sentiment analysis \n  - SST-2 [Socher et al., 2013](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n- Paraphrasing/sentence similarity\n  - MRPC [Dolan and Brockett, 2005](https://aclanthology.org/I05-5002)\n  - STS-B [Ceret al., 2017](https://arxiv.org/abs/1708.00055)\n  - QQP [Iyer et al., 2017](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs)\n- Natural language inference\n  - MNLI [Williams et al., 2017](https://arxiv.org/abs/1704.05426)\n  - QNLI [Rajpurkar et al.,2016](https://arxiv.org/abs/1606.05250)\n  - RTE [Dagan et al., 2005](https://link.springer.com/chapter/10.1007/11736790_9) \n  - CB [De Marneff et al., 2019](https://semanticsarchive.net/Archive/Tg3ZGI2M/Marneffe.pdf)\n- Sentence completion\n  - COPA [Roemmele et al., 2011](https://www.researchgate.net/publication/221251392_Choice_of_Plausible_Alternatives_An_Evaluation_of_Commonsense_Causal_Reasoning)\n- Word sense disambiguation\n  - WIC [Pilehvar and Camacho-Collados, 2018](https://arxiv.org/abs/1808.09121)\n- Question answering\n  - MultiRC [Khashabi et al., 2018](https://aclanthology.org/N18-1023)\n  - ReCoRD [Zhang et al., 2018](https://arxiv.org/abs/1810.12885)\n  - BoolQ [Clark et al., 2019](https://arxiv.org/abs/1905.10044)\n\n## Training Procedure\n\nIn their [abstract](https://jmlr.org/papers/volume21/20-074/20-074.pdf), the model developers write: \n\n> In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. \n\nThe framework introduced, the T5 framework, involves a training procedure that brings together the approaches studied in the paper. See the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for further details.\n\n# Evaluation\n\n## Testing Data, Factors & Metrics\n\nThe developers evaluated the model on 24 tasks, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for full details.\n\n## Results \n\nFor full results for T5-Large, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf), Table 14.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Google Cloud TPU Pods\n- **Hours used:** More information needed\n- **Cloud Provider:** GCP\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@article{2020t5,\n  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n  journal = {Journal of Machine Learning Research},\n  year    = {2020},\n  volume  = {21},\n  number  = {140},\n  pages   = {1-67},\n  url     = {http://jmlr.org/papers/v21/20-074.html}\n}\n```\n\n**APA:**\n- Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.\n\n# Model Card Authors\n\nThis model card was written by the team at Hugging Face.\n\n# How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import T5Tokenizer, T5Model\n\ntokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\nmodel = T5Model.from_pretrained(\"t5-large\")\n\ninput_ids = tokenizer(\n    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n).input_ids  # Batch size 1\ndecoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n# forward pass\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state\n```\n\nSee the [Hugging Face T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model) docs and a [Colab Notebook](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb) created by the model developers for more examples.\n</details>\n", "downloads": 345697, "likes": 210, "meta": {"datasets": ["c4"], "language": ["en", "fr", "ro", "de", "multilingual"], "license": "apache-2.0", "tags": ["summarization", "translation"]}, "inference_type": "huggingface"}
{"id": "VietAI/envit5-translation", "pipeline_tag": "translation", "tags": ["transformers", "pytorch", "tf", "jax", "t5", "text2text-generation", "translation", "vi", "en", "dataset:cc100", "license:openrail", "autotrain_compatible", "text-generation-inference", "endpoints_compatible", "region:us"], "description": "---\nlanguage:\n- vi\n- en\ndatasets:\n- cc100\ntags:\n- translation\nwidget:\n- text: 'vi: VietAI lÃ  tá»• chá»©c phi lá»£i nhuáº­n vá»›i sá»© má»‡nh Æ°Æ¡m máº§m tÃ i nÄƒng vá» trÃ­ tuá»‡\n    nhÃ¢n táº¡o vÃ  xÃ¢y dá»±ng má»™t cá»™ng Ä‘á»“ng cÃ¡c chuyÃªn gia trong lÄ©nh vá»±c trÃ­ tuá»‡ nhÃ¢n\n    táº¡o Ä‘áº³ng cáº¥p quá»‘c táº¿ táº¡i Viá»‡t Nam.'\nlicense: openrail\n---\n\n# EnViT5 Translation\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/mtet-multi-domain-translation-for-english/machine-translation-on-iwslt2015-english-1)](https://paperswithcode.com/sota/machine-translation-on-iwslt2015-english-1?p=mtet-multi-domain-translation-for-english)\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/mtet-multi-domain-translation-for-english-and/on-phomt)](https://paperswithcode.com/sota/on-phomt?p=mtet-multi-domain-translation-for-english-and)\n\n\nState-of-the-art English-Vietnamese and Vietnamese-English Translation models trained on [MTet](https://research.vietai.org/mtet/), [PhoMT](https://github.com/VinAIResearch/PhoMT).\n\n\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n\nmodel_name = \"VietAI/envit5-translation\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)  \nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ninputs = [\n    \"vi: VietAI lÃ  tá»• chá»©c phi lá»£i nhuáº­n vá»›i sá»© má»‡nh Æ°Æ¡m máº§m tÃ i nÄƒng vá» trÃ­ tuá»‡ nhÃ¢n táº¡o vÃ  xÃ¢y dá»±ng má»™t cá»™ng Ä‘á»“ng cÃ¡c chuyÃªn gia trong lÄ©nh vá»±c trÃ­ tuá»‡ nhÃ¢n táº¡o Ä‘áº³ng cáº¥p quá»‘c táº¿ táº¡i Viá»‡t Nam.\",\n    \"vi: Theo bÃ¡o cÃ¡o má»›i nháº¥t cá»§a Linkedin vá» danh sÃ¡ch viá»‡c lÃ m triá»ƒn vá»ng vá»›i má»©c lÆ°Æ¡ng háº¥p dáº«n nÄƒm 2020, cÃ¡c chá»©c danh cÃ´ng viá»‡c liÃªn quan Ä‘áº¿n AI nhÆ° ChuyÃªn gia AI (Artificial Intelligence Specialist), Ká»¹ sÆ° ML (Machine Learning Engineer) Ä‘á»u xáº¿p thá»© háº¡ng cao.\",\n    \"en: Our teams aspire to make discoveries that impact everyone, and core to our approach is sharing our research and tools to fuel progress in the field.\",\n    \"en: We're on a journey to advance and democratize artificial intelligence through open source and open science.\"\n    ]\n\noutputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids.to('cuda'), max_length=512)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n\n# ['en: VietAI is a non-profit organization with the mission of nurturing artificial intelligence talents and building an international - class community of artificial intelligence experts in Vietnam.',\n#  'en: According to the latest LinkedIn report on the 2020 list of attractive and promising jobs, AI - related job titles such as AI Specialist, ML Engineer and ML Engineer all rank high.',\n#  'vi: NhÃ³m chÃºng tÃ´i khao khÃ¡t táº¡o ra nhá»¯ng khÃ¡m phÃ¡ cÃ³ áº£nh hÆ°á»Ÿng Ä‘áº¿n má»i ngÆ°á»i, vÃ  cá»‘t lÃµi trong cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i lÃ  chia sáº» nghiÃªn cá»©u vÃ  cÃ´ng cá»¥ Ä‘á»ƒ thÃºc Ä‘áº©y sá»± tiáº¿n bá»™ trong lÄ©nh vá»±c nÃ y.',\n#  'vi: ChÃºng ta Ä‘ang trÃªn hÃ nh trÃ¬nh tiáº¿n bá»™ vÃ  dÃ¢n chá»§ hoÃ¡ trÃ­ tuá»‡ nhÃ¢n táº¡o thÃ´ng qua mÃ£ nguá»“n má»Ÿ vÃ  khoa há»c má»Ÿ.']\n\n```\n\n## Results\n\n![image](https://user-images.githubusercontent.com/44376091/195998681-5860e443-2071-4048-8a2b-873dcee14a72.png)\n\n## Citation\n```\n@misc{https://doi.org/10.48550/arxiv.2210.05610,\n  doi = {10.48550/ARXIV.2210.05610},\n  author = {Ngo, Chinh and Trinh, Trieu H. and Phan, Long and Tran, Hieu and Dang, Tai and Nguyen, Hieu and Nguyen, Minh and Luong, Minh-Thang},\n  title = {MTet: Multi-domain Translation for English and Vietnamese},\n  publisher = {arXiv},\n  year = {2022},\n}\n```", "downloads": 4414, "likes": 43, "meta": {"datasets": ["cc100"], "language": ["vi", "en"], "license": "openrail", "tags": ["translation"], "widget": [{"text": "vi: VietAI lÃ  tá»• chá»©c phi lá»£i nhuáº­n vá»›i sá»© má»‡nh Æ°Æ¡m máº§m tÃ i nÄƒng vá» trÃ­ tuá»‡ nhÃ¢n táº¡o vÃ  xÃ¢y dá»±ng má»™t cá»™ng Ä‘á»“ng cÃ¡c chuyÃªn gia trong lÄ©nh vá»±c trÃ­ tuá»‡ nhÃ¢n táº¡o Ä‘áº³ng cáº¥p quá»‘c táº¿ táº¡i Viá»‡t Nam."}]}, "inference_type": "huggingface"}
{"id": "human-centered-summarization/financial-summarization-pegasus", "pipeline_tag": "summarization", "tags": ["transformers", "pytorch", "tf", "safetensors", "pegasus", "text2text-generation", "summarization", "en", "dataset:xsum", "arxiv:1912.08777", "model-index", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage:\n- en\ntags:\n- summarization\ndatasets:\n- xsum\nmetrics:\n- rouge\nwidget:\n- text: National Commercial Bank (NCB), Saudi Arabiaâ€™s largest lender by assets, agreed\n    to buy rival Samba Financial Group for $15 billion in the biggest banking takeover\n    this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to\n    a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer\n    0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio\n    the banks set when they signed an initial framework agreement in June.The offer\n    is a 3.5% premium to Sambaâ€™s Oct. 8 closing price of 27.50 riyals and about 24%\n    higher than the level the shares traded at before the talks were made public.\n    Bloomberg News first reported the merger discussions.The new bank will have total\n    assets of more than $220 billion, creating the Gulf regionâ€™s third-largest lender.\n    The entityâ€™s $46 billion market capitalization nearly matches that of Qatar National\n    Bank QPSC, which is still the Middle Eastâ€™s biggest lender with about $268 billion\n    of assets.\nmodel-index:\n- name: human-centered-summarization/financial-summarization-pegasus\n  results:\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: xsum\n      type: xsum\n      config: default\n      split: test\n    metrics:\n    - type: rouge\n      value: 35.2055\n      name: ROUGE-1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTA5OTZkY2YxMDU1YzE3NGJlMmE1OTg1NjlmNzcxOTg4YzY2OThlOTlkNGFhMGFjZWY4YjdiMjU5NDdmMWYzNSIsInZlcnNpb24iOjF9.ufBRoV2JoX4UlEfAUOYq7F3tZougwngdpKlnaC37tYXJU3omsR5hTsWM69hSdYO-k0cKUbAWCAMzjmoGwIaPAw\n    - type: rouge\n      value: 16.5689\n      name: ROUGE-2\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOWQwMmM2NjJjNzM1N2Y3NjZmMmE5NzNlNjRjNjEwNzNhNjcyZTRiMGRlODY3NWUyMGQ0YzZmMGFhODYzOTRmOSIsInZlcnNpb24iOjF9.AZZkbaYBZG6rw6-QHYjRlSl-p0gBT2EtJxwjIP7QYH5XIQjeoiQsTnDPIq25dSMDbmQLSZnpHC104ZctX0f_Dg\n    - type: rouge\n      value: 30.1285\n      name: ROUGE-L\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOTRjYThlMTllZjI4MGFiMDZhZTVkYmRjMTNhZDUzNTQ0OWQyNDQxMmQ5ODJiMmJiNGI3OTAzYjhiMzc2MTI4NCIsInZlcnNpb24iOjF9.zTHd3F4ZlgS-azl-ZVjOckcTrtrJmDOGWVaC3qQsvvn2UW9TnseNkmo7KBc3DJU7_NmlxWZArl1BdSetED0NCg\n    - type: rouge\n      value: 30.1706\n      name: ROUGE-LSUM\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGMzZGFjNzVkYWI0NTJkMmZjZDQ0YjhiYjIxN2VkNmJjMTgwZTk1NjFlOGU2NjNjM2VjYTNlYTBhNTQ5MGZkNSIsInZlcnNpb24iOjF9.xQ2LoI3PwlEiXo1OT2o4Pq9o2thYCd9lSCKCWlLmZdxI5GxdsjcASBKmHKopzUcwCGBPR7zF95MHSAPyszOODA\n    - type: loss\n      value: 2.7092134952545166\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzQzODE0NDc5YTYzYjJlMWU2YTVjOGRjN2JmYWVkOWNkNTRlMTZlOWIyN2NiODJkMDljMjI3YzZmYzM3N2JjYSIsInZlcnNpb24iOjF9.Vv_pdeFuRMoKK3cPr5P6n7D6_18ChJX-2qcT0y4is3XX3mS98fk3U1AYEuy9nBHOwYR3o0U8WBgQ-Ya_FqefBg\n    - type: gen_len\n      value: 15.1414\n      name: gen_len\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYjk5OTk3NWRiNjZlZmQzMmYwOTU2MmQwOWE1MDNlNTg3YWVkOTgwOTc2ZTQ0MTBiZjliOWMyZTYwMDI2MDUzYiIsInZlcnNpb24iOjF9.Zvj84JzIhM50rWTQ2GrEeOU7HrS8KsILH-8ApTcSWSI6kVnucY0MyW2ODxvRAa_zHeCygFW6Q13TFGrT5kLNAA\n---\n\n### PEGASUS for Financial Summarization \n\nThis model was fine-tuned on a novel financial news dataset, which consists of 2K articles from [Bloomberg](https://www.bloomberg.com/europe), on topics such as stock, markets, currencies, rate and cryptocurrencies. \n\nIt is based on the [PEGASUS](https://huggingface.co/transformers/model_doc/pegasus.html) model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: [google/pegasus-xsum model](https://huggingface.co/google/pegasus-xsum). PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/pdf/1912.08777.pdf). \n\n*Note: This model serves as a base version. For an even more advanced model with significantly enhanced performance, please check out our [advanced version](https://rapidapi.com/medoid-ai-medoid-ai-default/api/financial-summarization-advanced) on Rapid API. The advanced model offers more than a 16% increase in ROUGE scores (similarity to a human-generated summary) compared to our base model. Moreover, our advanced model also offers several convenient plans tailored to different use cases and workloads, ensuring a seamless experience for both personal and enterprise access.*\n\n### How to use \nWe provide a simple snippet of how to use this model for the task of financial summarization in PyTorch.\n\n```Python\nfrom transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\n\n# Let's load the model and the tokenizer \nmodel_name = \"human-centered-summarization/financial-summarization-pegasus\"\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name) # If you want to use the Tensorflow model \n                                                                    # just replace with TFPegasusForConditionalGeneration\n\n\n# Some text to summarize here\ntext_to_summarize = \"National Commercial Bank (NCB), Saudi Arabiaâ€™s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Sambaâ€™s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf regionâ€™s third-largest lender. The entityâ€™s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle Eastâ€™s biggest lender with about $268 billion of assets.\"\n\n# Tokenize our text\n# If you want to run the code in Tensorflow, please remember to return the particular tensors as simply as using return_tensors = 'tf'\ninput_ids = tokenizer(text_to_summarize, return_tensors=\"pt\").input_ids\n\n# Generate the output (Here, we use beam search but you can also use any other strategy you like)\noutput = model.generate(\n    input_ids, \n    max_length=32, \n    num_beams=5, \n    early_stopping=True\n)\n\n# Finally, we can print the generated summary\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n# Generated Output: Saudi bank to pay a 3.5% premium to Samba share price. Gulf regionâ€™s third-largest lender will have total assets of $220 billion\n```\n\n## Evaluation Results\nThe results before and after the fine-tuning on our dataset are shown below:\n\n\n| Fine-tuning |  R-1  |  R-2  |  R-L   |  R-S  |\n|:-----------:|:-----:|:-----:|:------:|:-----:|\n| Yes         | 23.55 |  6.99 | 18.14  | 21.36 | \n| No          | 13.8  |  2.4  | 10.63  | 12.03 |\n\n\n## Citation\n\nYou can find more details about this work in the following workshop paper. If you use our model in your research, please consider citing our paper:\n\n> T. Passali, A. Gidiotis, E. Chatzikyriakidis and G. Tsoumakas. 2021. \n> Towards Human-Centered Summarization: A Case Study on Financial News.\n> In Proceedings of the First Workshop on Bridging Human-Computer Interaction and Natural Language Processing(pp. 21â€“27). Association for Computational Linguistics.\n\nBibTeX entry:\n\n```\n@inproceedings{passali-etal-2021-towards,\n    title = \"Towards Human-Centered Summarization: A Case Study on Financial News\",\n    author = \"Passali, Tatiana  and Gidiotis, Alexios  and Chatzikyriakidis, Efstathios  and Tsoumakas, Grigorios\",\n    booktitle = \"Proceedings of the First Workshop on Bridging Human{--}Computer Interaction and Natural Language Processing\",\n    month = apr,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2021.hcinlp-1.4\",\n    pages = \"21--27\",\n}\n```\n\n## Support\n\nContact us at [info@medoid.ai](mailto:info@medoid.ai) if you are interested in a more sophisticated version of the model, trained on more articles and adapted to your needs!\n\nMore information about Medoid AI: \n- Website: [https://www.medoid.ai](https://www.medoid.ai)\n- LinkedIn: [https://www.linkedin.com/company/medoid-ai/](https://www.linkedin.com/company/medoid-ai/)\n\n\n", "downloads": 19592, "likes": 140, "meta": {"datasets": ["xsum"], "language": ["en"], "metrics": ["rouge"], "tags": ["summarization"], "widget": [{"text": "National Commercial Bank (NCB), Saudi Arabiaâ€™s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Sambaâ€™s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf regionâ€™s third-largest lender. The entityâ€™s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle Eastâ€™s biggest lender with about $268 billion of assets."}], "model-index": [{"name": "human-centered-summarization/financial-summarization-pegasus", "results": [{"task": {"type": "summarization", "name": "Summarization"}, "dataset": {"name": "xsum", "type": "xsum", "config": "default", "split": "test"}, "metrics": [{"type": "rouge", "value": 35.2055, "name": "ROUGE-1", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTA5OTZkY2YxMDU1YzE3NGJlMmE1OTg1NjlmNzcxOTg4YzY2OThlOTlkNGFhMGFjZWY4YjdiMjU5NDdmMWYzNSIsInZlcnNpb24iOjF9.ufBRoV2JoX4UlEfAUOYq7F3tZougwngdpKlnaC37tYXJU3omsR5hTsWM69hSdYO-k0cKUbAWCAMzjmoGwIaPAw"}, {"type": "rouge", "value": 16.5689, "name": "ROUGE-2", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOWQwMmM2NjJjNzM1N2Y3NjZmMmE5NzNlNjRjNjEwNzNhNjcyZTRiMGRlODY3NWUyMGQ0YzZmMGFhODYzOTRmOSIsInZlcnNpb24iOjF9.AZZkbaYBZG6rw6-QHYjRlSl-p0gBT2EtJxwjIP7QYH5XIQjeoiQsTnDPIq25dSMDbmQLSZnpHC104ZctX0f_Dg"}, {"type": "rouge", "value": 30.1285, "name": "ROUGE-L", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOTRjYThlMTllZjI4MGFiMDZhZTVkYmRjMTNhZDUzNTQ0OWQyNDQxMmQ5ODJiMmJiNGI3OTAzYjhiMzc2MTI4NCIsInZlcnNpb24iOjF9.zTHd3F4ZlgS-azl-ZVjOckcTrtrJmDOGWVaC3qQsvvn2UW9TnseNkmo7KBc3DJU7_NmlxWZArl1BdSetED0NCg"}, {"type": "rouge", "value": 30.1706, "name": "ROUGE-LSUM", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGMzZGFjNzVkYWI0NTJkMmZjZDQ0YjhiYjIxN2VkNmJjMTgwZTk1NjFlOGU2NjNjM2VjYTNlYTBhNTQ5MGZkNSIsInZlcnNpb24iOjF9.xQ2LoI3PwlEiXo1OT2o4Pq9o2thYCd9lSCKCWlLmZdxI5GxdsjcASBKmHKopzUcwCGBPR7zF95MHSAPyszOODA"}, {"type": "loss", "value": 2.7092134952545166, "name": "loss", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzQzODE0NDc5YTYzYjJlMWU2YTVjOGRjN2JmYWVkOWNkNTRlMTZlOWIyN2NiODJkMDljMjI3YzZmYzM3N2JjYSIsInZlcnNpb24iOjF9.Vv_pdeFuRMoKK3cPr5P6n7D6_18ChJX-2qcT0y4is3XX3mS98fk3U1AYEuy9nBHOwYR3o0U8WBgQ-Ya_FqefBg"}, {"type": "gen_len", "value": 15.1414, "name": "gen_len", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYjk5OTk3NWRiNjZlZmQzMmYwOTU2MmQwOWE1MDNlNTg3YWVkOTgwOTc2ZTQ0MTBiZjliOWMyZTYwMDI2MDUzYiIsInZlcnNpb24iOjF9.Zvj84JzIhM50rWTQ2GrEeOU7HrS8KsILH-8ApTcSWSI6kVnucY0MyW2ODxvRAa_zHeCygFW6Q13TFGrT5kLNAA"}]}]}]}, "inference_type": "huggingface"}
{"id": "Falconsai/text_summarization", "pipeline_tag": "summarization", "tags": ["transformers", "pytorch", "coreml", "onnx", "safetensors", "t5", "text2text-generation", "summarization", "en", "license:apache-2.0", "autotrain_compatible", "text-generation-inference", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\nlanguage:\n- en\npipeline_tag: summarization\nwidget:\n- text: 'Hugging Face: Revolutionizing Natural Language Processing Introduction In\n    the rapidly evolving field of Natural Language Processing (NLP), Hugging Face\n    has emerged as a prominent and innovative force. This article will explore the\n    story and significance of Hugging Face, a company that has made remarkable contributions\n    to NLP and AI as a whole. From its inception to its role in democratizing AI,\n    Hugging Face has left an indelible mark on the industry.  The Birth of Hugging\n    Face Hugging Face was founded in 2016 by ClÃ©ment Delangue, Julien Chaumond, and\n    Thomas Wolf. The name Hugging Face was chosen to reflect the company''s mission\n    of making AI models more accessible and friendly to humans, much like a comforting\n    hug. Initially, they began as a chatbot company but later shifted their focus\n    to NLP, driven by their belief in the transformative potential of this technology.\n    Transformative Innovations Hugging Face is best known for its open-source contributions,\n    particularly the Transformers library. This library has become the de facto standard\n    for NLP and enables researchers, developers, and organizations to easily access\n    and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3,\n    and more. These models have countless applications, from chatbots and virtual\n    assistants to language translation and sentiment analysis. '\nexample_title: Summarization Example 1\n---\n# Model Card: Fine-Tuned T5 Small for Text Summarization\n\n## Model Description\n\nThe **Fine-Tuned T5 Small** is a variant of the T5 transformer model, designed for the task of text summarization. It is adapted and fine-tuned to generate concise and coherent summaries of input text.\n\nThe model, named \"t5-small,\" is pre-trained on a diverse corpus of text data, enabling it to capture essential information and generate meaningful summaries. Fine-tuning is conducted with careful attention to hyperparameter settings, including batch size and learning rate, to ensure optimal performance for text summarization.\n\nDuring the fine-tuning process, a batch size of 8 is chosen for efficient computation and learning. Additionally, a learning rate of 2e-5 is selected to balance convergence speed and model optimization. This approach guarantees not only rapid learning but also continuous refinement during training.\n\nThe fine-tuning dataset consists of a variety of documents and their corresponding human-generated summaries. This diverse dataset allows the model to learn the art of creating summaries that capture the most important information while maintaining coherence and fluency.\n\nThe goal of this meticulous training process is to equip the model with the ability to generate high-quality text summaries, making it valuable for a wide range of applications involving document summarization and content condensation.\n\n## Intended Uses & Limitations\n\n### Intended Uses\n- **Text Summarization**: The primary intended use of this model is to generate concise and coherent text summaries. It is well-suited for applications that involve summarizing lengthy documents, news articles, and textual content.\n\n### How to Use\nTo use this model for text summarization, you can follow these steps:\n\n\n```python\nfrom transformers import pipeline\n\nsummarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n\nARTICLE = \"\"\" \nHugging Face: Revolutionizing Natural Language Processing\nIntroduction\nIn the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.\nThe Birth of Hugging Face\nHugging Face was founded in 2016 by ClÃ©ment Delangue, Julien Chaumond, and Thomas Wolf. The name \"Hugging Face\" was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.\nTransformative Innovations\nHugging Face is best known for its open-source contributions, particularly the \"Transformers\" library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.\nKey Contributions:\n1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.\n2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.\n3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.\nDemocratizing AI\nHugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.\nBy providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.\nIndustry Adoption\nThe success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.\nFuture Directions\nHugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.\nConclusion\nHugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.\n\"\"\"\nprint(summarizer(ARTICLE, max_length=1000, min_length=30, do_sample=False))\n>>> [{'summary_text': 'Hugging Face has emerged as a prominent and innovative force in NLP . From its inception to its role in democratizing AI, the company has left an indelible mark on the industry . The name \"Hugging Face\" was chosen to reflect the company\\'s mission of making AI models more accessible and friendly to humans .'}]\n```\n\n\nLimitations\nSpecialized Task Fine-Tuning: While the model excels at text summarization, its performance may vary when applied to other natural language processing tasks. Users interested in employing this model for different tasks should explore fine-tuned versions available in the model hub for optimal results.\nTraining Data\nThe model's training data includes a diverse dataset of documents and their corresponding human-generated summaries. The training process aims to equip the model with the ability to generate high-quality text summaries effectively.\n\nTraining Stats\n- Evaluation Loss: 0.012345678901234567\n- Evaluation Rouge Score: 0.95 (F1)\n- Evaluation Runtime: 2.3456\n- Evaluation Samples per Second: 1234.56\n- Evaluation Steps per Second: 45.678\n\n\nResponsible Usage\nIt is essential to use this model responsibly and ethically, adhering to content guidelines and applicable regulations when implementing it in real-world applications, particularly those involving potentially sensitive content.\n\nReferences\nHugging Face Model Hub\nT5 Paper\nDisclaimer: The model's performance may be influenced by the quality and representativeness of the data it was fine-tuned on. Users are encouraged to assess the model's suitability for their specific applications and datasets.", "downloads": 37950, "likes": 235, "meta": {"language": ["en"], "license": "apache-2.0", "pipeline_tag": "summarization", "widget": [{"text": "Hugging Face: Revolutionizing Natural Language Processing Introduction In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.  The Birth of Hugging Face Hugging Face was founded in 2016 by ClÃ©ment Delangue, Julien Chaumond, and Thomas Wolf. The name Hugging Face was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology. Transformative Innovations Hugging Face is best known for its open-source contributions, particularly the Transformers library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis. "}], "example_title": "Summarization Example 1"}, "inference_type": "huggingface"}
{"id": "jotamunz/billsum_tiny_summarization", "pipeline_tag": "summarization", "tags": ["transformers", "pytorch", "t5", "text2text-generation", "generated_from_trainer", "summarization", "dataset:billsum", "base_model:google/t5-efficient-tiny", "base_model:finetune:google/t5-efficient-tiny", "license:apache-2.0", "model-index", "autotrain_compatible", "text-generation-inference", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\nbase_model: google/t5-efficient-tiny\ntags:\n- generated_from_trainer\ndatasets:\n- billsum\nmetrics:\n- rouge\npipeline_tag: summarization\nmodel-index:\n- name: billsum_tiny_summarization\n  results:\n  - task:\n      type: summarization\n      name: Sequence-to-sequence Language Modeling\n    dataset:\n      name: billsum\n      type: billsum\n      config: default\n      split: ca_test\n      args: default\n    metrics:\n    - type: rouge\n      value: 0.1503\n      name: Rouge1\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# billsum_tiny_summarization\n\nThis model is a fine-tuned version of [google/t5-efficient-tiny](https://huggingface.co/google/t5-efficient-tiny) on the billsum dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 3.5889\n- Rouge1: 0.1503\n- Rouge2: 0.0412\n- Rougel: 0.1244\n- Rougelsum: 0.1244\n- Gen Len: 19.0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1 | Rouge2 | Rougel | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:------:|:------:|:------:|:---------:|:-------:|\n| No log        | 1.0   | 62   | 4.2835          | 0.1413 | 0.0323 | 0.1125 | 0.1124    | 19.0    |\n| No log        | 2.0   | 124  | 3.7275          | 0.1507 | 0.0408 | 0.1263 | 0.1264    | 19.0    |\n| No log        | 3.0   | 186  | 3.6154          | 0.1499 | 0.0407 | 0.1244 | 0.1244    | 19.0    |\n| No log        | 4.0   | 248  | 3.5889          | 0.1503 | 0.0412 | 0.1244 | 0.1244    | 19.0    |\n\n\n### Framework versions\n\n- Transformers 4.33.3\n- Pytorch 2.0.1+cu118\n- Datasets 2.14.5\n- Tokenizers 0.13.3", "downloads": 256, "likes": 1, "meta": {"base_model": "google/t5-efficient-tiny", "datasets": ["billsum"], "license": "apache-2.0", "metrics": ["rouge"], "pipeline_tag": "summarization", "tags": ["generated_from_trainer"], "model-index": [{"name": "billsum_tiny_summarization", "results": [{"task": {"type": "summarization", "name": "Sequence-to-sequence Language Modeling"}, "dataset": {"name": "billsum", "type": "billsum", "config": "default", "split": "ca_test", "args": "default"}, "metrics": [{"type": "rouge", "value": 0.1503, "name": "Rouge1", "verified": false}]}]}]}, "inference_type": "huggingface"}
{"id": "google/pegasus-multi_news", "pipeline_tag": "summarization", "tags": ["transformers", "pytorch", "pegasus", "text2text-generation", "summarization", "en", "arxiv:1912.08777", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage: en\ntags:\n- summarization\n---\n\n### Pegasus Models\nSee Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html)\n\nOriginal TF 1 code [here](https://github.com/google-research/pegasus)\n\nAuthors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019\n\nMaintained by: [@sshleifer](https://twitter.com/sam_shleifer)\n\nTask: Summarization\n\nThe following is copied from the authors' README.\n\n# Mixed & Stochastic Checkpoints\n\nWe train a pegasus model with sampled gap sentence ratios on both C4 and HugeNews, and stochastically sample important sentences. The updated the results are reported in this table.\n\n| dataset | C4 | HugeNews | Mixed & Stochastic|\n| ---- | ---- | ---- | ----|\n| xsum | 45.20/22.06/36.99 | 47.21/24.56/39.25 | 47.60/24.83/39.64|\n| cnn_dailymail | 43.90/21.20/40.76 | 44.17/21.47/41.11 | 44.16/21.56/41.30|\n| newsroom | 45.07/33.39/41.28 | 45.15/33.51/41.33 | 45.98/34.20/42.18|\n| multi_news | 46.74/17.95/24.26 | 47.52/18.72/24.91 | 47.65/18.75/24.95|\n| gigaword | 38.75/19.96/36.14 | 39.12/19.86/36.24 | 39.65/20.47/36.76|\n| wikihow | 43.07/19.70/34.79 | 41.35/18.51/33.42 | 46.39/22.12/38.41 *|\n| reddit_tifu | 26.54/8.94/21.64 | 26.63/9.01/21.60 | 27.99/9.81/22.94|\n| big_patent | 53.63/33.16/42.25 | 53.41/32.89/42.07 | 52.29/33.08/41.66 *|\n| arxiv | 44.70/17.27/25.80 | 44.67/17.18/25.73 | 44.21/16.95/25.67|\n| pubmed | 45.49/19.90/27.69 | 45.09/19.56/27.42 | 45.97/20.15/28.25|\n| aeslc | 37.69/21.85/36.84 | 37.40/21.22/36.45 | 37.68/21.25/36.51|\n| billsum | 57.20/39.56/45.80 | 57.31/40.19/45.82 | 59.67/41.58/47.59|\n\nThe \"Mixed & Stochastic\" model has the following changes:\n- trained on both C4 and HugeNews (dataset mixture is weighted by their number of examples). \n- trained for 1.5M instead of 500k (we observe slower convergence on pretraining perplexity).\n- the model uniformly sample a gap sentence ratio between 15% and 45%.\n- importance sentences are sampled using a 20% uniform noise to importance scores.\n- the sentencepiece tokenizer is updated to be able to encode newline character.\n\n\n(*) the numbers of wikihow and big_patent datasets are not comparable because of change in tokenization and data:\n- wikihow dataset contains newline characters which is useful for paragraph segmentation, the C4 and HugeNews model's sentencepiece tokenizer doesn't encode newline and loose this information.\n- we update the BigPatent dataset to preserve casing, some format cleanings are also changed, please refer to change in TFDS.\n\n\nThe \"Mixed & Stochastic\" model has the following changes (from pegasus-large in the paper):\n\n\ntrained on both C4 and HugeNews (dataset mixture is weighted by their number of examples).\ntrained for 1.5M instead of 500k (we observe slower convergence on pretraining perplexity).\nthe model uniformly sample a gap sentence ratio between 15% and 45%.\nimportance sentences are sampled using a 20% uniform noise to importance scores.\nthe sentencepiece tokenizer is updated to be able to encode newline character.\n\n\nCitation\n```\n\n\n@misc{zhang2019pegasus,\n    title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization},\n    author={Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},\n    year={2019},\n    eprint={1912.08777},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```", "downloads": 209, "likes": 26, "meta": {"language": "en", "tags": ["summarization"]}, "inference_type": "huggingface"}
{"id": "knkarthick/meeting-summary-samsum", "pipeline_tag": "summarization", "tags": ["transformers", "pytorch", "bart", "text2text-generation", "seq2seq", "summarization", "en", "dataset:samsum", "license:apache-2.0", "model-index", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage: en\ntags:\n- bart\n- seq2seq\n- summarization\nlicense: apache-2.0\ndatasets:\n- samsum\nwidget:\n- text: 'Hannah: Hey, do you have Betty''s number?\n\n    Amanda: Lemme check\n\n    Amanda: Sorry, can''t find it.\n\n    Amanda: Ask Larry\n\n    Amanda: He called her last time we were at the park together\n\n    Hannah: I don''t know him well\n\n    Amanda: Don''t be shy, he''s very nice\n\n    Hannah: If you say so..\n\n    Hannah: I''d rather you texted him\n\n    Amanda: Just text him ðŸ™‚\n\n    Hannah: Urgh.. Alright\n\n    Hannah: Bye\n\n    Amanda: Bye bye\n\n    '\nmodel-index:\n- name: bart-large-xsum-samsum\n  results:\n  - task:\n      type: abstractive-text-summarization\n      name: Abstractive Text Summarization\n    dataset:\n      name: 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization'\n      type: samsum\n    metrics:\n    - type: rouge-1\n      value: 54.3921\n      name: Validation ROUGE-1\n    - type: rouge-2\n      value: 29.8078\n      name: Validation ROUGE-2\n    - type: rouge-l\n      value: 45.1543\n      name: Validation ROUGE-L\n    - type: rouge-1\n      value: 53.3059\n      name: Test ROUGE-1\n    - type: rouge-2\n      value: 28.355\n      name: Test ROUGE-2\n    - type: rouge-l\n      value: 44.0953\n      name: Test ROUGE-L\n---\n## `bart-large-xsum-samsum`\nThis model was obtained by fine-tuning `facebook/bart-large-xsum` on [Samsum](https://huggingface.co/datasets/samsum) dataset.\n## Usage\n```python\nfrom transformers import pipeline\nsummarizer = pipeline(\"summarization\", model=\"knkarthick/bart-large-xsum-samsum\")\nconversation = '''Hannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him ðŸ™‚\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye                                       \n'''\nsummarizer(conversation)\n```", "downloads": 180, "likes": 9, "meta": {"datasets": ["samsum"], "language": "en", "license": "apache-2.0", "tags": ["bart", "seq2seq", "summarization"], "widget": [{"text": "Hannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him ðŸ™‚\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye\n"}], "model-index": [{"name": "bart-large-xsum-samsum", "results": [{"task": {"type": "abstractive-text-summarization", "name": "Abstractive Text Summarization"}, "dataset": {"name": "SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization", "type": "samsum"}, "metrics": [{"type": "rouge-1", "value": 54.3921, "name": "Validation ROUGE-1", "verified": false}, {"type": "rouge-2", "value": 29.8078, "name": "Validation ROUGE-2", "verified": false}, {"type": "rouge-l", "value": 45.1543, "name": "Validation ROUGE-L", "verified": false}, {"type": "rouge-1", "value": 53.3059, "name": "Test ROUGE-1", "verified": false}, {"type": "rouge-2", "value": 28.355, "name": "Test ROUGE-2", "verified": false}, {"type": "rouge-l", "value": 44.0953, "name": "Test ROUGE-L", "verified": false}]}]}]}, "inference_type": "huggingface"}
{"id": "pszemraj/led-large-book-summary", "pipeline_tag": "summarization", "tags": ["transformers", "pytorch", "safetensors", "led", "text2text-generation", "summarization", "summary", "longformer", "booksum", "long-document", "long-form", "en", "dataset:kmfoda/booksum", "arxiv:2105.08209", "doi:10.57967/hf/0101", "license:apache-2.0", "license:bsd-3-clause", "model-index", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlanguage:\n- en\nlicense:\n- apache-2.0\n- bsd-3-clause\ntags:\n- summarization\n- led\n- summary\n- longformer\n- booksum\n- long-document\n- long-form\ndatasets:\n- kmfoda/booksum\nmetrics:\n- rouge\nwidget:\n- text: large earthquakes along a given fault segment do not occur at random intervals\n    because it takes time to accumulate the strain energy for the rupture. The rates\n    at which tectonic plates move and accumulate strain at their boundaries are approximately\n    uniform. Therefore, in first approximation, one may expect that large ruptures\n    of the same fault segment will occur at approximately constant time intervals.\n    If subsequent main shocks have different amounts of slip across the fault, then\n    the recurrence time may vary, and the basic idea of periodic mainshocks must be\n    modified. For great plate boundary ruptures the length and slip often vary by\n    a factor of 2. Along the southern segment of the San Andreas fault the recurrence\n    interval is 145 years with variations of several decades. The smaller the standard\n    deviation of the average recurrence interval, the more specific could be the long\n    term prediction of a future mainshock.\n  example_title: earthquakes\n- text: ' A typical feed-forward neural field algorithm. Spatiotemporal coordinates\n    are fed into a neural network that predicts values in the reconstructed domain.\n    Then, this domain is mapped to the sensor domain where sensor measurements are\n    available as supervision. Class and Section Problems Addressed Generalization\n    (Section 2) Inverse problems, ill-posed problems, editability; symmetries. Hybrid\n    Representations (Section 3) Computation & memory efficiency, representation capacity,\n    editability: Forward Maps (Section 4) Inverse problems Network Architecture (Section\n    5) Spectral bias, integration & derivatives. Manipulating Neural Fields (Section\n    6) Edit ability, constraints, regularization. Table 2: The five classes of techniques\n    in the neural field toolbox each addresses problems that arise in learning, inference,\n    and control. (Section 3). We can supervise reconstruction via differentiable forward\n    maps that transform Or project our domain (e.g, 3D reconstruction via 2D images;\n    Section 4) With appropriate network architecture choices, we can overcome neural\n    network spectral biases (blurriness) and efficiently compute derivatives and integrals\n    (Section 5). Finally, we can manipulate neural fields to add constraints and regularizations,\n    and to achieve editable representations (Section 6). Collectively, these classes\n    constitute a ''toolbox'' of techniques to help solve problems with neural fields\n    There are three components in a conditional neural field: (1) An encoder or inference\n    function â‚¬ that outputs the conditioning latent variable 2 given an observation\n    0 E(0) =2. 2 is typically a low-dimensional vector, and is often referred to aS\n    a latent code Or feature code_ (2) A mapping function 4 between Z and neural field\n    parameters O: Y(z) = O; (3) The neural field itself $. The encoder â‚¬ finds the\n    most probable z given the observations O: argmaxz P(2/0). The decoder maximizes\n    the inverse conditional probability to find the most probable 0 given Z: arg-\n    max P(Olz). We discuss different encoding schemes with different optimality guarantees\n    (Section 2.1.1), both global and local conditioning (Section 2.1.2), and different\n    mapping functions Y (Section 2.1.3) 2. Generalization Suppose we wish to estimate\n    a plausible 3D surface shape given a partial or noisy point cloud. We need a suitable\n    prior over the sur- face in its reconstruction domain to generalize to the partial\n    observations. A neural network expresses a prior via the function space of its\n    architecture and parameters 0, and generalization is influenced by the inductive\n    bias of this function space (Section 5).'\n  example_title: scientific paper\n- text: ' the big variety of data coming from diverse sources is one of the key properties\n    of the big data phenomenon. It is, therefore, beneficial to understand how data\n    is generated in various environments and scenarios, before looking at what should\n    be done with this data and how to design the best possible architecture to accomplish\n    this The evolution of IT architectures, described in Chapter 2, means that the\n    data is no longer processed by a few big monolith systems, but rather by a group\n    of services In parallel to the processing layer, the underlying data storage has\n    also changed and became more distributed This, in turn, required a significant\n    paradigm shift as the traditional approach to transactions (ACID) could no longer\n    be supported. On top of this, cloud computing is becoming a major approach with\n    the benefits of reducing costs and providing on-demand scalability but at the\n    same time introducing concerns about privacy, data ownership, etc In the meantime\n    the Internet continues its exponential growth: Every day both structured and unstructured\n    data is published and available for processing: To achieve competitive advantage\n    companies have to relate their corporate resources to external services, e.g.\n    financial markets, weather forecasts, social media, etc While several of the sites\n    provide some sort of API to access the data in a more orderly fashion; countless\n    sources require advanced web mining and Natural Language Processing (NLP) processing\n    techniques: Advances in science push researchers to construct new instruments\n    for observing the universe O conducting experiments to understand even better\n    the laws of physics and other domains. Every year humans have at their disposal\n    new telescopes, space probes, particle accelerators, etc These instruments generate\n    huge streams of data, which need to be stored and analyzed. The constant drive\n    for efficiency in the industry motivates the introduction of new automation techniques\n    and process optimization: This could not be done without analyzing the precise\n    data that describe these processes. As more and more human tasks are automated,\n    machines provide rich data sets, which can be analyzed in real-time to drive efficiency\n    to new levels. Finally, it is now evident that the growth of the Internet of Things\n    is becoming a major source of data. More and more of the devices are equipped\n    with significant computational power and can generate a continuous data stream\n    from their sensors. In the subsequent sections of this chapter, we will look at\n    the domains described above to see what they generate in terms of data sets. We\n    will compare the volumes but will also look at what is characteristic and important\n    from their respective points of view. 3.1 The Internet is undoubtedly the largest\n    database ever created by humans. While several well described; cleaned, and structured\n    data sets have been made available through this medium, most of the resources\n    are of an ambiguous, unstructured, incomplete or even erroneous nature. Still,\n    several examples in the areas such as opinion mining, social media analysis, e-governance,\n    etc, clearly show the potential lying in these resources. Those who can successfully\n    mine and interpret the Internet data can gain unique insight and competitive advantage\n    in their business An important area of data analytics on the edge of corporate\n    IT and the Internet is Web Analytics.'\n  example_title: data science textbook\n- text: 'Transformer-based models have shown to be very useful for many NLP tasks.\n    However, a major limitation of transformers-based models is its O(n^2)O(n 2) time\n    & memory complexity (where nn is sequence length). Hence, it''s computationally\n    very expensive to apply transformer-based models on long sequences n > 512n>512.\n    Several recent papers, e.g. Longformer, Performer, Reformer, Clustered attention\n    try to remedy this problem by approximating the full attention matrix. You can\n    checkout ðŸ¤—''s recent blog post in case you are unfamiliar with these models.\n\n    BigBird (introduced in paper) is one of such recent models to address this issue.\n    BigBird relies on block sparse attention instead of normal attention (i.e. BERT''s\n    attention) and can handle sequences up to a length of 4096 at a much lower computational\n    cost compared to BERT. It has achieved SOTA on various tasks involving very long\n    sequences such as long documents summarization, question-answering with long contexts.\n\n    BigBird RoBERTa-like model is now available in ðŸ¤—Transformers. The goal of this\n    post is to give the reader an in-depth understanding of big bird implementation\n    & ease one''s life in using BigBird with ðŸ¤—Transformers. But, before going into\n    more depth, it is important to remember that the BigBird''s attention is an approximation\n    of BERT''s full attention and therefore does not strive to be better than BERT''s\n    full attention, but rather to be more efficient. It simply allows to apply transformer-based\n    models to much longer sequences since BERT''s quadratic memory requirement quickly\n    becomes unbearable. Simply put, if we would have âˆž compute & âˆž time, BERT''s attention\n    would be preferred over block sparse attention (which we are going to discuss\n    in this post).\n\n    If you wonder why we need more compute when working with longer sequences, this\n    blog post is just right for you!\n\n    Some of the main questions one might have when working with standard BERT-like\n    attention include:\n\n    Do all tokens really have to attend to all other tokens? Why not compute attention\n    only over important tokens? How to decide what tokens are important? How to attend\n    to just a few tokens in a very efficient way? In this blog post, we will try to\n    answer those questions.\n\n    What tokens should be attended to? We will give a practical example of how attention\n    works by considering the sentence ''BigBird is now available in HuggingFace for\n    extractive question answering''. In BERT-like attention, every word would simply\n    attend to all other tokens.\n\n    Let''s think about a sensible choice of key tokens that a queried token actually\n    only should attend to by writing some pseudo-code. Will will assume that the token\n    available is queried and build a sensible list of key tokens to attend to.\n\n    >>> # let''s consider following sentence as an example >>> example = [''BigBird'',\n    ''is'', ''now'', ''available'', ''in'', ''HuggingFace'', ''for'', ''extractive'',\n    ''question'', ''answering'']\n\n    >>> # further let''s assume, we''re trying to understand the representation of\n    ''available'' i.e. >>> query_token = ''available'' >>> # We will initialize an\n    empty `set` and fill up the tokens of our interest as we proceed in this section.\n    >>> key_tokens = [] # => currently ''available'' token doesn''t have anything\n    to attend Nearby tokens should be important because, in a sentence (sequence of\n    words), the current word is highly dependent on neighboring past & future tokens.\n    This intuition is the idea behind the concept of sliding attention.'\n  example_title: bigbird blog intro\n- text: 'The majority of available text summarization datasets include short-form\n    source documents that lack long-range causal and temporal dependencies, and often\n    contain strong layout and stylistic biases. While relevant, such datasets will\n    offer limited challenges for future generations of text summarization systems.\n    We address these issues by introducing BookSum, a collection of datasets for long-form\n    narrative summarization. Our dataset covers source documents from the literature\n    domain, such as novels, plays and stories, and includes highly abstractive, human\n    written summaries on three levels of granularity of increasing difficulty: paragraph-,\n    chapter-, and book-level. The domain and structure of our dataset poses a unique\n    set of challenges for summarization systems, which include: processing very long\n    documents, non-trivial causal and temporal dependencies, and rich discourse structures.\n    To facilitate future work, we trained and evaluated multiple extractive and abstractive\n    summarization models as baselines for our dataset.'\n  example_title: BookSum Abstract\ninference:\n  parameters:\n    max_length: 64\n    min_length: 8\n    no_repeat_ngram_size: 3\n    early_stopping: true\n    repetition_penalty: 3.5\n    length_penalty: 0.3\n    encoder_no_repeat_ngram_size: 3\n    num_beams: 4\nmodel-index:\n- name: pszemraj/led-large-book-summary\n  results:\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: kmfoda/booksum\n      type: kmfoda/booksum\n      config: kmfoda--booksum\n      split: test\n    metrics:\n    - type: rouge\n      value: 31.7308\n      name: ROUGE-1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjJmZjMxYTY0OGU3MzNjNmIzNmYyODNlNDg2ZGRhZDAzNTMwMDM5YWMxODc1OTc1ZWE3MzM2OTg1ODFhZDBkNCIsInZlcnNpb24iOjF9.B8BCKgySYVZW910_1zP0LfCpQYJbAe6loyWut76JlgZb2kV1_x9ybqtNESX0ka-lNqhYyXUNDpuS-7pTmsJVDg\n    - type: rouge\n      value: 5.3311\n      name: ROUGE-2\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYzViMmY4ODFjYTc5ODk5MmRhMDQ3ZDRiYWQwMDg0OTk3ZTA4NDAxYTNiNDgyMmI4NDA3ZDMwYWViOTBkODBjNyIsInZlcnNpb24iOjF9.MOhJLDcgvv93mVFL1igIgIiTAH3b2Xa4gmBObq7RF44Mmu8Kxtd1KP7rOlDVFOrtrsooGPGsyE1GMCQ2kqeMDg\n    - type: rouge\n      value: 16.1465\n      name: ROUGE-L\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNzNjMzEwMTliZGE3ZmQ4M2UxMDAyMTY3YzJjZmMyMDYyN2YyNDM0N2VhNzI1MDc1YTg4MTRjMmEzNjVkNTk1NCIsInZlcnNpb24iOjF9.XLJ-DVKiYLlbw5E5rWADKbzUzf5fNHhlTCWPCC5dU4NI9Yeh76aR7TPt36ZzLDwTBknnR8KHqlaF8F8YAvBUAg\n    - type: rouge\n      value: 29.0883\n      name: ROUGE-LSUM\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTcwNzEwMmE5NjQxZTkzYmQyZDZmNzllYzYyNGI5OTMyNWMwNjdiM2I2YmM5YjdmY2E5OWQ3OTk3ZDA1MTc3YyIsInZlcnNpb24iOjF9.d6rFxjCB6RJNI_pn2DNNSjuZe4rdvj0RatkaTJRp5lP0F_AFfU5Zn9zRWzZJV7V-xMauIc4UhfdoLp9r_-CABA\n    - type: loss\n      value: 4.815707206726074\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTMwMTgxMmJkODY3MjkzOWJhMzJhOTIxMWVkODhjZmM0MWUzMWQ1N2JkZjRhOTQxNmU1YWVjYzQ0MDNlZWI3OSIsInZlcnNpb24iOjF9.mkBQHYhYFfDV6F4klXGJ1dSsF-pbCs-6F9zcw6IYznwmXUjtk7m5J4Zt4JAju5LKz4YizvEcUCl_L0WddnfvDA\n    - type: gen_len\n      value: 154.9036\n      name: gen_len\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTc0ZmM1ZDM4MDE0MzY3MDM3OWJhNDkzZjJkZDdkMjU5M2JmMDJjYTIxODA1OTllNmY5ZWQzZDlmNWFiYzk4NiIsInZlcnNpb24iOjF9.VQ_O_xSTz870tnM08PJXQOwg9OsNNwI_HVX4S7AuW57_FzGGyRaWSuGE5SWzRS4Tur9YP0QxV4VV0Yoaoi3IAA\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: samsum\n      type: samsum\n      config: samsum\n      split: test\n    metrics:\n    - type: rouge\n      value: 33.4484\n      name: ROUGE-1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTk4Yjg1YTc4YmY0MzBiZDU4ZjFhNzI4MjZkMWU1MzBlOWNlMjQ5ODMzY2YzYzRhYjJkMGUzNmI3ZjdkMzIzZSIsInZlcnNpb24iOjF9.AqS8A1OUiM0IZFBEGirv5F3Novk8lSUYSfPc3bYWLA6t-W7wgup3qA207eGbE5j9CkDWZ7QrSG1U6Z9A0sOqAA\n    - type: rouge\n      value: 10.4249\n      name: ROUGE-2\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2U4NjUyNTFmOGM5OTlhZDMyMTlmM2E4OWI2NGFiMDAyMGJjMzRjNWNlMGEyYWFmNTE5ZWMxM2I0ZGZmNWNmOCIsInZlcnNpb24iOjF9.SgJcHJ4qoRWXFvFiwv1PUutWktvsxQNynVPEv-GtBgxd6WI7o561ONyco5U-5tcyE_1SbSCJzz-L-R-q3cvoDA\n    - type: rouge\n      value: 24.5802\n      name: ROUGE-L\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZmQ5MDI5MzdiNGE5NDM0MmU5OThmZTBkNjkxMzg5N2IxNGVlODdhZTZhNjg3NzFjYWEyMzA3MTQxNjMyMjRkOCIsInZlcnNpb24iOjF9.Bg5dHqCcJjmxa-xGWNR5lD9g3quX7lKkH0pjiTd2xE5WiPoLLN2c0mYa2GovdW7__WnYwhhHC7es03jmvyZbCw\n    - type: rouge\n      value: 29.8226\n      name: ROUGE-LSUM\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNGFhOTEwNGM1MmZkNDk2ZjQ1Y2MyNjM3MGI5MGY3MWVkM2I0MjU2NWFiYmEwMjE4MTJlZWIwOGQ2MjQ3YjgzYSIsInZlcnNpb24iOjF9.W_aQKs10oXQdKEczJBGM3iiwJgb-VaXTpyA3sGof5WbhHf9vITAQA-xvynh5LgKtXQ1zjx737hnHgjEsu_Y0Cw\n    - type: loss\n      value: 4.176078796386719\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2JhODQ5YTZkNDZkZGYyNGU2MzkxMWU5MTEwMGM2YmVjZTA5YzI5NTMxMDNhYjhlOTAxMzFiMDYwYmM0MjEzZCIsInZlcnNpb24iOjF9.OvZrPBOR5jhkoTGBgsInkH7j3_xpacXHDoT7UIXEnyXzadfBO-O-K6fjalLNZw8wSkbjHIFcL_6S_qTTxPsNAQ\n    - type: gen_len\n      value: 65.4005\n      name: gen_len\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiM2NhYjc3ZjQzNDEwYmMzOTM0ODkyZTJhZWNhNzZhYmEyZTYxMzA2YTYzMWFjOTA5ZjlhYWMzODg3NzY1ZTUwYSIsInZlcnNpb24iOjF9.vk9bgmtQFeRwdY3VXjtrJr_5wUCIeoAkI3kO0cHxhxmJo6RvUnyXiut72FuB-mlLZvqgiNkaZ-u_bh0Z3DjuCw\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: billsum\n      type: billsum\n      config: default\n      split: test\n    metrics:\n    - type: rouge\n      value: 40.5843\n      name: ROUGE-1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTVjMDkyMWZjYTQ0NzgzNGUxZjNiMTg3NjU1MWJlNTQ2MWQ1NjE1MDk1OTU4ZjJiNGQ5ODg3Y2VlMWUyMzllNyIsInZlcnNpb24iOjF9.OhqBcVIuHk7fzmdrsWMvUe1bLeVMZVstZUoZpP7C1vR-3aIDl7r6eBmPrt5w-KcNq5p4teNPBsq7oKzbd5ZgDQ\n    - type: rouge\n      value: 17.3401\n      name: ROUGE-2\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNGQxYmQzMmE0OTcyNTM5NmMwNjIxNzYxZDcwMDFkYzJkOWY4YWY3NTdhZGRhZDdlMDAxNzcwODQ5OGM3Mzc1MCIsInZlcnNpb24iOjF9.Pksn25EEqvmx757N7Swrd4yXc_xU7-AMN9yNe8lrbBa-l1LoI_2PUASvnjML4f705cfuyMAfb0FkFp5WfER2AA\n    - type: rouge\n      value: 25.1256\n      name: ROUGE-L\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjhjYzI5MDBiMjk2NTY3MDNmZTdiOGYwMTRlYjIwZjAwMjdlNTAyYzdhYTJlODQ4MjYzYmQ3MjRlYTA2YzhhZSIsInZlcnNpb24iOjF9.1jPepsweS2bzIqDverQzzhmhFGch7gpoEGFGqQ8zW7K10aUKWFX8lt-uZAmTa1Z5ZhzyXGBzc3dReFPhWRRJBg\n    - type: rouge\n      value: 34.6619\n      name: ROUGE-LSUM\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiM2VkZDIxNWJjOTA0NzFjOTIwOTdjYjc1M2EyNDVjZjY2ZjY3MjIxNDk3YTc5YWExNzAwN2FhOTc1NjVhYjBkYiIsInZlcnNpb24iOjF9.8opqHSUckPohoSF9jfPTpXDz2AtDwvdMqOdIXx2kE1tkOcbLPbOBfcc8RhRR98y8S26yC6EYFhFnf03CV2ejAQ\n    - type: loss\n      value: 4.792657375335693\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYTY5ZTRkMGU3OGVkODMzMDU5OWE1NTM5YjA4NDliZDlmNzc2NzZjNjFmNTA3M2EwY2NmN2E0MWJmZjQ5ZDliMiIsInZlcnNpb24iOjF9.KCKdk8xt2NWcMmYKV3-9eVEsFm9MqGllSMu9QCFJFIQlnyNXllHKdBLouoaGQz8IRYXvZKH8_TLDPIQx-31jAg\n    - type: gen_len\n      value: 163.9394\n      name: gen_len\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYzdkZDYyZGUzYmFkZmI2NjUwYmQ0MzZjMmIyZjI1YTFiMzM4OThiZjBiMzljOTVkZTgwMjA0NTE5OGM2YmFjMiIsInZlcnNpb24iOjF9.XyMZLUdkUIF32KTJMuv_bJswQCx_Tfg4Fx823cURUixSeoIKps8_a634AreZ3Z8kb7bfE_sFGh3rM9KWsMxlDw\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: multi_news\n      type: multi_news\n      config: default\n      split: test\n    metrics:\n    - type: rouge\n      value: 39.0834\n      name: ROUGE-1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjYzMmVlMDM4MTNkMTI4MjAyMTU2YTg1ZWQwNTI1MmJlNGUwZmE1NTRmYTljZTQwY2RlMjcxOTgyZGMyYTc0ZiIsInZlcnNpb24iOjF9.6yuSr7UmsFatwqQ-mEO4gmsEtWI05kGB5Ib2pnl05H1OiPT2uUwmqdUytUw8KTx9u1jv9q0cTF1cL-n2kPEJAA\n    - type: rouge\n      value: 11.4043\n      name: ROUGE-2\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMWI5N2U2ZWI1ODM2MWUwOTIzYTAzNmRhNDA2OWEzZWRjMGEzMjBmY2EwN2YyYzU1NWE0YjIyZDE3MWE0MmMxZCIsInZlcnNpb24iOjF9.wonuxbBl25TzEaHUH_E816nHJ1OSXKfkaq7eJzbLpsfeGwcDklxUSxZxRO7VBiBMaY3Qttf9ywmEIPp40HnpBA\n    - type: rouge\n      value: 19.1813\n      name: ROUGE-L\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjU1NDZhN2NkMzZiZGJkODE4NDZiYjViOTZkNGMyNDlkNjBlZmFjYzU1N2IzMjFjYjY1MDU1Zjk2MzA0M2U4NyIsInZlcnNpb24iOjF9.bTCRzv3J9NiCh4aV23tAWGTvrdQCv_RS40zGwC4AJXtGS40cY7tJHYwBf9U9_rCetDBxqfjJpdaUbCAOglxLAA\n    - type: rouge\n      value: 35.1581\n      name: ROUGE-LSUM\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDNhNTUyZjE4NjYxYjIzYThmMDM2YWNhM2QwYzY1ODI2ZTE3NmNjMmVhOTAzZjZlOWQwYzc1NzU2NDNjNzIxMyIsInZlcnNpb24iOjF9.cWlSbEBgrMN5D-fV_yL9geNMyMkIItcVO3wehNJPzFi3E0v1-4q8pnX-UgjLzto8X7JLi6as2V_HtZE4-C-CDw\n    - type: loss\n      value: 4.654905319213867\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYTc5Nzk0ODhiNWUzNTAxNzk2YzZmMjU2NDliY2UzOTYyYTdmZGEyYjI5NDNhOTE0MGUxOTgxMGVjMmNhM2UyMSIsInZlcnNpb24iOjF9.eBBAebcl3AwkrjR6a8BvoSjDfpw8LWTRFjyIFHVzspvoOKVfnO8_NB_UeR_K127OwXyoZ70Z7X_aKJOe-2kTDA\n    - type: gen_len\n      value: 186.2494\n      name: gen_len\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOWI2NjVlYjgwYWJiMjcyMDUzMzEwNDNjZTMxMDM0MjAzMzk1ZmIwY2Q1ZDQ2Y2M5NDBlMDEzYzFkNWEyNzJmNiIsInZlcnNpb24iOjF9.iZ1Iy7FuWL4GH7LS5EylVj5eZRC3L2ZsbYQapAkMNzR_VXPoMGvoM69Hp-kU7gW55tmz2V4Qxhvoz9cM8fciBA\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: cnn_dailymail\n      type: cnn_dailymail\n      config: 3.0.0\n      split: test\n    metrics:\n    - type: rouge\n      value: 32.8774\n      name: ROUGE-1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYWVlNjQzNWU1NTgyNTk2MzdhMDkyM2U3N2UzYzQ3ODJmOTJiMGViZDc0NzNiNDlmZGZmNTQzZmNjYTFjMzJmMCIsInZlcnNpb24iOjF9.qA54KJrGf79XCLnDrAMPp0saErVL_zKicLso9ZX2xxNdCANGExal5PFmmTT7aw7TUdkmUsNhmIRI9cBZ8J_1BA\n    - type: rouge\n      value: 13.3706\n      name: ROUGE-2\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDMzZWVjZmQ4ZWI2MWZmMGEzNjJhY2JmZjJhZTYwMTk2OTM2ODhlMmFmYmMxZGUyZWQzMmUxYzA0ZjJiMjcwYiIsInZlcnNpb24iOjF9.03Di-BfbZoWAVqRJc3x37Tn1Ae6vtZWymZL2w1ob8OQ8iOggYwmDmNQwv-bCXjT7fLjXYvh9uTndYsL05nj_Ag\n    - type: rouge\n      value: 20.4365\n      name: ROUGE-L\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYjI5YzdjZmM0YmZjYTU0OTg3ZTRjZWZkYTU2NzhlZjkwNGE2YmUzYzI1OThjMDUxOTcyNzk3ZTUyNmIzMWYzZCIsInZlcnNpb24iOjF9.LDg9lCKTh74kilxRBpunGSeOXJohaICXWjNf525ck-1h21AtjIQB8U7BTm80eyNRe7yIQpAlgOruCAxRqpTHDw\n    - type: rouge\n      value: 30.4408\n      name: ROUGE-LSUM\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTZhMGJjMzg0MzQxY2U2ZTIzYTYzOGRhMGEyYjY1ZjQyZjNmNGIwMzFjOWJjNzU2NWQzMzc1Y2IxYWZkZGY5YyIsInZlcnNpb24iOjF9.LkvaIEsw0U-osBR--46f7rsF-s1fcu19Z22DkvwiMwWJj9AnsUwDWNcCecIyi5tziQpUx0PpZEKyXAhCrVx1Bw\n    - type: loss\n      value: 5.3488945960998535\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTc4Y2JlZWRlNDRkOTI4ODQyZjBlMjU5NmUyZTZmNzJjYTg0NjM1YzI4NzUzYjhmODBkY2U4NGJiMTlhYTc2ZiIsInZlcnNpb24iOjF9.CB6oO5j3cKJPOelM8pwT2lTenp5bZTkBFC5MPYW_nus-O5F1s4DaY-gdSUK3baTkMXbQ2yqaI_g_QAfNVmqhDQ\n    - type: gen_len\n      value: 181.8326\n      name: gen_len\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOThmMGNlMGEwYjljMmNiZjdkMjc5NzZhNTYwMzAzOWFkYzA1NzZiNTIyN2IxNDJmOTk4MDliYzY2YjdjNGY4MSIsInZlcnNpb24iOjF9._buvRpxKLuKNNtOmALbFm3-nWCs2NCLh1l8gfVqDmKmv8JqJHQ27cdgZ4mklPLYOUhf6YWjby5_lp3ZGEctkCQ\n---\n# led-large-book-summary\n\n<a href=\"https://colab.research.google.com/gist/pszemraj/3eba944ddc9fc9a4a1bfb21e83b57620/summarization-token-batching.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\nThis model is a fine-tuned version of [allenai/led-large-16384](https://huggingface.co/allenai/led-large-16384) on the `BookSum` dataset (`kmfoda/booksum`). It aims to generalize well and be useful in summarizing lengthy text for both academic and everyday purposes. \n\n- Handles up to 16,384 tokens input\n- See the Colab demo linked above or try the [demo on Spaces](https://huggingface.co/spaces/pszemraj/summarize-long-text)\n\n> **Note:** Due to inference API timeout constraints, outputs may be truncated before the fully summary is returned (try python or the demo)\n\n---\n\n## Basic Usage\n\nTo improve summary quality, use `encoder_no_repeat_ngram_size=3` when calling the pipeline object. This setting encourages the model to utilize new vocabulary and construct an abstractive summary.\n\nLoad the model into a pipeline object:\n\n```python\nimport torch\nfrom transformers import pipeline\n\nhf_name = 'pszemraj/led-large-book-summary'\n\nsummarizer = pipeline(\n    \"summarization\",\n    hf_name,\n    device=0 if torch.cuda.is_available() else -1,\n)\n```\n\nFeed the text into the pipeline object:\n\n```python\nwall_of_text = \"your words here\"\n\nresult = summarizer(\n    wall_of_text,\n    min_length=16,\n    max_length=256,\n    no_repeat_ngram_size=3,\n    encoder_no_repeat_ngram_size=3,\n    repetition_penalty=3.5,\n    num_beams=4,\n    early_stopping=True,\n)\n```\n\n**Important:** For optimal summary quality, use the global attention mask when decoding, as demonstrated in [this community notebook](https://colab.research.google.com/drive/12INTTR6n64TzS4RrXZxMSXfrOd9Xzamo?usp=sharing), see the definition of `generate_answer(batch)`.\n\nIf you're facing computing constraints, consider using the base version [`pszemraj/led-base-book-summary`](https://huggingface.co/pszemraj/led-base-book-summary). \n\n---\n\n## Training Information\n\n### Data\n\nThe model was fine-tuned on the [booksum](https://arxiv.org/abs/2105.08209) dataset. During training, the `chapter`was the input col, while the `summary_text` was the output. \n\n### Procedure\n\nFine-tuning was run on the BookSum dataset across 13+ epochs. Notably, the final four epochs combined the training and validation sets as 'train' to enhance generalization.\n\n### Hyperparameters\n\nThe training process involved different settings across stages:\n\n- **Initial Three Epochs:** Low learning rate (5e-05), batch size of 1, 4 gradient accumulation steps, and a linear learning rate scheduler.\n- **In-between Epochs:** Learning rate reduced to 4e-05, increased batch size to 2, 16 gradient accumulation steps, and switched to a cosine learning rate scheduler with a 0.05 warmup ratio.\n- **Final Two Epochs:** Further reduced learning rate (2e-05), batch size reverted to 1, maintained gradient accumulation steps at 16, and continued with a cosine learning rate scheduler, albeit with a lower warmup ratio (0.03).\n\n### Versions\n\n- Transformers 4.19.2\n- Pytorch 1.11.0+cu113\n- Datasets 2.2.2\n- Tokenizers 0.12.1\n\n---\n\n## Simplified Usage with TextSum\n\nTo streamline the process of using this and other models, I've developed [a Python package utility](https://github.com/pszemraj/textsum) named `textsum`. This package offers simple interfaces for applying summarization models to text documents of arbitrary length. \n\nInstall TextSum:\n\n```bash\npip install textsum\n```\n\nThen use it in Python with this model:\n\n```python\nfrom textsum.summarize import Summarizer\n\nmodel_name = \"pszemraj/led-large-book-summary\"\nsummarizer = Summarizer(\n    model_name_or_path=model_name,  # you can use any Seq2Seq model on the Hub\n    token_batch_length=4096,  # tokens to batch summarize at a time, up to 16384\n)\nlong_string = \"This is a long string of text that will be summarized.\"\nout_str = summarizer.summarize_string(long_string)\nprint(f\"summary: {out_str}\")\n```\n\nCurrently implemented interfaces include a Python API, a Command-Line Interface (CLI), and a demo/web UI. \n\nFor detailed explanations and documentation, check the [README](https://github.com/pszemraj/textsum) or the [wiki](https://github.com/pszemraj/textsum/wiki)\n\n\n---\n\n## Related Models\n\nCheck out these other related models, also trained on the BookSum dataset:\n\n- [LED-large continued](https://huggingface.co/pszemraj/led-large-book-summary-continued) - experiment with further fine-tuning\n- [Long-T5-tglobal-base](https://huggingface.co/pszemraj/long-t5-tglobal-base-16384-book-summary)\n- [BigBird-Pegasus-Large-K](https://huggingface.co/pszemraj/bigbird-pegasus-large-K-booksum)\n- [Pegasus-X-Large](https://huggingface.co/pszemraj/pegasus-x-large-book-summary)\n- [Long-T5-tglobal-XL](https://huggingface.co/pszemraj/long-t5-tglobal-xl-16384-book-summary)\n\nThere are also other variants on other datasets etc on my hf profile, feel free to try them out :)\n\n\n---\n", "downloads": 2447, "likes": 112, "meta": {"datasets": ["kmfoda/booksum"], "language": ["en"], "license": ["apache-2.0", "bsd-3-clause"], "metrics": ["rouge"], "tags": ["summarization", "led", "summary", "longformer", "booksum", "long-document", "long-form"], "widget": [{"text": "large earthquakes along a given fault segment do not occur at random intervals because it takes time to accumulate the strain energy for the rupture. The rates at which tectonic plates move and accumulate strain at their boundaries are approximately uniform. Therefore, in first approximation, one may expect that large ruptures of the same fault segment will occur at approximately constant time intervals. If subsequent main shocks have different amounts of slip across the fault, then the recurrence time may vary, and the basic idea of periodic mainshocks must be modified. For great plate boundary ruptures the length and slip often vary by a factor of 2. Along the southern segment of the San Andreas fault the recurrence interval is 145 years with variations of several decades. The smaller the standard deviation of the average recurrence interval, the more specific could be the long term prediction of a future mainshock.", "example_title": "earthquakes"}, {"text": " A typical feed-forward neural field algorithm. Spatiotemporal coordinates are fed into a neural network that predicts values in the reconstructed domain. Then, this domain is mapped to the sensor domain where sensor measurements are available as supervision. Class and Section Problems Addressed Generalization (Section 2) Inverse problems, ill-posed problems, editability; symmetries. Hybrid Representations (Section 3) Computation & memory efficiency, representation capacity, editability: Forward Maps (Section 4) Inverse problems Network Architecture (Section 5) Spectral bias, integration & derivatives. Manipulating Neural Fields (Section 6) Edit ability, constraints, regularization. Table 2: The five classes of techniques in the neural field toolbox each addresses problems that arise in learning, inference, and control. (Section 3). We can supervise reconstruction via differentiable forward maps that transform Or project our domain (e.g, 3D reconstruction via 2D images; Section 4) With appropriate network architecture choices, we can overcome neural network spectral biases (blurriness) and efficiently compute derivatives and integrals (Section 5). Finally, we can manipulate neural fields to add constraints and regularizations, and to achieve editable representations (Section 6). Collectively, these classes constitute a 'toolbox' of techniques to help solve problems with neural fields There are three components in a conditional neural field: (1) An encoder or inference function â‚¬ that outputs the conditioning latent variable 2 given an observation 0 E(0) =2. 2 is typically a low-dimensional vector, and is often referred to aS a latent code Or feature code_ (2) A mapping function 4 between Z and neural field parameters O: Y(z) = O; (3) The neural field itself $. The encoder â‚¬ finds the most probable z given the observations O: argmaxz P(2/0). The decoder maximizes the inverse conditional probability to find the most probable 0 given Z: arg- max P(Olz). We discuss different encoding schemes with different optimality guarantees (Section 2.1.1), both global and local conditioning (Section 2.1.2), and different mapping functions Y (Section 2.1.3) 2. Generalization Suppose we wish to estimate a plausible 3D surface shape given a partial or noisy point cloud. We need a suitable prior over the sur- face in its reconstruction domain to generalize to the partial observations. A neural network expresses a prior via the function space of its architecture and parameters 0, and generalization is influenced by the inductive bias of this function space (Section 5).", "example_title": "scientific paper"}, {"text": " the big variety of data coming from diverse sources is one of the key properties of the big data phenomenon. It is, therefore, beneficial to understand how data is generated in various environments and scenarios, before looking at what should be done with this data and how to design the best possible architecture to accomplish this The evolution of IT architectures, described in Chapter 2, means that the data is no longer processed by a few big monolith systems, but rather by a group of services In parallel to the processing layer, the underlying data storage has also changed and became more distributed This, in turn, required a significant paradigm shift as the traditional approach to transactions (ACID) could no longer be supported. On top of this, cloud computing is becoming a major approach with the benefits of reducing costs and providing on-demand scalability but at the same time introducing concerns about privacy, data ownership, etc In the meantime the Internet continues its exponential growth: Every day both structured and unstructured data is published and available for processing: To achieve competitive advantage companies have to relate their corporate resources to external services, e.g. financial markets, weather forecasts, social media, etc While several of the sites provide some sort of API to access the data in a more orderly fashion; countless sources require advanced web mining and Natural Language Processing (NLP) processing techniques: Advances in science push researchers to construct new instruments for observing the universe O conducting experiments to understand even better the laws of physics and other domains. Every year humans have at their disposal new telescopes, space probes, particle accelerators, etc These instruments generate huge streams of data, which need to be stored and analyzed. The constant drive for efficiency in the industry motivates the introduction of new automation techniques and process optimization: This could not be done without analyzing the precise data that describe these processes. As more and more human tasks are automated, machines provide rich data sets, which can be analyzed in real-time to drive efficiency to new levels. Finally, it is now evident that the growth of the Internet of Things is becoming a major source of data. More and more of the devices are equipped with significant computational power and can generate a continuous data stream from their sensors. In the subsequent sections of this chapter, we will look at the domains described above to see what they generate in terms of data sets. We will compare the volumes but will also look at what is characteristic and important from their respective points of view. 3.1 The Internet is undoubtedly the largest database ever created by humans. While several well described; cleaned, and structured data sets have been made available through this medium, most of the resources are of an ambiguous, unstructured, incomplete or even erroneous nature. Still, several examples in the areas such as opinion mining, social media analysis, e-governance, etc, clearly show the potential lying in these resources. Those who can successfully mine and interpret the Internet data can gain unique insight and competitive advantage in their business An important area of data analytics on the edge of corporate IT and the Internet is Web Analytics.", "example_title": "data science textbook"}, {"text": "Transformer-based models have shown to be very useful for many NLP tasks. However, a major limitation of transformers-based models is its O(n^2)O(n 2) time & memory complexity (where nn is sequence length). Hence, it's computationally very expensive to apply transformer-based models on long sequences n > 512n>512. Several recent papers, e.g. Longformer, Performer, Reformer, Clustered attention try to remedy this problem by approximating the full attention matrix. You can checkout ðŸ¤—'s recent blog post in case you are unfamiliar with these models.\nBigBird (introduced in paper) is one of such recent models to address this issue. BigBird relies on block sparse attention instead of normal attention (i.e. BERT's attention) and can handle sequences up to a length of 4096 at a much lower computational cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts.\nBigBird RoBERTa-like model is now available in ðŸ¤—Transformers. The goal of this post is to give the reader an in-depth understanding of big bird implementation & ease one's life in using BigBird with ðŸ¤—Transformers. But, before going into more depth, it is important to remember that the BigBird's attention is an approximation of BERT's full attention and therefore does not strive to be better than BERT's full attention, but rather to be more efficient. It simply allows to apply transformer-based models to much longer sequences since BERT's quadratic memory requirement quickly becomes unbearable. Simply put, if we would have âˆž compute & âˆž time, BERT's attention would be preferred over block sparse attention (which we are going to discuss in this post).\nIf you wonder why we need more compute when working with longer sequences, this blog post is just right for you!\nSome of the main questions one might have when working with standard BERT-like attention include:\nDo all tokens really have to attend to all other tokens? Why not compute attention only over important tokens? How to decide what tokens are important? How to attend to just a few tokens in a very efficient way? In this blog post, we will try to answer those questions.\nWhat tokens should be attended to? We will give a practical example of how attention works by considering the sentence 'BigBird is now available in HuggingFace for extractive question answering'. In BERT-like attention, every word would simply attend to all other tokens.\nLet's think about a sensible choice of key tokens that a queried token actually only should attend to by writing some pseudo-code. Will will assume that the token available is queried and build a sensible list of key tokens to attend to.\n>>> # let's consider following sentence as an example >>> example = ['BigBird', 'is', 'now', 'available', 'in', 'HuggingFace', 'for', 'extractive', 'question', 'answering']\n>>> # further let's assume, we're trying to understand the representation of 'available' i.e. >>> query_token = 'available' >>> # We will initialize an empty `set` and fill up the tokens of our interest as we proceed in this section. >>> key_tokens = [] # => currently 'available' token doesn't have anything to attend Nearby tokens should be important because, in a sentence (sequence of words), the current word is highly dependent on neighboring past & future tokens. This intuition is the idea behind the concept of sliding attention.", "example_title": "bigbird blog intro"}, {"text": "The majority of available text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases. While relevant, such datasets will offer limited challenges for future generations of text summarization systems. We address these issues by introducing BookSum, a collection of datasets for long-form narrative summarization. Our dataset covers source documents from the literature domain, such as novels, plays and stories, and includes highly abstractive, human written summaries on three levels of granularity of increasing difficulty: paragraph-, chapter-, and book-level. The domain and structure of our dataset poses a unique set of challenges for summarization systems, which include: processing very long documents, non-trivial causal and temporal dependencies, and rich discourse structures. To facilitate future work, we trained and evaluated multiple extractive and abstractive summarization models as baselines for our dataset.", "example_title": "BookSum Abstract"}], "inference": {"parameters": {"max_length": 64, "min_length": 8, "no_repeat_ngram_size": 3, "early_stopping": true, "repetition_penalty": 3.5, "length_penalty": 0.3, "encoder_no_repeat_ngram_size": 3, "num_beams": 4}}, "model-index": [{"name": "pszemraj/led-large-book-summary", "results": [{"task": {"type": "summarization", "name": "Summarization"}, "dataset": {"name": "kmfoda/booksum", "type": "kmfoda/booksum", "config": "kmfoda--booksum", "split": "test"}, "metrics": [{"type": "rouge", "value": 31.7308, "name": "ROUGE-1", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjJmZjMxYTY0OGU3MzNjNmIzNmYyODNlNDg2ZGRhZDAzNTMwMDM5YWMxODc1OTc1ZWE3MzM2OTg1ODFhZDBkNCIsInZlcnNpb24iOjF9.B8BCKgySYVZW910_1zP0LfCpQYJbAe6loyWut76JlgZb2kV1_x9ybqtNESX0ka-lNqhYyXUNDpuS-7pTmsJVDg"}, {"type": "rouge", "value": 5.3311, "name": "ROUGE-2", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYzViMmY4ODFjYTc5ODk5MmRhMDQ3ZDRiYWQwMDg0OTk3ZTA4NDAxYTNiNDgyMmI4NDA3ZDMwYWViOTBkODBjNyIsInZlcnNpb24iOjF9.MOhJLDcgvv93mVFL1igIgIiTAH3b2Xa4gmBObq7RF44Mmu8Kxtd1KP7rOlDVFOrtrsooGPGsyE1GMCQ2kqeMDg"}, {"type": "rouge", "value": 16.1465, "name": "ROUGE-L", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNzNjMzEwMTliZGE3ZmQ4M2UxMDAyMTY3YzJjZmMyMDYyN2YyNDM0N2VhNzI1MDc1YTg4MTRjMmEzNjVkNTk1NCIsInZlcnNpb24iOjF9.XLJ-DVKiYLlbw5E5rWADKbzUzf5fNHhlTCWPCC5dU4NI9Yeh76aR7TPt36ZzLDwTBknnR8KHqlaF8F8YAvBUAg"}, {"type": "rouge", "value": 29.0883, "name": "ROUGE-LSUM", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTcwNzEwMmE5NjQxZTkzYmQyZDZmNzllYzYyNGI5OTMyNWMwNjdiM2I2YmM5YjdmY2E5OWQ3OTk3ZDA1MTc3YyIsInZlcnNpb24iOjF9.d6rFxjCB6RJNI_pn2DNNSjuZe4rdvj0RatkaTJRp5lP0F_AFfU5Zn9zRWzZJV7V-xMauIc4UhfdoLp9r_-CABA"}, {"type": "loss", "value": 4.815707206726074, "name": "loss", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTMwMTgxMmJkODY3MjkzOWJhMzJhOTIxMWVkODhjZmM0MWUzMWQ1N2JkZjRhOTQxNmU1YWVjYzQ0MDNlZWI3OSIsInZlcnNpb24iOjF9.mkBQHYhYFfDV6F4klXGJ1dSsF-pbCs-6F9zcw6IYznwmXUjtk7m5J4Zt4JAju5LKz4YizvEcUCl_L0WddnfvDA"}, {"type": "gen_len", "value": 154.9036, "name": "gen_len", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTc0ZmM1ZDM4MDE0MzY3MDM3OWJhNDkzZjJkZDdkMjU5M2JmMDJjYTIxODA1OTllNmY5ZWQzZDlmNWFiYzk4NiIsInZlcnNpb24iOjF9.VQ_O_xSTz870tnM08PJXQOwg9OsNNwI_HVX4S7AuW57_FzGGyRaWSuGE5SWzRS4Tur9YP0QxV4VV0Yoaoi3IAA"}]}, {"task": {"type": "summarization", "name": "Summarization"}, "dataset": {"name": "samsum", "type": "samsum", "config": "samsum", "split": "test"}, "metrics": [{"type": "rouge", "value": 33.4484, "name": "ROUGE-1", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTk4Yjg1YTc4YmY0MzBiZDU4ZjFhNzI4MjZkMWU1MzBlOWNlMjQ5ODMzY2YzYzRhYjJkMGUzNmI3ZjdkMzIzZSIsInZlcnNpb24iOjF9.AqS8A1OUiM0IZFBEGirv5F3Novk8lSUYSfPc3bYWLA6t-W7wgup3qA207eGbE5j9CkDWZ7QrSG1U6Z9A0sOqAA"}, {"type": "rouge", "value": 10.4249, "name": "ROUGE-2", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2U4NjUyNTFmOGM5OTlhZDMyMTlmM2E4OWI2NGFiMDAyMGJjMzRjNWNlMGEyYWFmNTE5ZWMxM2I0ZGZmNWNmOCIsInZlcnNpb24iOjF9.SgJcHJ4qoRWXFvFiwv1PUutWktvsxQNynVPEv-GtBgxd6WI7o561ONyco5U-5tcyE_1SbSCJzz-L-R-q3cvoDA"}, {"type": "rouge", "value": 24.5802, "name": "ROUGE-L", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZmQ5MDI5MzdiNGE5NDM0MmU5OThmZTBkNjkxMzg5N2IxNGVlODdhZTZhNjg3NzFjYWEyMzA3MTQxNjMyMjRkOCIsInZlcnNpb24iOjF9.Bg5dHqCcJjmxa-xGWNR5lD9g3quX7lKkH0pjiTd2xE5WiPoLLN2c0mYa2GovdW7__WnYwhhHC7es03jmvyZbCw"}, {"type": "rouge", "value": 29.8226, "name": "ROUGE-LSUM", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNGFhOTEwNGM1MmZkNDk2ZjQ1Y2MyNjM3MGI5MGY3MWVkM2I0MjU2NWFiYmEwMjE4MTJlZWIwOGQ2MjQ3YjgzYSIsInZlcnNpb24iOjF9.W_aQKs10oXQdKEczJBGM3iiwJgb-VaXTpyA3sGof5WbhHf9vITAQA-xvynh5LgKtXQ1zjx737hnHgjEsu_Y0Cw"}, {"type": "loss", "value": 4.176078796386719, "name": "loss", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2JhODQ5YTZkNDZkZGYyNGU2MzkxMWU5MTEwMGM2YmVjZTA5YzI5NTMxMDNhYjhlOTAxMzFiMDYwYmM0MjEzZCIsInZlcnNpb24iOjF9.OvZrPBOR5jhkoTGBgsInkH7j3_xpacXHDoT7UIXEnyXzadfBO-O-K6fjalLNZw8wSkbjHIFcL_6S_qTTxPsNAQ"}, {"type": "gen_len", "value": 65.4005, "name": "gen_len", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiM2NhYjc3ZjQzNDEwYmMzOTM0ODkyZTJhZWNhNzZhYmEyZTYxMzA2YTYzMWFjOTA5ZjlhYWMzODg3NzY1ZTUwYSIsInZlcnNpb24iOjF9.vk9bgmtQFeRwdY3VXjtrJr_5wUCIeoAkI3kO0cHxhxmJo6RvUnyXiut72FuB-mlLZvqgiNkaZ-u_bh0Z3DjuCw"}]}, {"task": {"type": "summarization", "name": "Summarization"}, "dataset": {"name": "billsum", "type": "billsum", "config": "default", "split": "test"}, "metrics": [{"type": "rouge", "value": 40.5843, "name": "ROUGE-1", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTVjMDkyMWZjYTQ0NzgzNGUxZjNiMTg3NjU1MWJlNTQ2MWQ1NjE1MDk1OTU4ZjJiNGQ5ODg3Y2VlMWUyMzllNyIsInZlcnNpb24iOjF9.OhqBcVIuHk7fzmdrsWMvUe1bLeVMZVstZUoZpP7C1vR-3aIDl7r6eBmPrt5w-KcNq5p4teNPBsq7oKzbd5ZgDQ"}, {"type": "rouge", "value": 17.3401, "name": "ROUGE-2", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNGQxYmQzMmE0OTcyNTM5NmMwNjIxNzYxZDcwMDFkYzJkOWY4YWY3NTdhZGRhZDdlMDAxNzcwODQ5OGM3Mzc1MCIsInZlcnNpb24iOjF9.Pksn25EEqvmx757N7Swrd4yXc_xU7-AMN9yNe8lrbBa-l1LoI_2PUASvnjML4f705cfuyMAfb0FkFp5WfER2AA"}, {"type": "rouge", "value": 25.1256, "name": "ROUGE-L", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjhjYzI5MDBiMjk2NTY3MDNmZTdiOGYwMTRlYjIwZjAwMjdlNTAyYzdhYTJlODQ4MjYzYmQ3MjRlYTA2YzhhZSIsInZlcnNpb24iOjF9.1jPepsweS2bzIqDverQzzhmhFGch7gpoEGFGqQ8zW7K10aUKWFX8lt-uZAmTa1Z5ZhzyXGBzc3dReFPhWRRJBg"}, {"type": "rouge", "value": 34.6619, "name": "ROUGE-LSUM", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiM2VkZDIxNWJjOTA0NzFjOTIwOTdjYjc1M2EyNDVjZjY2ZjY3MjIxNDk3YTc5YWExNzAwN2FhOTc1NjVhYjBkYiIsInZlcnNpb24iOjF9.8opqHSUckPohoSF9jfPTpXDz2AtDwvdMqOdIXx2kE1tkOcbLPbOBfcc8RhRR98y8S26yC6EYFhFnf03CV2ejAQ"}, {"type": "loss", "value": 4.792657375335693, "name": "loss", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYTY5ZTRkMGU3OGVkODMzMDU5OWE1NTM5YjA4NDliZDlmNzc2NzZjNjFmNTA3M2EwY2NmN2E0MWJmZjQ5ZDliMiIsInZlcnNpb24iOjF9.KCKdk8xt2NWcMmYKV3-9eVEsFm9MqGllSMu9QCFJFIQlnyNXllHKdBLouoaGQz8IRYXvZKH8_TLDPIQx-31jAg"}, {"type": "gen_len", "value": 163.9394, "name": "gen_len", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYzdkZDYyZGUzYmFkZmI2NjUwYmQ0MzZjMmIyZjI1YTFiMzM4OThiZjBiMzljOTVkZTgwMjA0NTE5OGM2YmFjMiIsInZlcnNpb24iOjF9.XyMZLUdkUIF32KTJMuv_bJswQCx_Tfg4Fx823cURUixSeoIKps8_a634AreZ3Z8kb7bfE_sFGh3rM9KWsMxlDw"}]}, {"task": {"type": "summarization", "name": "Summarization"}, "dataset": {"name": "multi_news", "type": "multi_news", "config": "default", "split": "test"}, "metrics": [{"type": "rouge", "value": 39.0834, "name": "ROUGE-1", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjYzMmVlMDM4MTNkMTI4MjAyMTU2YTg1ZWQwNTI1MmJlNGUwZmE1NTRmYTljZTQwY2RlMjcxOTgyZGMyYTc0ZiIsInZlcnNpb24iOjF9.6yuSr7UmsFatwqQ-mEO4gmsEtWI05kGB5Ib2pnl05H1OiPT2uUwmqdUytUw8KTx9u1jv9q0cTF1cL-n2kPEJAA"}, {"type": "rouge", "value": 11.4043, "name": "ROUGE-2", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMWI5N2U2ZWI1ODM2MWUwOTIzYTAzNmRhNDA2OWEzZWRjMGEzMjBmY2EwN2YyYzU1NWE0YjIyZDE3MWE0MmMxZCIsInZlcnNpb24iOjF9.wonuxbBl25TzEaHUH_E816nHJ1OSXKfkaq7eJzbLpsfeGwcDklxUSxZxRO7VBiBMaY3Qttf9ywmEIPp40HnpBA"}, {"type": "rouge", "value": 19.1813, "name": "ROUGE-L", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjU1NDZhN2NkMzZiZGJkODE4NDZiYjViOTZkNGMyNDlkNjBlZmFjYzU1N2IzMjFjYjY1MDU1Zjk2MzA0M2U4NyIsInZlcnNpb24iOjF9.bTCRzv3J9NiCh4aV23tAWGTvrdQCv_RS40zGwC4AJXtGS40cY7tJHYwBf9U9_rCetDBxqfjJpdaUbCAOglxLAA"}, {"type": "rouge", "value": 35.1581, "name": "ROUGE-LSUM", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDNhNTUyZjE4NjYxYjIzYThmMDM2YWNhM2QwYzY1ODI2ZTE3NmNjMmVhOTAzZjZlOWQwYzc1NzU2NDNjNzIxMyIsInZlcnNpb24iOjF9.cWlSbEBgrMN5D-fV_yL9geNMyMkIItcVO3wehNJPzFi3E0v1-4q8pnX-UgjLzto8X7JLi6as2V_HtZE4-C-CDw"}, {"type": "loss", "value": 4.654905319213867, "name": "loss", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYTc5Nzk0ODhiNWUzNTAxNzk2YzZmMjU2NDliY2UzOTYyYTdmZGEyYjI5NDNhOTE0MGUxOTgxMGVjMmNhM2UyMSIsInZlcnNpb24iOjF9.eBBAebcl3AwkrjR6a8BvoSjDfpw8LWTRFjyIFHVzspvoOKVfnO8_NB_UeR_K127OwXyoZ70Z7X_aKJOe-2kTDA"}, {"type": "gen_len", "value": 186.2494, "name": "gen_len", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOWI2NjVlYjgwYWJiMjcyMDUzMzEwNDNjZTMxMDM0MjAzMzk1ZmIwY2Q1ZDQ2Y2M5NDBlMDEzYzFkNWEyNzJmNiIsInZlcnNpb24iOjF9.iZ1Iy7FuWL4GH7LS5EylVj5eZRC3L2ZsbYQapAkMNzR_VXPoMGvoM69Hp-kU7gW55tmz2V4Qxhvoz9cM8fciBA"}]}, {"task": {"type": "summarization", "name": "Summarization"}, "dataset": {"name": "cnn_dailymail", "type": "cnn_dailymail", "config": "3.0.0", "split": "test"}, "metrics": [{"type": "rouge", "value": 32.8774, "name": "ROUGE-1", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYWVlNjQzNWU1NTgyNTk2MzdhMDkyM2U3N2UzYzQ3ODJmOTJiMGViZDc0NzNiNDlmZGZmNTQzZmNjYTFjMzJmMCIsInZlcnNpb24iOjF9.qA54KJrGf79XCLnDrAMPp0saErVL_zKicLso9ZX2xxNdCANGExal5PFmmTT7aw7TUdkmUsNhmIRI9cBZ8J_1BA"}, {"type": "rouge", "value": 13.3706, "name": "ROUGE-2", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDMzZWVjZmQ4ZWI2MWZmMGEzNjJhY2JmZjJhZTYwMTk2OTM2ODhlMmFmYmMxZGUyZWQzMmUxYzA0ZjJiMjcwYiIsInZlcnNpb24iOjF9.03Di-BfbZoWAVqRJc3x37Tn1Ae6vtZWymZL2w1ob8OQ8iOggYwmDmNQwv-bCXjT7fLjXYvh9uTndYsL05nj_Ag"}, {"type": "rouge", "value": 20.4365, "name": "ROUGE-L", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYjI5YzdjZmM0YmZjYTU0OTg3ZTRjZWZkYTU2NzhlZjkwNGE2YmUzYzI1OThjMDUxOTcyNzk3ZTUyNmIzMWYzZCIsInZlcnNpb24iOjF9.LDg9lCKTh74kilxRBpunGSeOXJohaICXWjNf525ck-1h21AtjIQB8U7BTm80eyNRe7yIQpAlgOruCAxRqpTHDw"}, {"type": "rouge", "value": 30.4408, "name": "ROUGE-LSUM", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTZhMGJjMzg0MzQxY2U2ZTIzYTYzOGRhMGEyYjY1ZjQyZjNmNGIwMzFjOWJjNzU2NWQzMzc1Y2IxYWZkZGY5YyIsInZlcnNpb24iOjF9.LkvaIEsw0U-osBR--46f7rsF-s1fcu19Z22DkvwiMwWJj9AnsUwDWNcCecIyi5tziQpUx0PpZEKyXAhCrVx1Bw"}, {"type": "loss", "value": 5.3488945960998535, "name": "loss", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTc4Y2JlZWRlNDRkOTI4ODQyZjBlMjU5NmUyZTZmNzJjYTg0NjM1YzI4NzUzYjhmODBkY2U4NGJiMTlhYTc2ZiIsInZlcnNpb24iOjF9.CB6oO5j3cKJPOelM8pwT2lTenp5bZTkBFC5MPYW_nus-O5F1s4DaY-gdSUK3baTkMXbQ2yqaI_g_QAfNVmqhDQ"}, {"type": "gen_len", "value": 181.8326, "name": "gen_len", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOThmMGNlMGEwYjljMmNiZjdkMjc5NzZhNTYwMzAzOWFkYzA1NzZiNTIyN2IxNDJmOTk4MDliYzY2YjdjNGY4MSIsInZlcnNpb24iOjF9._buvRpxKLuKNNtOmALbFm3-nWCs2NCLh1l8gfVqDmKmv8JqJHQ27cdgZ4mklPLYOUhf6YWjby5_lp3ZGEctkCQ"}]}]}]}, "inference_type": "huggingface"}
{"id": "csebuetnlp/mT5_multilingual_XLSum", "pipeline_tag": "summarization", "tags": ["transformers", "pytorch", "mt5", "text2text-generation", "summarization", "mT5", "am", "ar", "az", "bn", "my", "zh", "en", "fr", "gu", "ha", "hi", "ig", "id", "ja", "rn", "ko", "ky", "mr", "ne", "om", "ps", "fa", "pcm", "pt", "pa", "ru", "gd", "sr", "si", "so", "es", "sw", "ta", "te", "th", "ti", "tr", "uk", "ur", "uz", "vi", "cy", "yo", "dataset:csebuetnlp/xlsum", "model-index", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\ntags:\n- summarization\n- mT5\ndatasets:\n- csebuetnlp/xlsum\nlanguage:\n- am\n- ar\n- az\n- bn\n- my\n- zh\n- en\n- fr\n- gu\n- ha\n- hi\n- ig\n- id\n- ja\n- rn\n- ko\n- ky\n- mr\n- ne\n- om\n- ps\n- fa\n- pcm\n- pt\n- pa\n- ru\n- gd\n- sr\n- si\n- so\n- es\n- sw\n- ta\n- te\n- th\n- ti\n- tr\n- uk\n- ur\n- uz\n- vi\n- cy\n- yo\nlicenses:\n- cc-by-nc-sa-4.0\nwidget:\n- text: Videos that say approved vaccines are dangerous and cause autism, cancer or\n    infertility are among those that will be taken down, the company said.  The policy\n    includes the termination of accounts of anti-vaccine influencers.  Tech giants\n    have been criticised for not doing more to counter false health information on\n    their sites.  In July, US President Joe Biden said social media platforms were\n    largely responsible for people's scepticism in getting vaccinated by spreading\n    misinformation, and appealed for them to address the issue.  YouTube, which is\n    owned by Google, said 130,000 videos were removed from its platform since last\n    year, when it implemented a ban on content spreading misinformation about Covid\n    vaccines.  In a blog post, the company said it had seen false claims about Covid\n    jabs \"spill over into misinformation about vaccines in general\". The new policy\n    covers long-approved vaccines, such as those against measles or hepatitis B.  \"We're\n    expanding our medical misinformation policies on YouTube with new guidelines on\n    currently administered vaccines that are approved and confirmed to be safe and\n    effective by local health authorities and the WHO,\" the post said, referring to\n    the World Health Organization.\nmodel-index:\n- name: csebuetnlp/mT5_multilingual_XLSum\n  results:\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: xsum\n      type: xsum\n      config: default\n      split: test\n    metrics:\n    - type: rouge\n      value: 36.5002\n      name: ROUGE-1\n      verified: true\n    - type: rouge\n      value: 13.934\n      name: ROUGE-2\n      verified: true\n    - type: rouge\n      value: 28.9876\n      name: ROUGE-L\n      verified: true\n    - type: rouge\n      value: 28.9958\n      name: ROUGE-LSUM\n      verified: true\n    - type: loss\n      value: 2.0674800872802734\n      name: loss\n      verified: true\n    - type: gen_len\n      value: 26.9733\n      name: gen_len\n      verified: true\n---\n\n# mT5-multilingual-XLSum\n\nThis repository contains the mT5 checkpoint finetuned on the 45 languages of [XL-Sum](https://huggingface.co/datasets/csebuetnlp/xlsum) dataset. For finetuning details and scripts,\nsee the [paper](https://aclanthology.org/2021.findings-acl.413/) and the [official repository](https://github.com/csebuetnlp/xl-sum). \n\n\n## Using this model in `transformers` (tested on 4.11.0.dev0)\n\n```python\nimport re\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nWHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n\narticle_text = \"\"\"Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs \"spill over into misinformation about vaccines in general\". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  \"We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,\" the post said, referring to the World Health Organization.\"\"\"\n\nmodel_name = \"csebuetnlp/mT5_multilingual_XLSum\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ninput_ids = tokenizer(\n    [WHITESPACE_HANDLER(article_text)],\n    return_tensors=\"pt\",\n    padding=\"max_length\",\n    truncation=True,\n    max_length=512\n)[\"input_ids\"]\n\noutput_ids = model.generate(\n    input_ids=input_ids,\n    max_length=84,\n    no_repeat_ngram_size=2,\n    num_beams=4\n)[0]\n\nsummary = tokenizer.decode(\n    output_ids,\n    skip_special_tokens=True,\n    clean_up_tokenization_spaces=False\n)\n\nprint(summary)\n```\n\n## Benchmarks\n\nScores on the XL-Sum test sets are as follows:\n\nLanguage | ROUGE-1 / ROUGE-2 / ROUGE-L\n---------|----------------------------\nAmharic | 20.0485 / 7.4111 / 18.0753\nArabic | 34.9107 / 14.7937 / 29.1623\nAzerbaijani | 21.4227 / 9.5214 / 19.3331\nBengali | 29.5653 / 12.1095 / 25.1315\nBurmese | 15.9626 / 5.1477 / 14.1819\nChinese (Simplified) | 39.4071 / 17.7913 / 33.406\nChinese (Traditional) | 37.1866 / 17.1432 / 31.6184\nEnglish | 37.601 / 15.1536 / 29.8817\nFrench | 35.3398 / 16.1739 / 28.2041\nGujarati | 21.9619 / 7.7417 / 19.86\nHausa | 39.4375 / 17.6786 / 31.6667\nHindi | 38.5882 / 16.8802 / 32.0132\nIgbo | 31.6148 / 10.1605 / 24.5309\nIndonesian | 37.0049 / 17.0181 / 30.7561\nJapanese | 48.1544 / 23.8482 / 37.3636\nKirundi | 31.9907 / 14.3685 / 25.8305\nKorean | 23.6745 / 11.4478 / 22.3619\nKyrgyz | 18.3751 / 7.9608 / 16.5033\nMarathi | 22.0141 / 9.5439 / 19.9208\nNepali | 26.6547 / 10.2479 / 24.2847\nOromo | 18.7025 / 6.1694 / 16.1862\nPashto | 38.4743 / 15.5475 / 31.9065\nPersian | 36.9425 / 16.1934 / 30.0701\nPidgin | 37.9574 / 15.1234 / 29.872\nPortuguese | 37.1676 / 15.9022 / 28.5586\nPunjabi | 30.6973 / 12.2058 / 25.515\nRussian | 32.2164 / 13.6386 / 26.1689\nScottish Gaelic | 29.0231 / 10.9893 / 22.8814\nSerbian (Cyrillic) | 23.7841 / 7.9816 / 20.1379\nSerbian (Latin) | 21.6443 / 6.6573 / 18.2336\nSinhala | 27.2901 / 13.3815 / 23.4699\nSomali | 31.5563 / 11.5818 / 24.2232\nSpanish | 31.5071 / 11.8767 / 24.0746\nSwahili | 37.6673 / 17.8534 / 30.9146\nTamil | 24.3326 / 11.0553 / 22.0741\nTelugu | 19.8571 / 7.0337 / 17.6101\nThai | 37.3951 / 17.275 / 28.8796\nTigrinya | 25.321 / 8.0157 / 21.1729\nTurkish | 32.9304 / 15.5709 / 29.2622\nUkrainian | 23.9908 / 10.1431 / 20.9199\nUrdu | 39.5579 / 18.3733 / 32.8442\nUzbek | 16.8281 / 6.3406 / 15.4055\nVietnamese | 32.8826 / 16.2247 / 26.0844\nWelsh | 32.6599 / 11.596 / 26.1164\nYoruba | 31.6595 / 11.6599 / 25.0898\n\n\n\n## Citation\n\nIf you use this model, please cite the following paper:\n```\n@inproceedings{hasan-etal-2021-xl,\n    title = \"{XL}-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages\",\n    author = \"Hasan, Tahmid  and\n      Bhattacharjee, Abhik  and\n      Islam, Md. Saiful  and\n      Mubasshir, Kazi  and\n      Li, Yuan-Fang  and\n      Kang, Yong-Bin  and\n      Rahman, M. Sohel  and\n      Shahriyar, Rifat\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021\",\n    month = aug,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.findings-acl.413\",\n    pages = \"4693--4703\",\n}\n```", "downloads": 59093, "likes": 300, "meta": {"datasets": ["csebuetnlp/xlsum"], "language": ["am", "ar", "az", "bn", "my", "zh", "en", "fr", "gu", "ha", "hi", "ig", "id", "ja", "rn", "ko", "ky", "mr", "ne", "om", "ps", "fa", "pcm", "pt", "pa", "ru", "gd", "sr", "si", "so", "es", "sw", "ta", "te", "th", "ti", "tr", "uk", "ur", "uz", "vi", "cy", "yo"], "tags": ["summarization", "mT5"], "licenses": ["cc-by-nc-sa-4.0"], "widget": [{"text": "Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs \"spill over into misinformation about vaccines in general\". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  \"We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,\" the post said, referring to the World Health Organization."}], "model-index": [{"name": "csebuetnlp/mT5_multilingual_XLSum", "results": [{"task": {"type": "summarization", "name": "Summarization"}, "dataset": {"name": "xsum", "type": "xsum", "config": "default", "split": "test"}, "metrics": [{"type": "rouge", "value": 36.5002, "name": "ROUGE-1", "verified": false}, {"type": "rouge", "value": 13.934, "name": "ROUGE-2", "verified": false}, {"type": "rouge", "value": 28.9876, "name": "ROUGE-L", "verified": false}, {"type": "rouge", "value": 28.9958, "name": "ROUGE-LSUM", "verified": false}, {"type": "loss", "value": 2.0674800872802734, "name": "loss", "verified": false}, {"type": "gen_len", "value": 26.9733, "name": "gen_len", "verified": false}]}]}]}, "inference_type": "huggingface"}
{"id": "Ultralytics/YOLO11", "pipeline_tag": "object-detection", "tags": ["ultralytics", "en", "zh", "ja", "ru", "de", "fr", "es", "pt", "tr", "vi", "ar", "license:agpl-3.0", "model-index", "region:us"], "description": "---\nlicense: agpl-3.0\nlanguage:\n- en\n- zh\n- ja\n- ru\n- de\n- fr\n- es\n- pt\n- tr\n- vi\n- ar\nlibrary_name: ultralytics\nmodel-index:\n- name: ultralytics/yolo11\n  results:\n  - task:\n      type: object-detection\n    dataset:\n      name: coco\n      type: merve/coco\n      split: validation\n    metrics:\n    - type: precision\n      value: 54.7\n      name: mAP@0.5:0.95\n---\n\n<div align=\"center\">\n  <p>\n    <a href=\"https://www.ultralytics.com/events/yolovision\" target=\"_blank\">\n      <img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\" alt=\"YOLO Vision banner\"></a>\n  </p>\n\n[ä¸­æ–‡](https://docs.ultralytics.com/zh) | [í•œêµ­ì–´](https://docs.ultralytics.com/ko) | [æ—¥æœ¬èªž](https://docs.ultralytics.com/ja) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://docs.ultralytics.com/ru) | [Deutsch](https://docs.ultralytics.com/de) | [FranÃ§ais](https://docs.ultralytics.com/fr) | [EspaÃ±ol](https://docs.ultralytics.com/es) | [PortuguÃªs](https://docs.ultralytics.com/pt) | [TÃ¼rkÃ§e](https://docs.ultralytics.com/tr) | [Tiáº¿ng Viá»‡t](https://docs.ultralytics.com/vi) | [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](https://docs.ultralytics.com/ar) <br>\n\n<div style=\"display: flex; justify-content: center; flex-wrap: wrap; gap: 5px;\">\n    <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/264818686\"><img src=\"https://zenodo.org/badge/264818686.svg\" alt=\"Ultralytics YOLO Citation\"></a>\n    <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Ultralytics Docker Pulls\"></a>\n    <a href=\"https://discord.com/invite/ultralytics\"><img alt=\"Ultralytics Discord\" src=\"https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue\"></a>\n    <a href=\"https://community.ultralytics.com/\"><img alt=\"Ultralytics Forums\" src=\"https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue\"></a>\n    <a href=\"https://reddit.com/r/ultralytics\"><img alt=\"Ultralytics Reddit\" src=\"https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue\"></a>\n    <br>\n    <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run Ultralytics on Gradient\"></a>\n    <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open Ultralytics In Colab\"></a>\n    <a href=\"https://www.kaggle.com/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open Ultralytics In Kaggle\"></a>\n</div>\n<br>\n\n[Ultralytics](https://www.ultralytics.com/) [YOLO11](https://github.com/ultralytics/ultralytics) is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility. YOLO11 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and tracking, instance segmentation, image classification and pose estimation tasks.\n\nWe hope that the resources here will help you get the most out of YOLO. Please browse the Ultralytics <a href=\"https://docs.ultralytics.com/\">Docs</a> for details, raise an issue on <a href=\"https://github.com/ultralytics/ultralytics/issues/new/choose\">GitHub</a> for support, questions, or discussions, become a member of the Ultralytics <a href=\"https://discord.com/invite/ultralytics\">Discord</a>, <a href=\"https://reddit.com/r/ultralytics\">Reddit</a> and <a href=\"https://community.ultralytics.com/\">Forums</a>!\n\nTo request an Enterprise License please complete the form at [Ultralytics Licensing](https://www.ultralytics.com/license).\n\n<img width=\"100%\" src=\"https://github.com/user-attachments/assets/a311a4ed-bbf2-43b5-8012-5f183a28a845\" alt=\"YOLO11 performance plots\"></a>\n\n<div style=\"display: flex; justify-content: center; flex-wrap: wrap;\">\n  <a href=\"https://github.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"35%\" alt=\"Ultralytics GitHub\"></a>\n  <a href=\"https://www.linkedin.com/company/ultralytics/\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"35%\" alt=\"Ultralytics LinkedIn\"></a>\n  <a href=\"https://twitter.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"35%\" alt=\"Ultralytics Twitter\"></a>\n  <a href=\"https://youtube.com/ultralytics?sub_confirmation=1\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"35%\" alt=\"Ultralytics YouTube\"></a>\n  <a href=\"https://www.tiktok.com/@ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"35%\" alt=\"Ultralytics TikTok\"></a>\n  <a href=\"https://ultralytics.com/bilibili\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png\" width=\"35%\" alt=\"Ultralytics BiliBili\"></a>\n  <a href=\"https://discord.com/invite/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png\" width=\"35% alt=\"Ultralytics Discord\"></a>\n</div>\n</div>\n\n## <div align=\"center\">Documentation</div>\n\nSee below for a quickstart install and usage examples, and see our [Docs](https://docs.ultralytics.com/) for full documentation on training, validation, prediction and deployment.\n\n<details open>\n<summary>Install</summary>\n\nPip install the ultralytics package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://pepy.tech/project/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)\n\n```bash\npip install ultralytics\n```\n\nFor alternative installation methods including [Conda](https://anaconda.org/conda-forge/ultralytics), [Docker](https://hub.docker.com/r/ultralytics/ultralytics), and Git, please refer to the [Quickstart Guide](https://docs.ultralytics.com/quickstart/).\n\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/ultralytics?logo=condaforge)](https://anaconda.org/conda-forge/ultralytics) [![Docker Image Version](https://img.shields.io/docker/v/ultralytics/ultralytics?sort=semver&logo=docker)](https://hub.docker.com/r/ultralytics/ultralytics)\n\n</details>\n\n<details open>\n<summary>Usage</summary>\n\n### CLI\n\nYOLO may be used directly in the Command Line Interface (CLI) with a `yolo` command:\n\n```bash\nyolo predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'\n```\n\n`yolo` can be used for a variety of tasks and modes and accepts additional arguments, i.e. `imgsz=640`. See the YOLO [CLI Docs](https://docs.ultralytics.com/usage/cli/) for examples.\n\n### Python\n\nYOLO may also be used directly in a Python environment, and accepts the same [arguments](https://docs.ultralytics.com/usage/cfg/) as in the CLI example above:\n\n```python\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model\ntrain_results = model.train(\n    data=\"coco8.yaml\",  # path to dataset YAML\n    epochs=100,  # number of training epochs\n    imgsz=640,  # training image size\n    device=\"cpu\",  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\n)\n\n# Evaluate model performance on the validation set\nmetrics = model.val()\n\n# Perform object detection on an image\nresults = model(\"path/to/image.jpg\")\nresults[0].show()\n\n# Export the model to ONNX format\npath = model.export(format=\"onnx\")  # return path to exported model\n```\n\nSee YOLO [Python Docs](https://docs.ultralytics.com/usage/python/) for more examples.\n\n</details>\n\n## <div align=\"center\">Models</div>\n\nYOLO11 [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/) and [Pose](https://docs.ultralytics.com/tasks/pose/) models pretrained on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset are available here, as well as YOLO11 [Classify](https://docs.ultralytics.com/tasks/classify/) models pretrained on the [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) dataset. [Track](https://docs.ultralytics.com/modes/track/) mode is available for all Detect, Segment and Pose models.\n\n<img width=\"1024\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png\" alt=\"Ultralytics YOLO supported tasks\">\n\nAll [Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/cfg/models) download automatically from the latest Ultralytics [release](https://github.com/ultralytics/assets/releases) on first use.\n\n<details open><summary>Detection (COCO)</summary>\n\nSee [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for usage examples with these models trained on [COCO](https://docs.ultralytics.com/datasets/detect/coco/), which include 80 pre-trained classes.\n\n| Model                                                                                | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| ------------------------------------------------------------------------------------ | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt) | 640                   | 39.5                 | 56.1 Â± 0.8                     | 1.5 Â± 0.0                           | 2.6                | 6.5               |\n| [YOLO11s](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt) | 640                   | 47.0                 | 90.0 Â± 1.2                     | 2.5 Â± 0.0                           | 9.4                | 21.5              |\n| [YOLO11m](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt) | 640                   | 51.5                 | 183.2 Â± 2.0                    | 4.7 Â± 0.1                           | 20.1               | 68.0              |\n| [YOLO11l](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt) | 640                   | 53.4                 | 238.6 Â± 1.4                    | 6.2 Â± 0.1                           | 25.3               | 86.9              |\n| [YOLO11x](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt) | 640                   | 54.7                 | 462.8 Â± 6.7                    | 11.3 Â± 0.2                          | 56.9               | 194.9             |\n\n- **mAP<sup>val</sup>** values are for single-model single-scale on [COCO val2017](https://cocodataset.org/) dataset. <br>Reproduce by `yolo val detect data=coco.yaml device=0`\n- **Speed** averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. <br>Reproduce by `yolo val detect data=coco.yaml batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Segmentation (COCO)</summary>\n\nSee [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for usage examples with these models trained on [COCO-Seg](https://docs.ultralytics.com/datasets/segment/coco/), which include 80 pre-trained classes.\n\n| Model                                                                                        | size<br><sup>(pixels) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| -------------------------------------------------------------------------------------------- | --------------------- | -------------------- | --------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-seg.pt) | 640                   | 38.9                 | 32.0                  | 65.9 Â± 1.1                     | 1.8 Â± 0.0                           | 2.9                | 10.4              |\n| [YOLO11s-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-seg.pt) | 640                   | 46.6                 | 37.8                  | 117.6 Â± 4.9                    | 2.9 Â± 0.0                           | 10.1               | 35.5              |\n| [YOLO11m-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-seg.pt) | 640                   | 51.5                 | 41.5                  | 281.6 Â± 1.2                    | 6.3 Â± 0.1                           | 22.4               | 123.3             |\n| [YOLO11l-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-seg.pt) | 640                   | 53.4                 | 42.9                  | 344.2 Â± 3.2                    | 7.8 Â± 0.2                           | 27.6               | 142.2             |\n| [YOLO11x-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-seg.pt) | 640                   | 54.7                 | 43.8                  | 664.5 Â± 3.2                    | 15.8 Â± 0.7                          | 62.1               | 319.0             |\n\n- **mAP<sup>val</sup>** values are for single-model single-scale on [COCO val2017](https://cocodataset.org/) dataset. <br>Reproduce by `yolo val segment data=coco-seg.yaml device=0`\n- **Speed** averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. <br>Reproduce by `yolo val segment data=coco-seg.yaml batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Classification (ImageNet)</summary>\n\nSee [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for usage examples with these models trained on [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/), which include 1000 pretrained classes.\n\n| Model                                                                                        | size<br><sup>(pixels) | acc<br><sup>top1 | acc<br><sup>top5 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) at 640 |\n| -------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | ------------------------------ | ----------------------------------- | ------------------ | ------------------------ |\n| [YOLO11n-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt) | 224                   | 70.0             | 89.4             | 5.0 Â± 0.3                      | 1.1 Â± 0.0                           | 1.6                | 3.3                      |\n| [YOLO11s-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-cls.pt) | 224                   | 75.4             | 92.7             | 7.9 Â± 0.2                      | 1.3 Â± 0.0                           | 5.5                | 12.1                     |\n| [YOLO11m-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-cls.pt) | 224                   | 77.3             | 93.9             | 17.2 Â± 0.4                     | 2.0 Â± 0.0                           | 10.4               | 39.3                     |\n| [YOLO11l-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-cls.pt) | 224                   | 78.3             | 94.3             | 23.2 Â± 0.3                     | 2.8 Â± 0.0                           | 12.9               | 49.4                     |\n| [YOLO11x-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-cls.pt) | 224                   | 79.5             | 94.9             | 41.4 Â± 0.9                     | 3.8 Â± 0.0                           | 28.4               | 110.4                    |\n\n- **acc** values are model accuracies on the [ImageNet](https://www.image-net.org/) dataset validation set. <br>Reproduce by `yolo val classify data=path/to/ImageNet device=0`\n- **Speed** averaged over ImageNet val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. <br>Reproduce by `yolo val classify data=path/to/ImageNet batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Pose (COCO)</summary>\n\nSee [Pose Docs](https://docs.ultralytics.com/tasks/pose/) for usage examples with these models trained on [COCO-Pose](https://docs.ultralytics.com/datasets/pose/coco/), which include 1 pre-trained class, person.\n\n| Model                                                                                          | size<br><sup>(pixels) | mAP<sup>pose<br>50-95 | mAP<sup>pose<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| ---------------------------------------------------------------------------------------------- | --------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt) | 640                   | 50.0                  | 81.0               | 52.4 Â± 0.5                     | 1.7 Â± 0.0                           | 2.9                | 7.6               |\n| [YOLO11s-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-pose.pt) | 640                   | 58.9                  | 86.3               | 90.5 Â± 0.6                     | 2.6 Â± 0.0                           | 9.9                | 23.2              |\n| [YOLO11m-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-pose.pt) | 640                   | 64.9                  | 89.4               | 187.3 Â± 0.8                    | 4.9 Â± 0.1                           | 20.9               | 71.7              |\n| [YOLO11l-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-pose.pt) | 640                   | 66.1                  | 89.9               | 247.7 Â± 1.1                    | 6.4 Â± 0.1                           | 26.2               | 90.7              |\n| [YOLO11x-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-pose.pt) | 640                   | 69.5                  | 91.1               | 488.0 Â± 13.9                   | 12.1 Â± 0.2                          | 58.8               | 203.3             |\n\n- **mAP<sup>val</sup>** values are for single-model single-scale on [COCO Keypoints val2017](https://cocodataset.org/) dataset. <br>Reproduce by `yolo val pose data=coco-pose.yaml device=0`\n- **Speed** averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. <br>Reproduce by `yolo val pose data=coco-pose.yaml batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>OBB (DOTAv1)</summary>\n\nSee [OBB Docs](https://docs.ultralytics.com/tasks/obb/) for usage examples with these models trained on [DOTAv1](https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10/), which include 15 pre-trained classes.\n\n| Model                                                                                        | size<br><sup>(pixels) | mAP<sup>test<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| -------------------------------------------------------------------------------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-obb.pt) | 1024                  | 78.4               | 117.6 Â± 0.8                    | 4.4 Â± 0.0                           | 2.7                | 17.2              |\n| [YOLO11s-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-obb.pt) | 1024                  | 79.5               | 219.4 Â± 4.0                    | 5.1 Â± 0.0                           | 9.7                | 57.5              |\n| [YOLO11m-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-obb.pt) | 1024                  | 80.9               | 562.8 Â± 2.9                    | 10.1 Â± 0.4                          | 20.9               | 183.5             |\n| [YOLO11l-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-obb.pt) | 1024                  | 81.0               | 712.5 Â± 5.0                    | 13.5 Â± 0.6                          | 26.2               | 232.0             |\n| [YOLO11x-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-obb.pt) | 1024                  | 81.3               | 1408.6 Â± 7.7                   | 28.6 Â± 1.0                          | 58.8               | 520.2             |\n\n- **mAP<sup>test</sup>** values are for single-model multiscale on [DOTAv1](https://captain-whu.github.io/DOTA/index.html) dataset. <br>Reproduce by `yolo val obb data=DOTAv1.yaml device=0 split=test` and submit merged results to [DOTA evaluation](https://captain-whu.github.io/DOTA/evaluation.html).\n- **Speed** averaged over DOTAv1 val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. <br>Reproduce by `yolo val obb data=DOTAv1.yaml batch=1 device=0|cpu`\n\n</details>\n\n## <div align=\"center\">Integrations</div>\n\nOur key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with [Roboflow](https://roboflow.com/?ref=ultralytics), ClearML, [Comet](https://bit.ly/yolov8-readme-comet), Neural Magic and [OpenVINO](https://docs.ultralytics.com/integrations/openvino/), can optimize your AI workflow.\n\n<br>\n<a href=\"https://www.ultralytics.com/hub\" target=\"_blank\">\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\" alt=\"Ultralytics active learning integrations\"></a>\n<br>\n<br>\n\n<div style=\"display: flex; justify-content: center; flex-wrap: wrap; gap: 50px;\">\n  <a href=\"https://roboflow.com/?ref=ultralytics\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-roboflow.png\" width=\"50%\" alt=\"Roboflow logo\"></a>\n  <a href=\"https://clear.ml/\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-clearml.png\" width=\"50%\" alt=\"ClearML logo\"></a>\n  <a href=\"https://bit.ly/yolov8-readme-comet\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png\" width=\"50%\" alt=\"Comet ML logo\"></a>\n  <a href=\"https://bit.ly/yolov5-neuralmagic\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png\" width=\"50%\" alt=\"NeuralMagic logo\"></a>\n</div>\n\n\n|                                                           Roboflow                                                           |                                                 ClearML â­ NEW                                                  |                                                                       Comet â­ NEW                                                                        |                                          Neural Magic â­ NEW                                           |\n| :--------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------: |\n| Label and export your custom datasets directly to YOLO11 for training with [Roboflow](https://roboflow.com/?ref=ultralytics) | Automatically track, visualize and even remotely train YOLO11 using [ClearML](https://clear.ml/) (open-source!) | Free forever, [Comet](https://bit.ly/yolov5-readme-comet) lets you save YOLO11 models, resume training, and interactively visualize and debug predictions | Run YOLO11 inference up to 6x faster with [Neural Magic DeepSparse](https://bit.ly/yolov5-neuralmagic) |\n\n## <div align=\"center\">Ultralytics HUB</div>\n\nExperience seamless AI with [Ultralytics HUB](https://www.ultralytics.com/hub) â­, the all-in-one solution for data visualization, YOLO11 ðŸš€ model training and deployment, without any coding. Transform images into actionable insights and bring your AI visions to life with ease using our cutting-edge platform and user-friendly [Ultralytics App](https://www.ultralytics.com/app-install). Start your journey for **Free** now!\n\n<a href=\"https://www.ultralytics.com/hub\" target=\"_blank\">\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png\" alt=\"Ultralytics HUB preview image\"></a>\n\n## <div align=\"center\">Contribute</div>\n\nWe love your input! Ultralytics YOLO would not be possible without help from our community. Please see our [Contributing Guide](https://docs.ultralytics.com/help/contributing/) to get started, and fill out our [Survey](https://www.ultralytics.com/survey?utm_source=github&utm_medium=social&utm_campaign=Survey) to send us feedback on your experience. Thank you ðŸ™ to all our contributors!\n\n<!-- SVG image from https://opencollective.com/ultralytics/contributors.svg?width=990 -->\n\n<a href=\"https://github.com/ultralytics/ultralytics/graphs/contributors\">\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/im/image-contributors.png\" alt=\"Ultralytics open-source contributors\"></a>\n\n## <div align=\"center\">License</div>\n\nUltralytics offers two licensing options to accommodate diverse use cases:\n\n- **AGPL-3.0 License**: This [OSI-approved](https://opensource.org/license) open-source license is ideal for students and enthusiasts, promoting open collaboration and knowledge sharing. See the [LICENSE](https://github.com/ultralytics/ultralytics/blob/main/LICENSE) file for more details.\n- **Enterprise License**: Designed for commercial use, this license permits seamless integration of Ultralytics software and AI models into commercial goods and services, bypassing the open-source requirements of AGPL-3.0. If your scenario involves embedding our solutions into a commercial offering, reach out through [Ultralytics Licensing](https://www.ultralytics.com/license).\n\n## <div align=\"center\">Contact</div>\n\nFor Ultralytics bug reports and feature requests please visit [GitHub Issues](https://github.com/ultralytics/ultralytics/issues). Become a member of the Ultralytics [Discord](https://discord.com/invite/ultralytics), [Reddit](https://www.reddit.com/r/ultralytics/), or [Forums](https://community.ultralytics.com/) for asking questions, sharing projects, learning discussions, or for help with all things Ultralytics!\n\n<br>\n<div style=\"display: flex; justify-content: center; flex-wrap: wrap; gap: 5px; \">\n  <a href=\"https://github.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"35%\" alt=\"Ultralytics GitHub\"></a>\n  <a href=\"https://www.linkedin.com/company/ultralytics/\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"35%\" alt=\"Ultralytics LinkedIn\"></a>\n  <a href=\"https://twitter.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"35%\" alt=\"Ultralytics Twitter\"></a>\n  <a href=\"https://youtube.com/ultralytics?sub_confirmation=1\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"35%\" alt=\"Ultralytics YouTube\"></a>\n  <a href=\"https://www.tiktok.com/@ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"35%\" alt=\"Ultralytics TikTok\"></a>\n  <a href=\"https://ultralytics.com/bilibili\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png\" width=\"35%\" alt=\"Ultralytics BiliBili\"></a>\n  <a href=\"https://discord.com/invite/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png\" width=\"35% alt=\"Ultralytics Discord\"></a>\n</div>", "downloads": 5026, "likes": 87, "meta": {"language": ["en", "zh", "ja", "ru", "de", "fr", "es", "pt", "tr", "vi", "ar"], "library_name": "ultralytics", "license": "agpl-3.0", "model-index": [{"name": "ultralytics/yolo11", "results": [{"task": {"type": "object-detection"}, "dataset": {"name": "coco", "type": "merve/coco", "split": "validation"}, "metrics": [{"type": "precision", "value": 54.7, "name": "mAP@0.5:0.95", "verified": false}]}]}]}, "inference_type": "local"}
{"id": "openai/whisper-large-v3", "pipeline_tag": "automatic-speech-recognition", "tags": ["transformers", "pytorch", "jax", "safetensors", "whisper", "automatic-speech-recognition", "audio", "hf-asr-leaderboard", "en", "zh", "de", "es", "ru", "ko", "fr", "ja", "pt", "tr", "pl", "ca", "nl", "ar", "sv", "it", "id", "hi", "fi", "vi", "he", "uk", "el", "ms", "cs", "ro", "da", "hu", "ta", "no", "th", "ur", "hr", "bg", "lt", "la", "mi", "ml", "cy", "sk", "te", "fa", "lv", "bn", "sr", "az", "sl", "kn", "et", "mk", "br", "eu", "is", "hy", "ne", "mn", "bs", "kk", "sq", "sw", "gl", "mr", "pa", "si", "km", "sn", "yo", "so", "af", "oc", "ka", "be", "tg", "sd", "gu", "am", "yi", "lo", "uz", "fo", "ht", "ps", "tk", "nn", "mt", "sa", "lb", "my", "bo", "tl", "mg", "as", "tt", "haw", "ln", "ha", "ba", "jw", "su", "arxiv:2212.04356", "license:apache-2.0", "endpoints_compatible", "region:us"], "description": "---\nlanguage:\n- en\n- zh\n- de\n- es\n- ru\n- ko\n- fr\n- ja\n- pt\n- tr\n- pl\n- ca\n- nl\n- ar\n- sv\n- it\n- id\n- hi\n- fi\n- vi\n- he\n- uk\n- el\n- ms\n- cs\n- ro\n- da\n- hu\n- ta\n- false\n- th\n- ur\n- hr\n- bg\n- lt\n- la\n- mi\n- ml\n- cy\n- sk\n- te\n- fa\n- lv\n- bn\n- sr\n- az\n- sl\n- kn\n- et\n- mk\n- br\n- eu\n- is\n- hy\n- ne\n- mn\n- bs\n- kk\n- sq\n- sw\n- gl\n- mr\n- pa\n- si\n- km\n- sn\n- yo\n- so\n- af\n- oc\n- ka\n- be\n- tg\n- sd\n- gu\n- am\n- yi\n- lo\n- uz\n- fo\n- ht\n- ps\n- tk\n- nn\n- mt\n- sa\n- lb\n- my\n- bo\n- tl\n- mg\n- as\n- tt\n- haw\n- ln\n- ha\n- ba\n- jw\n- su\ntags:\n- audio\n- automatic-speech-recognition\n- hf-asr-leaderboard\nwidget:\n- example_title: Librispeech sample 1\n  src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\n  src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\npipeline_tag: automatic-speech-recognition\nlicense: apache-2.0\n---\n\n# Whisper\n\nWhisper is a state-of-the-art model for automatic speech recognition (ASR) and speech translation, proposed in the paper \n[Robust Speech Recognition via Large-Scale Weak Supervision](https://huggingface.co/papers/2212.04356) by Alec Radford \net al. from OpenAI. Trained on >5M hours of labeled data, Whisper demonstrates a strong ability to generalise to many \ndatasets and domains in a zero-shot setting.\n\nWhisper large-v3 has the same architecture as the previous [large](https://huggingface.co/openai/whisper-large) and [large-v2](https://huggingface.co/openai/whisper-large-v2) \nmodels, except for the following minor differences:\n\n1. The spectrogram input uses 128 Mel frequency bins instead of 80\n2. A new language token for Cantonese\n\nThe Whisper large-v3 model was trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled \naudio collected using Whisper [large-v2](https://huggingface.co/openai/whisper-large-v2) . The model was trained for 2.0 epochs over this mixture dataset.\n\nThe large-v3 model shows improved performance over a wide variety of languages, showing 10% to 20% reduction of errors \ncompared to Whisper [large-v2](https://huggingface.co/openai/whisper-large-v2) . For more details on the different checkpoints available, refer to the section [Model details](#model-details).\n\n**Disclaimer**: Content for this model card has partly been written by the ðŸ¤— Hugging Face team, and partly copied and \npasted from the original model card.\n\n## Usage\n\nWhisper large-v3 is supported in Hugging Face ðŸ¤— Transformers. To run the model, first install the Transformers \nlibrary. For this example, we'll also install ðŸ¤— Datasets to load toy audio dataset from the Hugging Face Hub, and \nðŸ¤— Accelerate to reduce the model loading time:\n\n```bash\npip install --upgrade pip\npip install --upgrade transformers datasets[audio] accelerate\n```\n\nThe model can be used with the [`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)\nclass to transcribe audios of arbitrary length:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"text\"])\n```\n\nTo transcribe a local audio file, simply pass the path to your audio file when you call the pipeline:\n\n```python\nresult = pipe(\"audio.mp3\")\n```\n\nMultiple audio files can be transcribed in parallel by specifying them as a list and setting the `batch_size` parameter:\n\n```python\nresult = pipe([\"audio_1.mp3\", \"audio_2.mp3\"], batch_size=2)\n```\n\nTransformers is compatible with all Whisper decoding strategies, such as temperature fallback and condition on previous \ntokens. The following example demonstrates how to enable these heuristics:\n\n```python\ngenerate_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\nresult = pipe(sample, generate_kwargs=generate_kwargs)\n```\n\nWhisper predicts the language of the source audio automatically. If the source audio language is known *a-priori*, it \ncan be passed as an argument to the pipeline:\n\n```python\nresult = pipe(sample, generate_kwargs={\"language\": \"english\"})\n```\n\nBy default, Whisper performs the task of *speech transcription*, where the source audio language is the same as the target\ntext language. To perform *speech translation*, where the target text is in English, set the task to `\"translate\"`:\n\n```python\nresult = pipe(sample, generate_kwargs={\"task\": \"translate\"})\n```\n\nFinally, the model can be made to predict timestamps. For sentence-level timestamps, pass the `return_timestamps` argument:\n\n```python\nresult = pipe(sample, return_timestamps=True)\nprint(result[\"chunks\"])\n```\n\nAnd for word-level timestamps:\n\n```python\nresult = pipe(sample, return_timestamps=\"word\")\nprint(result[\"chunks\"])\n```\n\nThe above arguments can be used in isolation or in combination. For example, to perform the task of speech transcription \nwhere the source audio is in French, and we want to return sentence-level timestamps, the following can be used:\n\n```python\nresult = pipe(sample, return_timestamps=True, generate_kwargs={\"language\": \"french\", \"task\": \"translate\"})\nprint(result[\"chunks\"])\n```\n\n<details>\n\n<summary> For more control over the generation parameters, use the model + processor API directly: </summary>\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import Audio, load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\ndataset = dataset.cast_column(\"audio\", Audio(processor.feature_extractor.sampling_rate))\nsample = dataset[0][\"audio\"]\n\ninputs = processor(\n    sample[\"array\"],\n    sampling_rate=sample[\"sampling_rate\"],\n    return_tensors=\"pt\",\n    truncation=False,\n    padding=\"longest\",\n    return_attention_mask=True,\n)\ninputs = inputs.to(device, dtype=torch_dtype)\n\ngen_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\npred_ids = model.generate(**inputs, **gen_kwargs)\npred_text = processor.batch_decode(pred_ids, skip_special_tokens=True, decode_with_timestamps=False)\n\nprint(pred_text)\n```\n\n</details>\n\n## Additional Speed & Memory Improvements\n\nYou can apply additional speed and memory improvements to Whisper to further reduce the inference speed and VRAM \nrequirements.\n\n### Chunked Long-Form\n\nWhisper has a receptive field of 30-seconds. To transcribe audios longer than this, one of two long-form algorithms are\nrequired:\n1. **Sequential:** uses a \"sliding window\" for buffered inference, transcribing 30-second slices one after the other\n2. **Chunked:** splits long audio files into shorter ones (with a small overlap between segments), transcribes each segment independently, and stitches the resulting transcriptions at the boundaries\n\nThe sequential long-form algorithm should be used in either of the following scenarios:\n1. Transcription accuracy is the most important factor, and speed is less of a consideration\n2. You are transcribing **batches** of long audio files, in which case the latency of sequential is comparable to chunked, while being up to 0.5% WER more accurate\n\nConversely, the chunked algorithm should be used when:\n1. Transcription speed is the most important factor\n2. You are transcribing a **single** long audio file\n\nBy default, Transformers uses the sequential algorithm. To enable the chunked algorithm, pass the `chunk_length_s` \nparameter to the `pipeline`. For large-v3, a chunk length of 30-seconds is optimal. To activate batching over long \naudio files, pass the argument `batch_size`:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    chunk_length_s=30,\n    batch_size=16,  # batch size for inference - set based on your device\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"text\"])\n```\n\n#### Torch compile\n\nThe Whisper forward pass is compatible with [`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html)\nfor 4.5x speed-ups.\n\n**Note:** `torch.compile` is currently not compatible with the Chunked long-form algorithm or Flash Attention 2 âš ï¸\n\n```python\nimport torch\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\ntorch.set_float32_matmul_precision(\"high\")\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n).to(device)\n\n# Enable static cache and compile the forward pass\nmodel.generation_config.cache_implementation = \"static\"\nmodel.generation_config.max_new_tokens = 256\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\n# 2 warmup steps\nfor _ in tqdm(range(2), desc=\"Warm-up step\"):\n    with sdpa_kernel(SDPBackend.MATH):\n        result = pipe(sample.copy(), generate_kwargs={\"min_new_tokens\": 256, \"max_new_tokens\": 256})\n\n# fast run\nwith sdpa_kernel(SDPBackend.MATH):\n    result = pipe(sample.copy())\n\nprint(result[\"text\"])\n```\n\n#### Flash Attention 2\n\nWe recommend using [Flash-Attention 2](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#flashattention-2) if your GPU supports it and you are not using [torch.compile](#torch-compile). \nTo do so, first install [Flash Attention](https://github.com/Dao-AILab/flash-attention):\n\n```\npip install flash-attn --no-build-isolation\n```\n\nThen pass `attn_implementation=\"flash_attention_2\"` to `from_pretrained`:\n\n```python\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"flash_attention_2\")\n```\n\n#### Torch Scale-Product-Attention (SDPA)\n\nIf your GPU does not support Flash Attention, we recommend making use of PyTorch [scaled dot-product attention (SDPA)](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html). \nThis attention implementation is activated **by default** for PyTorch versions 2.1.1 or greater. To check \nwhether you have a compatible PyTorch version, run the following Python code snippet:\n\n```python\nfrom transformers.utils import is_torch_sdpa_available\n\nprint(is_torch_sdpa_available())\n```\n\nIf the above returns `True`, you have a valid version of PyTorch installed and SDPA is activated by default. If it \nreturns `False`, you need to upgrade your PyTorch version according to the [official instructions](https://pytorch.org/get-started/locally/)\n\nOnce a valid PyTorch version is installed, SDPA is activated by default. It can also be set explicitly by specifying \n`attn_implementation=\"sdpa\"` as follows:\n\n```python\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"sdpa\")\n```\n\nFor more information about how to use the SDPA refer to the [Transformers SDPA documentation](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention).\n\n\n## Model details\n\nWhisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model. There are two\nflavours of Whisper model: English-only and multilingual. The English-only models were trained on the task of English \nspeech recognition. The multilingual models were trained simultaneously on multilingual speech recognition and speech \ntranslation. For speech recognition, the model predicts transcriptions in the *same* language as the audio. For speech \ntranslation, the model predicts transcriptions to a *different* language to the audio.\n\nWhisper checkpoints come in five configurations of varying model sizes. The smallest four are available as English-only \nand multilingual. The largest checkpoints are multilingual only. All ten of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:\n\n| Size     | Parameters | English-only                                         | Multilingual                                        |\n|----------|------------|------------------------------------------------------|-----------------------------------------------------|\n| tiny     | 39 M       | [âœ“](https://huggingface.co/openai/whisper-tiny.en)   | [âœ“](https://huggingface.co/openai/whisper-tiny)     |\n| base     | 74 M       | [âœ“](https://huggingface.co/openai/whisper-base.en)   | [âœ“](https://huggingface.co/openai/whisper-base)     |\n| small    | 244 M      | [âœ“](https://huggingface.co/openai/whisper-small.en)  | [âœ“](https://huggingface.co/openai/whisper-small)    |\n| medium   | 769 M      | [âœ“](https://huggingface.co/openai/whisper-medium.en) | [âœ“](https://huggingface.co/openai/whisper-medium)   |\n| large    | 1550 M     | x                                                    | [âœ“](https://huggingface.co/openai/whisper-large)    |\n| large-v2 | 1550 M     | x                                                    | [âœ“](https://huggingface.co/openai/whisper-large-v2) |\n| large-v3 | 1550 M     | x                                                    | [âœ“](https://huggingface.co/openai/whisper-large-v3) |\n\n\n## Fine-Tuning\n\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However, \nits predictive capabilities can be improved further for certain languages and tasks through *fine-tuning*. The blog \npost [Fine-Tune Whisper with ðŸ¤— Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step \nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only â€œintendedâ€ uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nThe large-v3 checkpoint is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled audio collected using Whisper large-v2. \n\nAs discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\n\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper modelsâ€™ transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box â€“ their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```", "downloads": 2897335, "likes": 4595, "meta": {"language": ["en", "zh", "de", "es", "ru", "ko", "fr", "ja", "pt", "tr", "pl", "ca", "nl", "ar", "sv", "it", "id", "hi", "fi", "vi", "he", "uk", "el", "ms", "cs", "ro", "da", "hu", "ta", "no", "th", "ur", "hr", "bg", "lt", "la", "mi", "ml", "cy", "sk", "te", "fa", "lv", "bn", "sr", "az", "sl", "kn", "et", "mk", "br", "eu", "is", "hy", "ne", "mn", "bs", "kk", "sq", "sw", "gl", "mr", "pa", "si", "km", "sn", "yo", "so", "af", "oc", "ka", "be", "tg", "sd", "gu", "am", "yi", "lo", "uz", "fo", "ht", "ps", "tk", "nn", "mt", "sa", "lb", "my", "bo", "tl", "mg", "as", "tt", "haw", "ln", "ha", "ba", "jw", "su"], "license": "apache-2.0", "pipeline_tag": "automatic-speech-recognition", "tags": ["audio", "automatic-speech-recognition", "hf-asr-leaderboard"], "widget": [{"example_title": "Librispeech sample 1", "src": "https://cdn-media.huggingface.co/speech_samples/sample1.flac"}, {"example_title": "Librispeech sample 2", "src": "https://cdn-media.huggingface.co/speech_samples/sample2.flac"}]}, "inference_type": "huggingface"}
{"id": "deepset/roberta-base-squad2", "pipeline_tag": "question-answering", "tags": ["transformers", "pytorch", "tf", "jax", "rust", "safetensors", "roberta", "question-answering", "en", "dataset:squad_v2", "base_model:FacebookAI/roberta-base", "base_model:finetune:FacebookAI/roberta-base", "license:cc-by-4.0", "model-index", "endpoints_compatible", "region:us"], "description": "---\nlanguage: en\nlicense: cc-by-4.0\ndatasets:\n- squad_v2\nbase_model:\n- FacebookAI/roberta-base\nmodel-index:\n- name: deepset/roberta-base-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 79.9309\n      name: Exact Match\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDhhNjg5YzNiZGQ1YTIyYTAwZGUwOWEzZTRiYzdjM2QzYjA3ZTUxNDM1NjE1MTUyMjE1MGY1YzEzMjRjYzVjYiIsInZlcnNpb24iOjF9.EH5JJo8EEFwU7osPz3s7qanw_tigeCFhCXjSfyN0Y1nWVnSfulSxIk_DbAEI5iE80V4EKLyp5-mYFodWvL2KDA\n    - type: f1\n      value: 82.9501\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjk5ZDYwOGQyNjNkMWI0OTE4YzRmOTlkY2JjNjQ0YTZkNTMzMzNkYTA0MDFmNmI3NjA3NjNlMjhiMDQ2ZjJjNSIsInZlcnNpb24iOjF9.DDm0LNTkdLbGsue58bg1aH_s67KfbcmkvL-6ZiI2s8IoxhHJMSf29H_uV2YLyevwx900t-MwTVOW3qfFnMMEAQ\n    - type: total\n      value: 11869\n      name: total\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGFkMmI2ODM0NmY5NGNkNmUxYWViOWYxZDNkY2EzYWFmOWI4N2VhYzY5MGEzMTVhOTU4Zjc4YWViOGNjOWJjMCIsInZlcnNpb24iOjF9.fexrU1icJK5_MiifBtZWkeUvpmFISqBLDXSQJ8E6UnrRof-7cU0s4tX_dIsauHWtUpIHMPZCf5dlMWQKXZuAAA\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad\n      type: squad\n      config: plain_text\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 85.289\n      name: Exact Match\n    - type: f1\n      value: 91.841\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: adversarial_qa\n      type: adversarial_qa\n      config: adversarialQA\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 29.5\n      name: Exact Match\n    - type: f1\n      value: 40.367\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_adversarial\n      type: squad_adversarial\n      config: AddOneSent\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 78.567\n      name: Exact Match\n    - type: f1\n      value: 84.469\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts amazon\n      type: squadshifts\n      config: amazon\n      split: test\n    metrics:\n    - type: exact_match\n      value: 69.924\n      name: Exact Match\n    - type: f1\n      value: 83.284\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts new_wiki\n      type: squadshifts\n      config: new_wiki\n      split: test\n    metrics:\n    - type: exact_match\n      value: 81.204\n      name: Exact Match\n    - type: f1\n      value: 90.595\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts nyt\n      type: squadshifts\n      config: nyt\n      split: test\n    metrics:\n    - type: exact_match\n      value: 82.931\n      name: Exact Match\n    - type: f1\n      value: 90.756\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts reddit\n      type: squadshifts\n      config: reddit\n      split: test\n    metrics:\n    - type: exact_match\n      value: 71.55\n      name: Exact Match\n    - type: f1\n      value: 82.939\n      name: F1\n---\n\n# roberta-base for Extractive QA \n\nThis is the [roberta-base](https://huggingface.co/roberta-base) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering. \nWe have also released a distilled version of this model called [deepset/tinyroberta-squad2](https://huggingface.co/deepset/tinyroberta-squad2). It has a comparable prediction quality and runs at twice the speed of [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2).\n\n\n## Overview\n**Language model:** roberta-base  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 4x Tesla v100\n\n## Hyperparameters\n\n```\nbatch_size = 96\nn_epochs = 2\nbase_LM_model = \"roberta-base\"\nmax_seq_len = 386\nlearning_rate = 3e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\n``` \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/roberta-base-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/roberta-base-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/).\n\n```\n\"exact\": 79.87029394424324,\n\"f1\": 82.91251169582613,\n\n\"total\": 11873,\n\"HasAns_exact\": 77.93522267206478,\n\"HasAns_f1\": 84.02838248389763,\n\"HasAns_total\": 5928,\n\"NoAns_exact\": 81.79983179142137,\n\"NoAns_f1\": 81.79983179142137,\n\"NoAns_total\": 5945\n```\n\n## Authors\n**Branden Chan:** branden.chan@deepset.ai  \n**Timo MÃ¶ller:** timo.moeller@deepset.ai  \n**Malte Pietsch:** malte.pietsch@deepset.ai  \n**Tanay Soni:**  tanay.soni@deepset.ai \n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product)\n- [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)", "downloads": 1569187, "likes": 888, "meta": {"base_model": ["FacebookAI/roberta-base"], "datasets": ["squad_v2"], "language": "en", "license": "cc-by-4.0", "model-index": [{"name": "deepset/roberta-base-squad2", "results": [{"task": {"type": "question-answering", "name": "Question Answering"}, "dataset": {"name": "squad_v2", "type": "squad_v2", "config": "squad_v2", "split": "validation"}, "metrics": [{"type": "exact_match", "value": 79.9309, "name": "Exact Match", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDhhNjg5YzNiZGQ1YTIyYTAwZGUwOWEzZTRiYzdjM2QzYjA3ZTUxNDM1NjE1MTUyMjE1MGY1YzEzMjRjYzVjYiIsInZlcnNpb24iOjF9.EH5JJo8EEFwU7osPz3s7qanw_tigeCFhCXjSfyN0Y1nWVnSfulSxIk_DbAEI5iE80V4EKLyp5-mYFodWvL2KDA"}, {"type": "f1", "value": 82.9501, "name": "F1", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjk5ZDYwOGQyNjNkMWI0OTE4YzRmOTlkY2JjNjQ0YTZkNTMzMzNkYTA0MDFmNmI3NjA3NjNlMjhiMDQ2ZjJjNSIsInZlcnNpb24iOjF9.DDm0LNTkdLbGsue58bg1aH_s67KfbcmkvL-6ZiI2s8IoxhHJMSf29H_uV2YLyevwx900t-MwTVOW3qfFnMMEAQ"}, {"type": "total", "value": 11869, "name": "total", "verified": true, "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGFkMmI2ODM0NmY5NGNkNmUxYWViOWYxZDNkY2EzYWFmOWI4N2VhYzY5MGEzMTVhOTU4Zjc4YWViOGNjOWJjMCIsInZlcnNpb24iOjF9.fexrU1icJK5_MiifBtZWkeUvpmFISqBLDXSQJ8E6UnrRof-7cU0s4tX_dIsauHWtUpIHMPZCf5dlMWQKXZuAAA"}]}, {"task": {"type": "question-answering", "name": "Question Answering"}, "dataset": {"name": "squad", "type": "squad", "config": "plain_text", "split": "validation"}, "metrics": [{"type": "exact_match", "value": 85.289, "name": "Exact Match", "verified": false}, {"type": "f1", "value": 91.841, "name": "F1", "verified": false}]}, {"task": {"type": "question-answering", "name": "Question Answering"}, "dataset": {"name": "adversarial_qa", "type": "adversarial_qa", "config": "adversarialQA", "split": "validation"}, "metrics": [{"type": "exact_match", "value": 29.5, "name": "Exact Match", "verified": false}, {"type": "f1", "value": 40.367, "name": "F1", "verified": false}]}, {"task": {"type": "question-answering", "name": "Question Answering"}, "dataset": {"name": "squad_adversarial", "type": "squad_adversarial", "config": "AddOneSent", "split": "validation"}, "metrics": [{"type": "exact_match", "value": 78.567, "name": "Exact Match", "verified": false}, {"type": "f1", "value": 84.469, "name": "F1", "verified": false}]}, {"task": {"type": "question-answering", "name": "Question Answering"}, "dataset": {"name": "squadshifts amazon", "type": "squadshifts", "config": "amazon", "split": "test"}, "metrics": [{"type": "exact_match", "value": 69.924, "name": "Exact Match", "verified": false}, {"type": "f1", "value": 83.284, "name": "F1", "verified": false}]}, {"task": {"type": "question-answering", "name": "Question Answering"}, "dataset": {"name": "squadshifts new_wiki", "type": "squadshifts", "config": "new_wiki", "split": "test"}, "metrics": [{"type": "exact_match", "value": 81.204, "name": "Exact Match", "verified": false}, {"type": "f1", "value": 90.595, "name": "F1", "verified": false}]}, {"task": {"type": "question-answering", "name": "Question Answering"}, "dataset": {"name": "squadshifts nyt", "type": "squadshifts", "config": "nyt", "split": "test"}, "metrics": [{"type": "exact_match", "value": 82.931, "name": "Exact Match", "verified": false}, {"type": "f1", "value": 90.756, "name": "F1", "verified": false}]}, {"task": {"type": "question-answering", "name": "Question Answering"}, "dataset": {"name": "squadshifts reddit", "type": "squadshifts", "config": "reddit", "split": "test"}, "metrics": [{"type": "exact_match", "value": 71.55, "name": "Exact Match", "verified": false}, {"type": "f1", "value": 82.939, "name": "F1", "verified": false}]}]}]}, "inference_type": "huggingface"}
{"id": "BAAI/bge-m3", "pipeline_tag": "sentence-similarity", "tags": ["sentence-transformers", "pytorch", "onnx", "xlm-roberta", "feature-extraction", "sentence-similarity", "arxiv:2402.03216", "arxiv:2004.04906", "arxiv:2106.14807", "arxiv:2107.05720", "arxiv:2004.12832", "license:mit", "autotrain_compatible", "text-embeddings-inference", "endpoints_compatible", "region:us"], "description": "---\npipeline_tag: sentence-similarity\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\nlicense: mit\n---\n\nFor more details please refer to our github repo: https://github.com/FlagOpen/FlagEmbedding\n\n# BGE-M3 ([paper](https://arxiv.org/pdf/2402.03216.pdf), [code](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3))\n\nIn this project, we introduce BGE-M3, which is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity. \n- Multi-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval. \n- Multi-Linguality: It can support more than 100 working languages. \n- Multi-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. \n\n\n\n**Some suggestions for retrieval pipeline in RAG**\n\nWe recommend to use the following pipeline: hybrid retrieval + re-ranking. \n- Hybrid retrieval leverages the strengths of various methods, offering higher accuracy and stronger generalization capabilities. \nA classic example: using both embedding retrieval and the BM25 algorithm. \nNow, you can try to use BGE-M3, which supports both embedding and sparse retrieval. \nThis allows you to obtain token weights (similar to the BM25) without any additional cost when generate dense embeddings.\nTo use hybrid retrieval, you can refer to [Vespa](https://github.com/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/mother-of-all-embedding-models-cloud.ipynb\n) and [Milvus](https://github.com/milvus-io/pymilvus/blob/master/examples/hello_hybrid_sparse_dense.py).\n\n- As cross-encoder models, re-ranker demonstrates higher accuracy than bi-encoder embedding model. \nUtilizing the re-ranking model (e.g., [bge-reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker), [bge-reranker-v2](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker)) after retrieval can further filter the selected text.\n\n\n## News:\n- 2024/7/1: **We update the MIRACL evaluation results of BGE-M3**. To reproduce the new results, you can refer to: [bge-m3_miracl_2cr](https://huggingface.co/datasets/hanhainebula/bge-m3_miracl_2cr). We have also updated our [paper](https://arxiv.org/pdf/2402.03216) on arXiv.\n  <details>\n  <summary> Details </summary>\n\n  The previous test results were lower because we mistakenly removed the passages that have the same id as the query from the search results. After correcting this mistake, the overall performance of BGE-M3 on MIRACL is higher than the previous results, but the experimental conclusion remains unchanged. The other results are not affected by this mistake. To reproduce the previous lower results, you need to add the `--remove-query` parameter when using `pyserini.search.faiss` or `pyserini.search.lucene` to search the passages.\n\n  </details>\n- 2024/3/20: **Thanks Milvus team!** Now you can use hybrid retrieval of bge-m3 in Milvus: [pymilvus/examples\n/hello_hybrid_sparse_dense.py](https://github.com/milvus-io/pymilvus/blob/master/examples/hello_hybrid_sparse_dense.py).\n- 2024/3/8: **Thanks for the [experimental results](https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05) from @[Yannael](https://huggingface.co/Yannael). In this benchmark, BGE-M3 achieves top performance in both English and other languages, surpassing models such as OpenAI.**\n- 2024/3/2: Release unified fine-tuning [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/unified_finetune) and [data](https://huggingface.co/datasets/Shitao/bge-m3-data) \n- 2024/2/6: We release the [MLDR](https://huggingface.co/datasets/Shitao/MLDR) (a long document retrieval dataset covering 13 languages) and [evaluation pipeline](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB/MLDR). \n- 2024/2/1: **Thanks for the excellent tool from Vespa.** You can easily use multiple modes of BGE-M3 following this [notebook](https://github.com/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/mother-of-all-embedding-models-cloud.ipynb)\n\n\n## Specs\n\n- Model  \n\n| Model Name |  Dimension | Sequence Length | Introduction |\n|:----:|:---:|:---:|:---:|\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3) | 1024 | 8192 | multilingual; unified fine-tuning (dense, sparse, and colbert) from bge-m3-unsupervised|\n| [BAAI/bge-m3-unsupervised](https://huggingface.co/BAAI/bge-m3-unsupervised) | 1024 | 8192 | multilingual; contrastive learning from bge-m3-retromae |\n| [BAAI/bge-m3-retromae](https://huggingface.co/BAAI/bge-m3-retromae) | -- | 8192 | multilingual; extend the max_length of [xlm-roberta](https://huggingface.co/FacebookAI/xlm-roberta-large) to 8192 and further pretrained via [retromae](https://github.com/staoxiao/RetroMAE)| \n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) | 1024 | 512 | English model | \n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |  768 | 512 | English model | \n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |  384 | 512 | English model | \n\n- Data\n\n|                          Dataset                           |                   Introduction                    |\n|:----------------------------------------------------------:|:-------------------------------------------------:|\n|    [MLDR](https://huggingface.co/datasets/Shitao/MLDR)     | Docuemtn Retrieval Dataset, covering 13 languages |\n| [bge-m3-data](https://huggingface.co/datasets/Shitao/bge-m3-data) |          Fine-tuning data used by bge-m3          |\n\n\n\n## FAQ\n\n**1. Introduction for different retrieval methods**\n\n- Dense retrieval: map the text into a single embedding, e.g., [DPR](https://arxiv.org/abs/2004.04906), [BGE-v1.5](https://github.com/FlagOpen/FlagEmbedding)\n- Sparse retrieval (lexical matching): a vector of size equal to the vocabulary, with the majority of positions set to zero, calculating a weight only for tokens present in the text. e.g., BM25, [unicoil](https://arxiv.org/pdf/2106.14807.pdf), and [splade](https://arxiv.org/abs/2107.05720)\n- Multi-vector retrieval: use multiple vectors to represent a text, e.g., [ColBERT](https://arxiv.org/abs/2004.12832).\n\n\n**2. How to use BGE-M3 in other projects?**\n\nFor embedding retrieval, you can employ the BGE-M3 model using the same approach as BGE. \nThe only difference is that the BGE-M3 model no longer requires adding instructions to the queries. \n\nFor hybrid retrieval, you can use [Vespa](https://github.com/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/mother-of-all-embedding-models-cloud.ipynb\n) and [Milvus](https://github.com/milvus-io/pymilvus/blob/master/examples/hello_hybrid_sparse_dense.py).\n\n\n**3. How to fine-tune bge-M3 model?**\n\nYou can follow the common in this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) \nto fine-tune the dense embedding.\n\nIf you want to fine-tune all embedding function of m3 (dense, sparse and colbert), you can refer to the [unified_fine-tuning example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/unified_finetune)\n\n\n\n\n\n\n## Usage\n\nInstall: \n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\npip install -e .\n```\nor: \n```\npip install -U FlagEmbedding\n```\n\n\n\n### Generate Embedding for text\n\n- Dense Embedding\n```python\nfrom FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel('BAAI/bge-m3',  \n                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n\nembeddings_1 = model.encode(sentences_1, \n                            batch_size=12, \n                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n                            )['dense_vecs']\nembeddings_2 = model.encode(sentences_2)['dense_vecs']\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n# [[0.6265, 0.3477], [0.3499, 0.678 ]]\n```\nYou also can use sentence-transformers and huggingface transformers to generate dense embeddings.\nRefer to [baai_general_embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding#usage) for details.\n\n\n- Sparse Embedding (Lexical Weight)\n```python\nfrom FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n\noutput_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=False)\noutput_2 = model.encode(sentences_2, return_dense=True, return_sparse=True, return_colbert_vecs=False)\n\n# you can see the weight for each token:\nprint(model.convert_id_to_token(output_1['lexical_weights']))\n# [{'What': 0.08356, 'is': 0.0814, 'B': 0.1296, 'GE': 0.252, 'M': 0.1702, '3': 0.2695, '?': 0.04092}, \n#  {'De': 0.05005, 'fin': 0.1368, 'ation': 0.04498, 'of': 0.0633, 'BM': 0.2515, '25': 0.3335}]\n\n\n# compute the scores via lexical mathcing\nlexical_scores = model.compute_lexical_matching_score(output_1['lexical_weights'][0], output_2['lexical_weights'][0])\nprint(lexical_scores)\n# 0.19554901123046875\n\nprint(model.compute_lexical_matching_score(output_1['lexical_weights'][0], output_1['lexical_weights'][1]))\n# 0.0\n```\n\n- Multi-Vector (ColBERT)\n```python\nfrom FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) \n\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n\noutput_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=True)\noutput_2 = model.encode(sentences_2, return_dense=True, return_sparse=True, return_colbert_vecs=True)\n\nprint(model.colbert_score(output_1['colbert_vecs'][0], output_2['colbert_vecs'][0]))\nprint(model.colbert_score(output_1['colbert_vecs'][0], output_2['colbert_vecs'][1]))\n# 0.7797\n# 0.4620\n```\n\n\n### Compute score for text pairs\nInput a list of text pairs, you can get the scores computed by different methods.\n```python\nfrom FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) \n\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n\nsentence_pairs = [[i,j] for i in sentences_1 for j in sentences_2]\n\nprint(model.compute_score(sentence_pairs, \n                          max_passage_length=128, # a smaller max length leads to a lower latency\n                          weights_for_different_modes=[0.4, 0.2, 0.4])) # weights_for_different_modes(w) is used to do weighted sum: w[0]*dense_score + w[1]*sparse_score + w[2]*colbert_score\n\n# {\n#   'colbert': [0.7796499729156494, 0.4621465802192688, 0.4523794651031494, 0.7898575067520142], \n#   'sparse': [0.195556640625, 0.00879669189453125, 0.0, 0.1802978515625], \n#   'dense': [0.6259765625, 0.347412109375, 0.349853515625, 0.67822265625], \n#   'sparse+dense': [0.482503205537796, 0.23454029858112335, 0.2332356721162796, 0.5122477412223816], \n#   'colbert+sparse+dense': [0.6013619303703308, 0.3255828022956848, 0.32089319825172424, 0.6232916116714478]\n# }\n```\n\n\n\n\n## Evaluation  \n\nWe provide the evaluation script for [MKQA](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB/MKQA) and [MLDR](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB/MLDR)\n\n### Benchmarks from the open-source community\n  ![avatar](./imgs/others.webp)\n The BGE-M3 model emerged as the top performer on this benchmark (OAI is short for OpenAI). \n  For more details, please refer to the [article](https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05) and [Github Repo](https://github.com/Yannael/multilingual-embeddings)\n\n\n### Our results\n- Multilingual (Miracl dataset) \n\n![avatar](./imgs/miracl.jpg)\n\n- Cross-lingual (MKQA dataset)\n\n![avatar](./imgs/mkqa.jpg)\n\n- Long Document Retrieval\n  - MLDR:   \n  ![avatar](./imgs/long.jpg)\n  Please note that [MLDR](https://huggingface.co/datasets/Shitao/MLDR) is a document retrieval dataset we constructed via LLM, \n  covering 13 languages, including test set, validation set, and training set. \n  We utilized the training set from MLDR to enhance the model's long document retrieval capabilities. \n  Therefore, comparing baselines with `Dense w.o.long`(fine-tuning without long document dataset) is more equitable. \n  Additionally, this long document retrieval dataset will be open-sourced to address the current lack of open-source multilingual long text retrieval datasets.\n  We believe that this data will be helpful for the open-source community in training document retrieval models.\n\n  - NarritiveQA:  \n  ![avatar](./imgs/nqa.jpg)\n\n- Comparison with BM25  \n\nWe utilized Pyserini to implement BM25, and the test results can be reproduced by this [script](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB/MLDR#bm25-baseline).\nWe tested BM25 using two different tokenizers: \none using Lucene Analyzer and the other using the same tokenizer as M3 (i.e., the tokenizer of xlm-roberta). \nThe results indicate that BM25 remains a competitive baseline, \nespecially in long document retrieval.\n\n![avatar](./imgs/bm25.jpg)\n\n\n\n## Training\n- Self-knowledge Distillation: combining multiple outputs from different \nretrieval modes as reward signal to enhance the performance of single mode(especially for sparse retrieval and multi-vec(colbert) retrival)\n- Efficient Batching: Improve the efficiency when fine-tuning on long text. \nThe small-batch strategy is simple but effective, which also can used to fine-tune large embedding model.\n- MCLS: A simple method to improve the performance on long text without fine-tuning. \nIf you have no enough resource to fine-tuning model with long text, the method is useful.\n\nRefer to our [report](https://arxiv.org/pdf/2402.03216.pdf) for more details. \n\n\n\n\n\n\n## Acknowledgement\n\nThanks to the authors of open-sourced datasets, including Miracl, MKQA, NarritiveQA, etc. \nThanks to the open-sourced libraries like [Tevatron](https://github.com/texttron/tevatron), [Pyserini](https://github.com/castorini/pyserini).\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge-m3,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n", "downloads": 4031544, "likes": 2177, "meta": {"license": "mit", "pipeline_tag": "sentence-similarity", "tags": ["sentence-transformers", "feature-extraction", "sentence-similarity"]}, "inference_type": "huggingface"}
{"id": "Salesforce/blip-image-captioning-base", "pipeline_tag": "image-to-text", "tags": ["transformers", "pytorch", "tf", "blip", "image-text-to-text", "image-captioning", "image-to-text", "arxiv:2201.12086", "license:bsd-3-clause", "endpoints_compatible", "region:us"], "description": "---\npipeline_tag: image-to-text\ntags:\n- image-captioning\nlanguages:\n- en\nlicense: bsd-3-clause\n---\n\n# BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\n\nModel card for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone).\n\n| ![BLIP.gif](https://cdn-uploads.huggingface.co/production/uploads/1670928184033-62441d1d9fdefb55a0b7d12c.gif) |\n|:--:|\n| <b> Pull figure from BLIP official repo | Image source: https://github.com/salesforce/BLIP </b>|\n\n## TL;DR\n\nAuthors from the [paper](https://arxiv.org/abs/2201.12086) write in the abstract:\n\n*Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.*\n\n## Usage\n\nYou can use this model for conditional and un-conditional image captioning\n\n### Using the Pytorch model\n\n#### Running the model on CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n```\n</details>\n\n#### Running the model on GPU\n\n##### In full precision \n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n```\n</details>\n\n##### In half precision (`float16`)\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float16).to(\"cuda\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n```\n</details>\n\n## Ethical Considerations\nThis release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact peopleâ€™s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.\n\n\n## BibTex and citation info\n\n```\n@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n", "downloads": 2515982, "likes": 745, "meta": {"license": "bsd-3-clause", "pipeline_tag": "image-to-text", "tags": ["image-captioning"], "languages": ["en"]}, "inference_type": "local"}
{"id": "microsoft/trocr-base-printed", "pipeline_tag": "image-to-text", "tags": ["transformers", "pytorch", "safetensors", "vision-encoder-decoder", "image-text-to-text", "trocr", "image-to-text", "arxiv:2109.10282", "endpoints_compatible", "region:us"], "description": "---\ntags:\n- trocr\n- image-to-text\nwidget:\n- src: https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X00016469612_1.jpg\n  example_title: Printed 1\n- src: https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X51005255805_7.jpg\n  example_title: Printed 2\n- src: https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X51005745214_6.jpg\n  example_title: Printed 3\n---\n\n# TrOCR (base-sized model, fine-tuned on SROIE) \n\nTrOCR model fine-tuned on the [SROIE dataset](https://rrc.cvc.uab.es/?ch=13). It was introduced in the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Li et al. and first released in [this repository](https://github.com/microsoft/unilm/tree/master/trocr). \n\nDisclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.\n\n## Intended uses & limitations\n\nYou can use the raw model for optical character recognition (OCR) on single text-line images. See the [model hub](https://huggingface.co/models?search=microsoft/trocr) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n\n# load image from the IAM database (actually this model is meant to be used on printed text)\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{li2021trocr,\n      title={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models}, \n      author={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\n      year={2021},\n      eprint={2109.10282},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "downloads": 281715, "likes": 179, "meta": {"tags": ["trocr", "image-to-text"], "widget": [{"src": "https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X00016469612_1.jpg", "example_title": "Printed 1"}, {"src": "https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X51005255805_7.jpg", "example_title": "Printed 2"}, {"src": "https://layoutlm.blob.core.windows.net/trocr/dataset/SROIE2019Task2Crop/train/X51005745214_6.jpg", "example_title": "Printed 3"}]}, "inference_type": "local"}
{"id": "microsoft/trocr-base-handwritten", "pipeline_tag": "image-to-text", "tags": ["transformers", "pytorch", "safetensors", "vision-encoder-decoder", "image-text-to-text", "trocr", "image-to-text", "arxiv:2109.10282", "license:mit", "endpoints_compatible", "region:us"], "description": "---\ntags:\n- trocr\n- image-to-text\nwidget:\n- src: https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\n  example_title: Note 1\n- src: https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSoolxi9yWGAT5SLZShv8vVd0bz47UWRzQC19fDTeE8GmGv_Rn-PCF1pP1rrUx8kOjA4gg&usqp=CAU\n  example_title: Note 2\n- src: https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRNYtTuSBpZPV_nkBYPMFwVVD9asZOPgHww4epu9EqWgDmXW--sE2o8og40ZfDGo87j5w&usqp=CAU\n  example_title: Note 3\nlicense: mit\n---\n\n# TrOCR (base-sized model, fine-tuned on IAM) \n\nTrOCR model fine-tuned on the [IAM dataset](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database). It was introduced in the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Li et al. and first released in [this repository](https://github.com/microsoft/unilm/tree/master/trocr). \n\nDisclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.\n\n## Intended uses & limitations\n\nYou can use the raw model for optical character recognition (OCR) on single text-line images. See the [model hub](https://huggingface.co/models?search=microsoft/trocr) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n\n# load image from the IAM database\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\npixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{li2021trocr,\n      title={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models}, \n      author={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\n      year={2021},\n      eprint={2109.10282},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "downloads": 185361, "likes": 414, "meta": {"license": "mit", "tags": ["trocr", "image-to-text"], "widget": [{"src": "https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg", "example_title": "Note 1"}, {"src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSoolxi9yWGAT5SLZShv8vVd0bz47UWRzQC19fDTeE8GmGv_Rn-PCF1pP1rrUx8kOjA4gg&usqp=CAU", "example_title": "Note 2"}, {"src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRNYtTuSBpZPV_nkBYPMFwVVD9asZOPgHww4epu9EqWgDmXW--sE2o8og40ZfDGo87j5w&usqp=CAU", "example_title": "Note 3"}]}, "inference_type": "local"}
{"id": "breezedeus/pix2text-mfr", "pipeline_tag": "image-to-text", "tags": ["transformers", "onnx", "vision-encoder-decoder", "image-text-to-text", "latex-ocr", "math-ocr", "math-formula-recognition", "mfr", "pix2text", "p2t", "image-to-text", "doi:10.57967/hf/1833", "license:mit", "endpoints_compatible", "region:us"], "description": "---\ntags:\n- latex-ocr\n- math-ocr\n- math-formula-recognition\n- mfr\n- pix2text\n- p2t\n- image-to-text\nlicense: mit\nlibrary_name: transformers\n---\n\n# Model Card: Pix2Text-MFR\nMathematical Formula Recognition (MFR) model from [Pix2Text (P2T)](https://github.com/breezedeus/Pix2Text).\n\n## Model Details / æ¨¡åž‹ç»†èŠ‚\n\nThis MFR model utilizes the [TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr) architecture developed by Microsoft, starting with its initial values and retrained using a dataset of mathematical formula images. \nThe resulting MFR model can be used to convert images of mathematical formulas into LaTeX text representation. More detailed can be found: [Pix2Text V1.0 New Release: The Best Open-Source Formula Recognition Model | Breezedeus.com](https://www.breezedeus.com/article/p2t-v1.0).\n\n\næ­¤ MFR æ¨¡åž‹ä½¿ç”¨äº†å¾®è½¯çš„ [TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr) æž¶æž„ï¼Œä»¥å…¶ä¸ºåˆå§‹å€¼å¹¶åˆ©ç”¨æ•°å­¦å…¬å¼å›¾ç‰‡æ•°æ®é›†è¿›è¡Œäº†é‡æ–°è®­ç»ƒã€‚\nèŽ·å¾—çš„ MFR æ¨¡åž‹å¯ç”¨äºŽæŠŠæ•°å­¦å…¬å¼å›¾ç‰‡è½¬æ¢ä¸º LaTeX æ–‡æœ¬è¡¨ç¤ºã€‚æ›´å¤šç»†èŠ‚è¯·è§ï¼š[Pix2Text V1.0 æ–°ç‰ˆå‘å¸ƒï¼šæœ€å¥½çš„å¼€æºå…¬å¼è¯†åˆ«æ¨¡åž‹ | Breezedeus.com](https://www.breezedeus.com/article/p2t-v1.0)ã€‚\n\n\n\n## Usage and Limitations / ä½¿ç”¨å’Œé™åˆ¶\n\n- **Purpose**: This model is a mathematical formula recognition model, capable of converting input images of mathematical formulas into LaTeX text representation.\n- **Limitation**: Since the model is trained on images of mathematical formulas, it may not work when recognizing other types of images.\n\n\n- **ç”¨é€”**ï¼šæ­¤æ¨¡åž‹ä¸ºæ•°å­¦å…¬å¼è¯†åˆ«æ¨¡åž‹ï¼Œå®ƒå¯ä»¥æŠŠè¾“å…¥çš„æ•°å­¦å…¬å¼å›¾ç‰‡è½¬æ¢ä¸º LaTeX æ–‡æœ¬è¡¨ç¤ºã€‚\n- **é™åˆ¶**ï¼šç”±äºŽæ¨¡åž‹æ˜¯åœ¨æ•°å­¦å…¬å¼å›¾ç‰‡æ•°æ®ä¸Šè®­ç»ƒçš„ï¼Œå®ƒåœ¨è¯†åˆ«å…¶ä»–ç±»åž‹çš„å›¾ç‰‡æ—¶å¯èƒ½æ— æ³•å·¥ä½œã€‚\n\n\n\n## Documents / æ–‡æ¡£\n\n- [Pix2Text V1.0 New Release: The Best Open-Source Formula Recognition Model | Breezedeus.com](https://www.breezedeus.com/article/p2t-v1.0) ;\n- Pix2Text (P2T) Github: [breezedeus/pix2text](https://github.com/breezedeus/Pix2Text) ;\n- Pix2Text Online Free Service: [p2t.breezedeus.com](https://p2t.breezedeus.com/) ;\n- Pix2Text Online Docs: [Docs](https://pix2text.readthedocs.io) ;\n- Pix2Text More: [breezedeus.com/pix2text](https://breezedeus.com/article/pix2text) ;\n- Pix2Text Discard: https://discord.gg/GgD87WM8Tf\n\n\n## Examples / ç¤ºä¾‹\n\n### Printed Math Formula Images / å°åˆ·ä½“å…¬å¼å›¾ç‰‡\n\n![printed-formula examples](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F9341931a-53f0-48e1-b026-0f1ad17b457c%2F26046b54-ae87-4faa-ab18-9acda74fd920%2FUntitled.jpeg?table=block&id=f422e590-4465-4648-8edd-ce2e1b00d959)\n\n\n### Handwritten Math Formula Images / å°åˆ·ä½“å…¬å¼å›¾ç‰‡\n\n![handwritten-formula examples](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F9341931a-53f0-48e1-b026-0f1ad17b457c%2Fcdbebff6-5b01-4e2a-a6f0-250da5cc39fe%2FUntitled.jpeg?table=block&id=e1029b05-25c5-40f0-9c3e-505744c0afa5)\n\n\n## Model Use / æ¨¡åž‹ä½¿ç”¨\n\n### Method 1: Using the model Directly\n\nThis method doesn't need to install pix2text, but can only recognize pure formula images.\n\nè¿™ç§æ–¹æ³•æ— éœ€å®‰è£… pix2textï¼Œä½†åªèƒ½è¯†åˆ«çº¯å…¬å¼å›¾ç‰‡ã€‚\n\n```python\n#! pip install transformers>=4.37.0 pillow optimum[onnxruntime]\nfrom PIL import Image\nfrom transformers import TrOCRProcessor\nfrom optimum.onnxruntime import ORTModelForVision2Seq\n\nprocessor = TrOCRProcessor.from_pretrained('breezedeus/pix2text-mfr')\nmodel = ORTModelForVision2Seq.from_pretrained('breezedeus/pix2text-mfr', use_cache=False)\n\nimage_fps = [\n    'examples/example.jpg',\n    'examples/42.png',\n    'examples/0000186.png',\n]\nimages = [Image.open(fp).convert('RGB') for fp in image_fps]\npixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\nprint(f'generated_ids: {generated_ids}, \\ngenerated text: {generated_text}')\n\n```\n\n### Method 2: Using Pix2Text\n\nThis method requires the installation of pix2text, utilizing the Mathematical Formula Detection model (MFD) within Pix2Text. It is capable of recognizing not only pure formula images but also mixed images containing text.\n\nè¿™ç§æ–¹æ³•éœ€è¦å®‰è£… pix2textï¼Œå€ŸåŠ© Pix2Text ä¸­çš„æ•°å­¦å…¬å¼æ£€æµ‹æ¨¡åž‹ï¼ˆMFDï¼‰ï¼Œå®ƒä¸ä»…å¯ä»¥è¯†åˆ«çº¯å…¬å¼å›¾ç‰‡ï¼Œè¿˜å¯ä»¥è¯†åˆ«åŒ…å«æ–‡æœ¬çš„æ··åˆå›¾ç‰‡ã€‚\n\n```bash\n$ pip install pix2text>=1.1\n```\n\n```python\n#! pip install pix2text>=1.1\n\nfrom pix2text import Pix2Text, merge_line_texts\n\nimage_fps = [\n    'examples/example.jpg',\n    'examples/42.png',\n    'examples/0000186.png',\n]\np2t = Pix2Text.from_config()\nouts = p2t.recognize_formula(image_fps)  # recognize pure formula images\n\nouts2 = p2t.recognize('examples/mixed.jpg', file_type='text_formula', return_text=True, save_analysis_res='mixed-out.jpg')  # recognize mixed images\nprint(outs2)\n```\n\n### Method 3: Notebook\n\nJust try Pix2Text with this notebook: [https://github.com/breezedeus/Pix2Text/blob/main/pix2text_v1_1.ipynb](https://github.com/breezedeus/Pix2Text/blob/main/pix2text_v1_1.ipynb).\n\n\n## Performance / æ€§èƒ½\n\nThe original images for the test data are derived from real data uploaded by users on the [Pix2Text Online Service](https://p2t.breezedeus.com). Initially, real user data from a specific period is selected, and then the Mathematical Formula Detection model (MFD) within Pix2Text is used to detect the mathematical formulas in these images and crop the corresponding parts. A subset of these formula images is then randomly chosen for manual annotation to create the test dataset. The following image shows some sample pictures from the test dataset. It is evident that the images in the test dataset are quite diverse, including mathematical formulas of various lengths and complexities, from single letters to formula groups and even matrices. This test dataset includes `485` images.\n\næµ‹è¯•æ•°æ®å¯¹åº”çš„åŽŸå§‹å›¾ç‰‡æ¥æºäºŽ [Pix2Text ç½‘é¡µç‰ˆ](https://p2t.breezedeus.com) ç”¨æˆ·ä¸Šä¼ çš„çœŸå®žæ•°æ®ã€‚é¦–å…ˆé€‰å–ä¸€æ®µæ—¶é—´å†…ç”¨æˆ·çš„çœŸå®žæ•°æ®ï¼Œç„¶åŽåˆ©ç”¨ Pix2Text ä¸­æ•°å­¦å…¬å¼æ£€æµ‹æ¨¡åž‹ï¼ˆMFDï¼‰æ£€æµ‹å‡ºè¿™äº›å›¾ç‰‡ä¸­çš„æ•°å­¦å…¬å¼å¹¶æˆªå–å‡ºå¯¹åº”çš„éƒ¨åˆ†ï¼Œå†ä»Žä¸­éšæœºé€‰å–éƒ¨åˆ†å…¬å¼å›¾ç‰‡è¿›è¡Œäººå·¥æ ‡æ³¨ã€‚å°±èŽ·å¾—äº†ç”¨äºŽæµ‹è¯•çš„æµ‹è¯•æ•°æ®é›†äº†ã€‚ä¸‹å›¾æ˜¯æµ‹è¯•æ•°æ®é›†ä¸­çš„éƒ¨åˆ†æ ·ä¾‹å›¾ç‰‡ã€‚ä»Žä¸­å¯ä»¥çœ‹å‡ºæµ‹è¯•æ•°æ®é›†ä¸­çš„å›¾ç‰‡æ¯”è¾ƒå¤šæ ·ï¼ŒåŒ…æ‹¬äº†å„ç§ä¸åŒé•¿åº¦å’Œå¤æ‚åº¦çš„æ•°å­¦å…¬å¼ï¼Œæœ‰å•ä¸ªå­—æ¯çš„å›¾ç‰‡ï¼Œä¹Ÿæœ‰å…¬å¼ç»„ç”šè‡³çŸ©é˜µå›¾ç‰‡ã€‚æœ¬æµ‹è¯•æ•°æ®é›†åŒ…æ‹¬äº† `485` å¼ å›¾ç‰‡ã€‚\n\n![Examples from test data](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F9341931a-53f0-48e1-b026-0f1ad17b457c%2Ffb23b2d4-cdcf-46c9-9095-027591402a54%2FUntitled.png?table=block&id=269900d5-299a-4dcd-a26c-6555e831caff)\n\nBelow are the Character Error Rates (CER, the lower, the better) of various models on this test dataset. For the true annotated results, as well as the output of each model, normalization was first performed to ensure that irrelevant factors such as spaces do not affect the test outcomes. For the recognition results of Texify, the leading and trailing symbols `$` or `$$` of the formula are removed first.\n\nä»¥ä¸‹æ˜¯å„ä¸ªæ¨¡åž‹åœ¨æ­¤æµ‹è¯•æ•°æ®é›†ä¸Šçš„ CERï¼ˆå­—é”™è¯¯çŽ‡ï¼Œè¶Šå°è¶Šå¥½ï¼‰ã€‚å…¶ä¸­å¯¹çœŸå®žæ ‡æ³¨ç»“æžœï¼Œä»¥åŠæ¯ä¸ªæ¨¡åž‹çš„è¾“å‡ºéƒ½é¦–å…ˆè¿›è¡Œäº†æ ‡å‡†åŒ–ï¼Œä»¥ä¿è¯ä¸ä¼šå› ä¸ºç©ºæ ¼ç­‰æ— å…³å› ç´ å½±å“æµ‹è¯•ç»“æžœã€‚å¯¹ Texify çš„è¯†åˆ«ç»“æžœä¼šé¦–å…ˆåŽ»æŽ‰å…¬å¼çš„é¦–å°¾ç¬¦å·$æˆ–$$ã€‚\n\n![CER Comparison Among Different MFR Models](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F9341931a-53f0-48e1-b026-0f1ad17b457c%2F976b6c14-879d-4a3b-b027-6d2b15ce28b3%2FUntitled.png?table=block&id=6c503402-9b34-4937-a103-e4fd3bdbe754)\n\nAs can be seen from the figure above, the Pix2Text V1.0 MFR open-source free version model has significantly outperformed the previous versions of the paid model. Moreover, compared to the V1.0 MFR open-source free model, the precision of the Pix2Text V1.0 MFR paid model has been further improved.\n\nç”±ä¸Šå›¾å¯è§ï¼ŒPix2Text V1.0 MFR å¼€æºå…è´¹ç‰ˆæ¨¡åž‹å·²ç»å¤§å¤§ä¼˜äºŽä¹‹å‰ç‰ˆæœ¬çš„ä»˜è´¹æ¨¡åž‹ã€‚è€Œç›¸æ¯” V1.0 MFR å¼€æºå…è´¹æ¨¡åž‹ï¼ŒPix2Text V1.0 MFR ä»˜è´¹æ¨¡åž‹ç²¾åº¦å¾—åˆ°äº†è¿›ä¸€æ­¥çš„æå‡ã€‚\n\n> [Texify](https://github.com/VikParuchuri/texify) is more suited for recognizing images with standard formatting. It performs poorly in recognizing images containing single letters. This is the main reason why Texify's performance on this test dataset is inferior to that of Latex-OCR.\n> \n> [Texify](https://github.com/VikParuchuri/texify) æ›´é€‚ç”¨äºŽè¯†åˆ«æ ‡å‡†æŽ’ç‰ˆçš„å›¾ç‰‡ï¼Œå®ƒå¯¹åŒ…å«å•å­—æ¯çš„å›¾ç‰‡è¯†åˆ«è¾ƒå·®ã€‚è¿™ä¹Ÿæ˜¯ Texify åœ¨æ­¤æµ‹è¯•æ•°æ®é›†ä¸Šæ•ˆæžœæ¯” Latex-OCR è¿˜å·®çš„ä¸»è¦åŽŸå› ã€‚\n\n\n## Feedback / åé¦ˆ\n\n> Where to send questions or comments about the model.\n\nWelcome to contact the author [Breezedeus](https://www.breezedeus.com/article/join-group).\n\næ¬¢è¿Žè”ç³»ä½œè€…  [Breezedeus](https://www.breezedeus.com/article/join-group) ã€‚\n", "downloads": 119655, "likes": 39, "meta": {"library_name": "transformers", "license": "mit", "tags": ["latex-ocr", "math-ocr", "math-formula-recognition", "mfr", "pix2text", "p2t", "image-to-text"]}, "inference_type": "local"}
{"id": "to-be/donut-base-finetuned-invoices", "pipeline_tag": "image-to-text", "tags": ["transformers", "pytorch", "vision-encoder-decoder", "image-text-to-text", "donut", "image-to-text", "vision", "invoices", "arxiv:2111.15664", "license:cc-by-nc-sa-4.0", "endpoints_compatible", "region:us"], "description": "---\nlicense: cc-by-nc-sa-4.0\ntags:\n- donut\n- image-to-text\n- vision\n- invoices\n---\n\n# Donut finetuned on invoices \n\nBased on Donut base model (introduced in the paper [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by Geewok et al. and first released in [this repository](https://github.com/clovaai/donut).\n\nThe model was trained with a few thousand of annotated invoices and non-invoices (for those the doctype will be 'Other'). They span across different countries and languages. They are always one page only. The dataset is proprietary unfortunately. Model is set to input resolution of 1280x1920 pixels. So any sample you want to try with higher dpi than 150 has no added value.\nIt was trained for about 4 hours on a NVIDIA RTX A4000 for 20k steps with a val_metric of 0.03413819904382196 at the end.\nThe following indexes were included in the train set:\n\nDocType\nCurrency\nDocumentDate\nGrossAmount\nInvoiceNumber\nNetAmount\nTaxAmount\nOrderNumber\nCreditorCountry\n\n[Demo space can be found here](https://huggingface.co/spaces/to-be/invoice_document_headers_extraction_with_donut)\n\n## Model description\n\nDonut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. \n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/donut_architecture.jpg)\n\n## Intended uses & limitations\n\nThis model is meant as a research in how well it fares with multilanguage invoices.\nSee my observations in the [demo space](https://huggingface.co/spaces/to-be/invoice_document_headers_extraction_with_donut).\n\n### How to use\n\nLook at the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/donut) which includes code examples.\n\n\n", "downloads": 1522, "likes": 22, "meta": {"license": "cc-by-nc-sa-4.0", "tags": ["donut", "image-to-text", "vision", "invoices"]}, "inference_type": "local"}
{"id": "AWeirdDev/human-disease-prediction", "pipeline_tag": "tabular-classification", "tags": ["sklearn", "joblib", "tabular-classification", "license:mit", "model-index", "region:us"], "description": "---\nlicense: mit\nlibrary_name: sklearn\npipeline_tag: tabular-classification\nmodel-index:\n- name: human-disease-prediction\n  results:\n  - task:\n      type: tabular-classification\n    dataset:\n      name: human-disease-prediction\n      type: kaggle\n    metrics:\n    - type: Score\n      value: 1\n      name: Score\n---\n\n# human-disease-prediction\n\nTry it on [ðŸ¤— Spaces](https://huggingface.co/spaces/AWeirdDev/human-disease-prediction)\n\n```python\nimport joblib\nimport numpy\nfrom huggingface_hub import hf_hub_download\n\nmodel = joblib.load(\n\thf_hub_download(\"AWeirdDev/human-disease-prediction\", \"sklearn_model.joblib\")\n)\n\nmodel.predict(\n  numpy.array([your_data])\n)\n```\n\nThe `your_data` variable should be a vector of zeros and ones.\n\nA zero means \"False,\" and a one means \"True.\"\n\nCreate a vector that pairs with the following symptoms, then the model will predict what disease it might be.\n\n```python\n['itching',\n 'skin_rash',\n 'nodal_skin_eruptions',\n 'continuous_sneezing',\n 'shivering',\n 'chills',\n 'joint_pain',\n 'stomach_pain',\n 'acidity',\n 'ulcers_on_tongue',\n 'muscle_wasting',\n 'vomiting',\n 'burning_micturition',\n 'spotting_ urination',\n 'fatigue',\n 'weight_gain',\n 'anxiety',\n 'cold_hands_and_feets',\n 'mood_swings',\n 'weight_loss',\n 'restlessness',\n 'lethargy',\n 'patches_in_throat',\n 'irregular_sugar_level',\n 'cough',\n 'high_fever',\n 'sunken_eyes',\n 'breathlessness',\n 'sweating',\n 'dehydration',\n 'indigestion',\n 'headache',\n 'yellowish_skin',\n 'dark_urine',\n 'nausea',\n 'loss_of_appetite',\n 'pain_behind_the_eyes',\n 'back_pain',\n 'constipation',\n 'abdominal_pain',\n 'diarrhoea',\n 'mild_fever',\n 'yellow_urine',\n 'yellowing_of_eyes',\n 'acute_liver_failure',\n 'fluid_overload',\n 'swelling_of_stomach',\n 'swelled_lymph_nodes',\n 'malaise',\n 'blurred_and_distorted_vision',\n 'phlegm',\n 'throat_irritation',\n 'redness_of_eyes',\n 'sinus_pressure',\n 'runny_nose',\n 'congestion',\n 'chest_pain',\n 'weakness_in_limbs',\n 'fast_heart_rate',\n 'pain_during_bowel_movements',\n 'pain_in_anal_region',\n 'bloody_stool',\n 'irritation_in_anus',\n 'neck_pain',\n 'dizziness',\n 'cramps',\n 'bruising',\n 'obesity',\n 'swollen_legs',\n 'swollen_blood_vessels',\n 'puffy_face_and_eyes',\n 'enlarged_thyroid',\n 'brittle_nails',\n 'swollen_extremeties',\n 'excessive_hunger',\n 'extra_marital_contacts',\n 'drying_and_tingling_lips',\n 'slurred_speech',\n 'knee_pain',\n 'hip_joint_pain',\n 'muscle_weakness',\n 'stiff_neck',\n 'swelling_joints',\n 'movement_stiffness',\n 'spinning_movements',\n 'loss_of_balance',\n 'unsteadiness',\n 'weakness_of_one_body_side',\n 'loss_of_smell',\n 'bladder_discomfort',\n 'foul_smell_of urine',\n 'continuous_feel_of_urine',\n 'passage_of_gases',\n 'internal_itching',\n 'toxic_look_(typhos)',\n 'depression',\n 'irritability',\n 'muscle_pain',\n 'altered_sensorium',\n 'red_spots_over_body',\n 'belly_pain',\n 'abnormal_menstruation',\n 'dischromic _patches',\n 'watering_from_eyes',\n 'increased_appetite',\n 'polyuria',\n 'family_history',\n 'mucoid_sputum',\n 'rusty_sputum',\n 'lack_of_concentration',\n 'visual_disturbances',\n 'receiving_blood_transfusion',\n 'receiving_unsterile_injections',\n 'coma',\n 'stomach_bleeding',\n 'distention_of_abdomen',\n 'history_of_alcohol_consumption',\n 'fluid_overload.1',\n 'blood_in_sputum',\n 'prominent_veins_on_calf',\n 'palpitations',\n 'painful_walking',\n 'pus_filled_pimples',\n 'blackheads',\n 'scurring',\n 'skin_peeling',\n 'silver_like_dusting',\n 'small_dents_in_nails',\n 'inflammatory_nails',\n 'blister',\n 'red_sore_around_nose',\n 'yellow_crust_ooze']\n```\n\n## Accuracy\n\nIt has been reported as `1.0` (100%), but I don't believe it.", "downloads": 49, "likes": 2, "meta": {"library_name": "sklearn", "license": "mit", "pipeline_tag": "tabular-classification", "model-index": [{"name": "human-disease-prediction", "results": [{"task": {"type": "tabular-classification"}, "dataset": {"name": "human-disease-prediction", "type": "kaggle"}, "metrics": [{"type": "Score", "value": 1, "name": "Score", "verified": false}]}]}]}, "inference_type": "local"}
{"id": "keras-io/imbalanced_classification", "pipeline_tag": "tabular-classification", "tags": ["tf-keras", "tensorboard", "tabular-classification", "imbalanced-classification", "region:us"], "description": "---\nlibrary_name: tf-keras\ntags:\n- tabular-classification\n- imbalanced-classification\n---\n\n## Model Description\n### Keras Implementation of Imbalanced classification: credit card fraud detection\nThis repo contains the trained model of [Imbalanced classification: credit card fraud detection](https://keras.io/examples/structured_data/imbalanced_classification/).\nThe full credit goes to: [fchollet](https://twitter.com/fchollet)\n\n## Intended uses & limitations\n- The trained model is used to detect of a specific transaction is fraudulent or not.\n\n## Training dataset\n- [Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)\n- Due to the high imbalance of the target feature (417 frauds or 0.18% of total 284,807 samples), training weight was applied to reduce the False Negatives to the lowest level as possible.\n\n## Training procedure\n### Training hyperparameter \nThe following hyperparameters were used during training:\n- optimizer: 'Adam'\n- learning_rate: 0.01\n- loss: 'binary_crossentropy'\n- epochs: 30\n- batch_size: 2048\n- beta_1: 0.9\n- beta_2: 0.999\n- epsilon: 1e-07\n- training_precision: float32\n\n ## Training Metrics\n\n| Epochs | Train Loss | Train Fn | Train Fp | Train Tn | Train Tp | Train Precision | Train Recall | Validation Loss | Validation Fn | Validation Fp | Validation Tn | Validation Tp | Validation Precision | Validation Recall |\n |--- |--- |--- |--- |--- |--- |--- |--- |--- |--- |--- |--- |--- |--- |--- |\n| 1| 0.0|  14.0|  6202.0|  221227.0|  403.0|  0.061|  0.966|  0.043|  9.0|  622.0|  56264.0|  66.0|  0.096|  0.88| \n| 2| 0.0|  3.0|  3514.0|  223915.0|  414.0|  0.105|  0.993|  0.025|  10.0|  528.0|  56358.0|  65.0|  0.11|  0.867| \n| 3| 0.0|  2.0|  2419.0|  225010.0|  415.0|  0.146|  0.995|  0.014|  11.0|  283.0|  56603.0|  64.0|  0.184|  0.853| \n| 4| 0.0|  3.0|  2482.0|  224947.0|  414.0|  0.143|  0.993|  0.027|  11.0|  340.0|  56546.0|  64.0|  0.158|  0.853| \n| 5| 0.0|  2.0|  2295.0|  225134.0|  415.0|  0.153|  0.995|  0.034|  11.0|  245.0|  56641.0|  64.0|  0.207|  0.853| \n| 6| 0.0|  3.0|  2239.0|  225190.0|  414.0|  0.156|  0.993|  0.037|  10.0|  495.0|  56391.0|  65.0|  0.116|  0.867| \n| 7| 0.0|  2.0|  3095.0|  224334.0|  415.0|  0.118|  0.995|  0.011|  11.0|  194.0|  56692.0|  64.0|  0.248|  0.853| \n| 8| 0.0|  4.0|  1844.0|  225585.0|  413.0|  0.183|  0.99|  0.035|  9.0|  429.0|  56457.0|  66.0|  0.133|  0.88| \n| 9| 0.0|  1.0|  2119.0|  225310.0|  416.0|  0.164|  0.998|  0.012|  11.0|  167.0|  56719.0|  64.0|  0.277|  0.853| \n| 10| 0.0|  3.0|  1539.0|  225890.0|  414.0|  0.212|  0.993|  0.013|  13.0|  144.0|  56742.0|  62.0|  0.301|  0.827| \n| 11| 0.0|  6.0|  3444.0|  223985.0|  411.0|  0.107|  0.986|  0.039|  11.0|  394.0|  56492.0|  64.0|  0.14|  0.853| \n| 12| 0.0|  4.0|  3818.0|  223611.0|  413.0|  0.098|  0.99|  0.03|  9.0|  523.0|  56363.0|  66.0|  0.112|  0.88| \n| 13| 0.0|  7.0|  4482.0|  222947.0|  410.0|  0.084|  0.983|  0.059|  6.0|  1364.0|  55522.0|  69.0|  0.048|  0.92| \n| 14| 0.0|  2.0|  3064.0|  224365.0|  415.0|  0.119|  0.995|  0.033|  9.0|  699.0|  56187.0|  66.0|  0.086|  0.88| \n| 15| 0.0|  4.0|  3563.0|  223866.0|  413.0|  0.104|  0.99|  0.066|  8.0|  956.0|  55930.0|  67.0|  0.065|  0.893| \n| 16| 0.0|  4.0|  2536.0|  224893.0|  413.0|  0.14|  0.99|  0.016|  9.0|  339.0|  56547.0|  66.0|  0.163|  0.88| \n| 17| 0.0|  6.0|  2594.0|  224835.0|  411.0|  0.137|  0.986|  0.049|  8.0|  821.0|  56065.0|  67.0|  0.075|  0.893| \n| 18| 0.0|  1.0|  1911.0|  225518.0|  416.0|  0.179|  0.998|  0.013|  8.0|  215.0|  56671.0|  67.0|  0.238|  0.893| \n| 19| 0.0|  2.0|  1457.0|  225972.0|  415.0|  0.222|  0.995|  0.018|  7.0|  342.0|  56544.0|  68.0|  0.166|  0.907| \n| 20| 0.0|  0.0|  1132.0|  226297.0|  417.0|  0.269|  1.0|  0.011|  10.0|  172.0|  56714.0|  65.0|  0.274|  0.867| \n| 21| 0.0|  1.0|  840.0|  226589.0|  416.0|  0.331|  0.998|  0.008|  11.0|  100.0|  56786.0|  64.0|  0.39|  0.853| \n| 22| 0.0|  1.0|  2124.0|  225305.0|  416.0|  0.164|  0.998|  0.075|  10.0|  350.0|  56536.0|  65.0|  0.157|  0.867| \n| 23| 0.0|  2.0|  1457.0|  225972.0|  415.0|  0.222|  0.995|  0.03|  11.0|  242.0|  56644.0|  64.0|  0.209|  0.853| \n| 24| 0.0|  5.0|  2761.0|  224668.0|  412.0|  0.13|  0.988|  0.297|  6.0|  2741.0|  54145.0|  69.0|  0.025|  0.92| \n| 25| 0.0|  3.0|  2484.0|  224945.0|  414.0|  0.143|  0.993|  0.025|  10.0|  199.0|  56687.0|  65.0|  0.246|  0.867| \n| 26| 0.0|  4.0|  4867.0|  222562.0|  413.0|  0.078|  0.99|  0.021|  18.0|  33.0|  56853.0|  57.0|  0.633|  0.76| \n| 27| 0.0|  8.0|  4230.0|  223199.0|  409.0|  0.088|  0.981|  0.053|  9.0|  1541.0|  55345.0|  66.0|  0.041|  0.88| \n| 28| 0.0|  9.0|  5305.0|  222124.0|  408.0|  0.071|  0.978|  0.026|  9.0|  398.0|  56488.0|  66.0|  0.142|  0.88| \n| 29| 0.0|  5.0|  4846.0|  222583.0|  412.0|  0.078|  0.988|  0.242|  6.0|  7883.0|  49003.0|  69.0|  0.009|  0.92| \n| 30| 0.0|  5.0|  5193.0|  222236.0|  412.0|  0.074|  0.988|  0.026|  7.0|  449.0|  56437.0|  68.0|  0.132|  0.907| \n\n ## Model Plot\n\n<details>\n<summary>View Model Plot</summary>\n\n![Model Image](./model.png)\n\n</details>", "downloads": 41, "likes": 9, "meta": {"library_name": "tf-keras", "tags": ["tabular-classification", "imbalanced-classification"]}, "inference_type": "local"}
{"id": "sadhaklal/mlp-california-housing", "pipeline_tag": "tabular-regression", "tags": ["pytorch", "safetensors", "model_hub_mixin", "pytorch_model_hub_mixin", "tabular-regression", "dataset:gvlassis/california_housing", "region:us"], "description": "---\ntags:\n- model_hub_mixin\n- pytorch_model_hub_mixin\nmetrics:\n- rmse\nlibrary_name: pytorch\ndatasets:\n- gvlassis/california_housing\npipeline_tag: tabular-regression\n---\n\n# mlp-california-housing\n\nA multi-layer perceptron (MLP) trained on the California Housing dataset.\n\nIt takes eight inputs: `'MedInc'`, `'HouseAge'`, `'AveRooms'`, `'AveBedrms'`, `'Population'`, `'AveOccup'`, `'Latitude'` and `'Longitude'`. It predicts `'MedHouseVal'`.\n\nIt is a PyTorch adaptation of the TensorFlow model in Chapter 10 of Aurelien Geron's book 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow'.\n\nCode: https://github.com/sambitmukherjee/handson-ml3-pytorch/blob/main/chapter10/mlp_california_housing.ipynb\n\nExperiment tracking: https://wandb.ai/sadhaklal/mlp-california-housing\n\n## Usage\n\n```\nfrom sklearn.datasets import fetch_california_housing\n\nhousing = fetch_california_housing(as_frame=True)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_full, X_test, y_train_full, y_test = train_test_split(housing['data'], housing['target'], test_size=0.25, random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n\nX_means, X_stds = X_train.mean(axis=0), X_train.std(axis=0)\nX_train = (X_train - X_means) / X_stds\nX_valid = (X_valid - X_means) / X_stds\nX_test = (X_test - X_means) / X_stds\n\nimport torch\n\ndevice = torch.device(\"cpu\")\n\nimport torch.nn as nn\nfrom huggingface_hub import PyTorchModelHubMixin\n\nclass MLP(nn.Module, PyTorchModelHubMixin):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(8, 50)\n        self.fc2 = nn.Linear(50, 50)\n        self.fc3 = nn.Linear(50, 50)\n        self.fc4 = nn.Linear(50, 1)\n\n    def forward(self, x):\n        act = torch.relu(self.fc1(x))\n        act = torch.relu(self.fc2(act))\n        act = torch.relu(self.fc3(act))\n        return self.fc4(act)\n\nmodel = MLP.from_pretrained(\"sadhaklal/mlp-california-housing\")\nmodel.to(device)\nmodel.eval()\n\n# Let's predict on 3 unseen examples from the test set:\nprint(f\"Ground truth housing prices: {y_test.values[:3]}\")\nx_new = torch.tensor(X_test.values[:3], dtype=torch.float32)\nx_new = x_new.to(device)\nwith torch.no_grad():\n    preds = model(x_new)\nprint(f\"Predicted housing prices: {preds.squeeze()}\")\n```\n\n## Metric\n\nRMSE on the test set: 0.5502\n\n---\n\nThis model has been pushed to the Hub using the [PyTorchModelHubMixin](https://huggingface.co/docs/huggingface_hub/package_reference/mixins#huggingface_hub.PyTorchModelHubMixin) integration.", "downloads": 0, "likes": 0, "meta": {"datasets": ["gvlassis/california_housing"], "library_name": "pytorch", "metrics": ["rmse"], "pipeline_tag": "tabular-regression", "tags": ["model_hub_mixin", "pytorch_model_hub_mixin"]}, "inference_type": "local"}
{"id": "quantile-forest/california-housing-example", "pipeline_tag": "tabular-regression", "tags": ["quantile-forest", "sklearn", "skops", "tabular-regression", "quantile-regression", "uncertainty-estimation", "prediction-intervals", "license:apache-2.0", "region:us"], "description": "---\nlibrary_name: quantile-forest\nlicense: apache-2.0\ntags:\n- quantile-forest\n- sklearn\n- skops\n- tabular-regression\n- quantile-regression\n- uncertainty-estimation\n- prediction-intervals\nmodel_format: pickle\nmodel_file: model.pkl\nwidget:\n- structuredData:\n    AveBedrms:\n    - 1.0238095238095237\n    - 0.9718804920913884\n    - 1.073446327683616\n    AveOccup:\n    - 2.5555555555555554\n    - 2.109841827768014\n    - 2.8022598870056497\n    AveRooms:\n    - 6.984126984126984\n    - 6.238137082601054\n    - 8.288135593220339\n    HouseAge:\n    - 41.0\n    - 21.0\n    - 52.0\n    Latitude:\n    - 37.88\n    - 37.86\n    - 37.85\n    Longitude:\n    - -122.23\n    - -122.22\n    - -122.24\n    MedInc:\n    - 8.3252\n    - 8.3014\n    - 7.2574\n    Population:\n    - 322.0\n    - 2401.0\n    - 496.0\n---\n\n# Model description\n\nThis is a RandomForestQuantileRegressor trained on the California Housing dataset.\n\n## Intended uses & limitations\n\nThis model is not ready to be used in production.\n\n## Training Procedure\n\nThe model was trained using default parameters on a 5-fold cross-validation pipeline.\n\n### Hyperparameters\n\n<details>\n<summary> Click to expand </summary>\n\n| Hyperparameter           | Value                |\n|--------------------------|----------------------|\n| bootstrap                | True                 |\n| ccp_alpha                | 0.0                  |\n| criterion                | squared_error        |\n| default_quantiles        | 0.5                  |\n| max_depth                |                      |\n| max_features             | 1.0                  |\n| max_leaf_nodes           |                      |\n| max_samples              |                      |\n| max_samples_leaf         | 1                    |\n| min_impurity_decrease    | 0.0                  |\n| min_samples_leaf         | 1                    |\n| min_samples_split        | 2                    |\n| min_weight_fraction_leaf | 0.0                  |\n| monotonic_cst            |                      |\n| n_estimators             | 100                  |\n| n_jobs                   |                      |\n| oob_score                | False                |\n| random_state             | RandomState(MT19937) |\n| verbose                  | 0                    |\n| warm_start               | False                |\n\n</details>\n\n### Model Plot\n\n<style>#sk-container-id-1 {/* Definition of color scheme common for light and dark mode */--sklearn-color-text: #000;--sklearn-color-text-muted: #666;--sklearn-color-line: gray;/* Definition of color scheme for unfitted estimators */--sklearn-color-unfitted-level-0: #fff5e6;--sklearn-color-unfitted-level-1: #f6e4d2;--sklearn-color-unfitted-level-2: #ffe0b3;--sklearn-color-unfitted-level-3: chocolate;/* Definition of color scheme for fitted estimators */--sklearn-color-fitted-level-0: #f0f8ff;--sklearn-color-fitted-level-1: #d4ebff;--sklearn-color-fitted-level-2: #b3dbfd;--sklearn-color-fitted-level-3: cornflowerblue;/* Specific color for light theme */--sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));--sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));--sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));--sklearn-color-icon: #696969;@media (prefers-color-scheme: dark) {/* Redefinition of color scheme for dark theme */--sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));--sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));--sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));--sklearn-color-icon: #878787;}\n}#sk-container-id-1 {color: var(--sklearn-color-text);\n}#sk-container-id-1 pre {padding: 0;\n}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;\n}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed var(--sklearn-color-line);margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: var(--sklearn-color-background);\n}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }`but bootstrap.min.css set `[hidden] { display: none !important; }`so we also need the `!important` here to be able to override thedefault hidden behavior on the sphinx rendered scikit-learn.org.See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;\n}#sk-container-id-1 div.sk-text-repr-fallback {display: none;\n}div.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {/* draw centered vertical line to link estimators */background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));background-size: 2px 100%;background-repeat: no-repeat;background-position: center center;\n}/* Parallel-specific style estimator block */#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 2px solid var(--sklearn-color-text-on-default-background);flex-grow: 1;\n}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: var(--sklearn-color-background);position: relative;\n}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;\n}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;\n}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;\n}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;\n}/* Serial-specific style estimator block */#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: var(--sklearn-color-background);padding-right: 1em;padding-left: 1em;\n}/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*//* Pipeline and ColumnTransformer style (default) */#sk-container-id-1 div.sk-toggleable {/* Default theme specific background. It is overwritten whether we have aspecific estimator or a Pipeline/ColumnTransformer */background-color: var(--sklearn-color-background);\n}/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: flex;width: 100%;margin-bottom: 0;padding: 0.5em;box-sizing: border-box;text-align: center;align-items: start;justify-content: space-between;gap: 0.5em;\n}#sk-container-id-1 label.sk-toggleable__label .caption {font-size: 0.6rem;font-weight: lighter;color: var(--sklearn-color-text-muted);\n}#sk-container-id-1 label.sk-toggleable__label-arrow:before {/* Arrow on the left of the label */content: \"â–¸\";float: left;margin-right: 0.25em;color: var(--sklearn-color-icon);\n}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: var(--sklearn-color-text);\n}/* Toggleable content - dropdown */#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;/* unfitted */background-color: var(--sklearn-color-unfitted-level-0);\n}#sk-container-id-1 div.sk-toggleable__content.fitted {/* fitted */background-color: var(--sklearn-color-fitted-level-0);\n}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;border-radius: 0.25em;color: var(--sklearn-color-text);/* unfitted */background-color: var(--sklearn-color-unfitted-level-0);\n}#sk-container-id-1 div.sk-toggleable__content.fitted pre {/* unfitted */background-color: var(--sklearn-color-fitted-level-0);\n}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {/* Expand drop-down */max-height: 200px;max-width: 100%;overflow: auto;\n}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";\n}/* Pipeline/ColumnTransformer-specific style */#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {color: var(--sklearn-color-text);background-color: var(--sklearn-color-unfitted-level-2);\n}#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: var(--sklearn-color-fitted-level-2);\n}/* Estimator-specific style *//* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {/* unfitted */background-color: var(--sklearn-color-unfitted-level-2);\n}#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {/* fitted */background-color: var(--sklearn-color-fitted-level-2);\n}#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {/* The background is the default theme color */color: var(--sklearn-color-text-on-default-background);\n}/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {color: var(--sklearn-color-text);background-color: var(--sklearn-color-unfitted-level-2);\n}/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {color: var(--sklearn-color-text);background-color: var(--sklearn-color-fitted-level-2);\n}/* Estimator label */#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;\n}#sk-container-id-1 div.sk-label-container {text-align: center;\n}/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {font-family: monospace;border: 1px dotted var(--sklearn-color-border-box);border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;/* unfitted */background-color: var(--sklearn-color-unfitted-level-0);\n}#sk-container-id-1 div.sk-estimator.fitted {/* fitted */background-color: var(--sklearn-color-fitted-level-0);\n}/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {/* unfitted */background-color: var(--sklearn-color-unfitted-level-2);\n}#sk-container-id-1 div.sk-estimator.fitted:hover {/* fitted */background-color: var(--sklearn-color-fitted-level-2);\n}/* Specification for estimator info (e.g. \"i\" and \"?\") *//* Common style for \"i\" and \"?\" */.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {float: right;font-size: smaller;line-height: 1em;font-family: monospace;background-color: var(--sklearn-color-background);border-radius: 1em;height: 1em;width: 1em;text-decoration: none !important;margin-left: 0.5em;text-align: center;/* unfitted */border: var(--sklearn-color-unfitted-level-1) 1pt solid;color: var(--sklearn-color-unfitted-level-1);\n}.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {/* fitted */border: var(--sklearn-color-fitted-level-1) 1pt solid;color: var(--sklearn-color-fitted-level-1);\n}/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {/* unfitted */background-color: var(--sklearn-color-unfitted-level-3);color: var(--sklearn-color-background);text-decoration: none;\n}div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {/* fitted */background-color: var(--sklearn-color-fitted-level-3);color: var(--sklearn-color-background);text-decoration: none;\n}/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {display: none;z-index: 9999;position: relative;font-weight: normal;right: .2ex;padding: .5ex;margin: .5ex;width: min-content;min-width: 20ex;max-width: 50ex;color: var(--sklearn-color-text);box-shadow: 2pt 2pt 4pt #999;/* unfitted */background: var(--sklearn-color-unfitted-level-0);border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}.sk-estimator-doc-link.fitted span {/* fitted */background: var(--sklearn-color-fitted-level-0);border: var(--sklearn-color-fitted-level-3);\n}.sk-estimator-doc-link:hover span {display: block;\n}/* \"?\"-specific style due to the `<a>` HTML tag */#sk-container-id-1 a.estimator_doc_link {float: right;font-size: 1rem;line-height: 1em;font-family: monospace;background-color: var(--sklearn-color-background);border-radius: 1rem;height: 1rem;width: 1rem;text-decoration: none;/* unfitted */color: var(--sklearn-color-unfitted-level-1);border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}#sk-container-id-1 a.estimator_doc_link.fitted {/* fitted */border: var(--sklearn-color-fitted-level-1) 1pt solid;color: var(--sklearn-color-fitted-level-1);\n}/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {/* unfitted */background-color: var(--sklearn-color-unfitted-level-3);color: var(--sklearn-color-background);text-decoration: none;\n}#sk-container-id-1 a.estimator_doc_link.fitted:hover {/* fitted */background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\" style=\"overflow: auto;\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestQuantileRegressor(random_state=RandomState(MT19937) at 0x10B346040)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>RandomForestQuantileRegressor</div></div><div><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></div></label><div class=\"sk-toggleable__content \"><pre>RandomForestQuantileRegressor(random_state=RandomState(MT19937) at 0x10B346040)</pre></div> </div></div></div></div>\n\n## Evaluation Results\n\n| Metric                         |    Value |\n|--------------------------------|----------|\n| Mean Absolute Percentage Error | 0.164007 |\n| Median Absolute Error          | 0.171    |\n| Mean Squared Error             | 0.25832  |\n| R-Squared                      | 0.806    |\n\n# How to Get Started with the Model\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom examples.plot_qrf_huggingface_inference import CrossValidationPipeline\npipeline = CrossValidationPipeline.load(qrf_pkl_filename)\n```\n\n</details>\n\n# Model Card Authors\n\nThis model card is written by following authors:\n\n[More Information Needed]\n\n# Model Card Contact\n\nYou can contact the model card authors through following channels:\n[More Information Needed]\n\n# Citation\n\nBelow you can find information related to citation.\n\n**BibTeX:**\n```\n[More Information Needed]\n```\n", "downloads": 27, "likes": 0, "meta": {"library_name": "quantile-forest", "license": "apache-2.0", "tags": ["quantile-forest", "sklearn", "skops", "tabular-regression", "quantile-regression", "uncertainty-estimation", "prediction-intervals"], "model_format": "pickle", "model_file": "model.pkl", "widget": [{"structuredData": {"AveBedrms": [1.0238095238095237, 0.9718804920913884, 1.073446327683616], "AveOccup": [2.5555555555555554, 2.109841827768014, 2.8022598870056497], "AveRooms": [6.984126984126984, 6.238137082601054, 8.288135593220339], "HouseAge": [41, 21, 52], "Latitude": [37.88, 37.86, 37.85], "Longitude": [-122.23, -122.22, -122.24], "MedInc": [8.3252, 8.3014, 7.2574], "Population": [322, 2401, 496]}}]}, "inference_type": "local"}
{"id": "deepseek-ai/deepseek-coder-1.3b-base", "pipeline_tag": "text-generation", "tags": ["transformers", "pytorch", "llama", "text-generation", "license:other", "autotrain_compatible", "text-generation-inference", "endpoints_compatible", "region:us"], "description": "---\nlicense: other\nlicense_name: deepseek-license\nlicense_link: LICENSE\n---\n\n\n<p align=\"center\">\n<img width=\"1000px\" alt=\"DeepSeek Coder\" src=\"https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/pictures/logo.png?raw=true\">\n</p>\n<p align=\"center\"><a href=\"https://www.deepseek.com/\">[ðŸ Homepage]</a>  |  <a href=\"https://coder.deepseek.com/\">[ðŸ¤– Chat with DeepSeek Coder]</a>  |  <a href=\"https://discord.gg/Tc7c45Zzu5\">[Discord]</a>  |  <a href=\"https://github.com/guoday/assert/blob/main/QR.png?raw=true\">[Wechat(å¾®ä¿¡)]</a> </p>\n<hr>\n\n\n\n### 1. Introduction of Deepseek Coder\n\nDeepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, to support  project-level code completion and infilling. For coding capabilities, Deepseek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks. \n\n- **Massive Training Data**: Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.\n  \n- **Highly Flexible & Scalable**: Offered in model sizes of 1.3B, 5.7B, 6.7B, and 33B, enabling users to choose the setup most suitable for their requirements.\n  \n- **Superior Model Performance**: State-of-the-art performance among publicly available code models on HumanEval, MultiPL-E, MBPP, DS-1000, and APPS benchmarks.\n  \n- **Advanced Code Completion Capabilities**: A window size of 16K and a fill-in-the-blank task, supporting project-level code completion and infilling tasks.\n\n  \n\n### 2. Model Summary\ndeepseek-coder-1.3b-base is a 1.3B parameter model with Multi-Head Attention trained on 1 trillion tokens.\n- **Home Page:** [DeepSeek](https://deepseek.com/)\n- **Repository:** [deepseek-ai/deepseek-coder](https://github.com/deepseek-ai/deepseek-coder)\n- **Chat With DeepSeek Coder:** [DeepSeek-Coder](https://coder.deepseek.com/)\n\n\n### 3. How to Use\nHere give some examples of how to use our model.\n#### 1ï¼‰Code Completion\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\", trust_remote_code=True).cuda()\ninput_text = \"#write a quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n#### 2ï¼‰Code Insertion\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\", trust_remote_code=True).cuda()\ninput_text = \"\"\"<ï½œfimâ–beginï½œ>def quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[0]\n    left = []\n    right = []\n<ï½œfimâ–holeï½œ>\n        if arr[i] < pivot:\n            left.append(arr[i])\n        else:\n            right.append(arr[i])\n    return quick_sort(left) + [pivot] + quick_sort(right)<ï½œfimâ–endï½œ>\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(input_text):])\n```\n\n#### 3ï¼‰Repository Level Code Completion\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\", trust_remote_code=True).cuda()\n\ninput_text = \"\"\"#utils.py\nimport torch\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\ndef load_data():\n    iris = datasets.load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Standardize the data\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    # Convert numpy data to PyTorch tensors\n    X_train = torch.tensor(X_train, dtype=torch.float32)\n    X_test = torch.tensor(X_test, dtype=torch.float32)\n    y_train = torch.tensor(y_train, dtype=torch.int64)\n    y_test = torch.tensor(y_test, dtype=torch.int64)\n    \n    return X_train, X_test, y_train, y_test\n\ndef evaluate_predictions(y_test, y_pred):\n    return accuracy_score(y_test, y_pred)\n#model.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nclass IrisClassifier(nn.Module):\n    def __init__(self):\n        super(IrisClassifier, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(4, 16),\n            nn.ReLU(),\n            nn.Linear(16, 3)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n    def train_model(self, X_train, y_train, epochs, lr, batch_size):\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(self.parameters(), lr=lr)\n        \n        # Create DataLoader for batches\n        dataset = TensorDataset(X_train, y_train)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n        for epoch in range(epochs):\n            for batch_X, batch_y in dataloader:\n                optimizer.zero_grad()\n                outputs = self(batch_X)\n                loss = criterion(outputs, batch_y)\n                loss.backward()\n                optimizer.step()\n\n    def predict(self, X_test):\n        with torch.no_grad():\n            outputs = self(X_test)\n            _, predicted = outputs.max(1)\n        return predicted.numpy()\n#main.py\nfrom utils import load_data, evaluate_predictions\nfrom model import IrisClassifier as Classifier\n\ndef main():\n    # Model training and evaluation\n\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=140)\nprint(tokenizer.decode(outputs[0]))\n```\n\n\n\n### 4. License\nThis code repository is licensed under the MIT License. The use of DeepSeek Coder models is subject to the Model License. DeepSeek Coder supports commercial use.\n\nSee the [LICENSE-MODEL](https://github.com/deepseek-ai/deepseek-coder/blob/main/LICENSE-MODEL) for more details.\n\n### 5. Contact\n\nIf you have any questions, please raise an issue or contact us at [agi_code@deepseek.com](mailto:agi_code@deepseek.com).\n", "downloads": 56265, "likes": 98, "meta": {"license": "other", "license_name": "deepseek-license", "license_link": "LICENSE"}, "inference_type": "local"}
{"id": "sileod/deberta-v3-base-tasksource-nli", "pipeline_tag": "zero-shot-classification", "tags": ["transformers", "pytorch", "safetensors", "deberta-v2", "text-classification", "deberta-v3-base", "deberta-v3", "deberta", "nli", "natural-language-inference", "multitask", "multi-task", "pipeline", "extreme-multi-task", "extreme-mtl", "tasksource", "zero-shot", "rlhf", "zero-shot-classification", "en", "dataset:glue", "dataset:nyu-mll/multi_nli", "dataset:multi_nli", "dataset:super_glue", "dataset:anli", "dataset:tasksource/babi_nli", "dataset:sick", "dataset:snli", "dataset:scitail", "dataset:OpenAssistant/oasst1", "dataset:universal_dependencies", "dataset:hans", "dataset:qbao775/PARARULE-Plus", "dataset:alisawuffles/WANLI", "dataset:metaeval/recast", "dataset:sileod/probability_words_nli", "dataset:joey234/nan-nli", "dataset:pietrolesci/nli_fever", "dataset:pietrolesci/breaking_nli", "dataset:pietrolesci/conj_nli", "dataset:pietrolesci/fracas", "dataset:pietrolesci/dialogue_nli", "dataset:pietrolesci/mpe", "dataset:pietrolesci/dnc", "dataset:pietrolesci/gpt3_nli", "dataset:pietrolesci/recast_white", "dataset:pietrolesci/joci", "dataset:martn-nguyen/contrast_nli", "dataset:pietrolesci/robust_nli", "dataset:pietrolesci/robust_nli_is_sd", "dataset:pietrolesci/robust_nli_li_ts", "dataset:pietrolesci/gen_debiased_nli", "dataset:pietrolesci/add_one_rte", "dataset:metaeval/imppres", "dataset:pietrolesci/glue_diagnostics", "dataset:hlgd", "dataset:PolyAI/banking77", "dataset:paws", "dataset:quora", "dataset:medical_questions_pairs", "dataset:conll2003", "dataset:nlpaueb/finer-139", "dataset:Anthropic/hh-rlhf", "dataset:Anthropic/model-written-evals", "dataset:truthful_qa", "dataset:nightingal3/fig-qa", "dataset:tasksource/bigbench", "dataset:blimp", "dataset:cos_e", "dataset:cosmos_qa", "dataset:dream", "dataset:openbookqa", "dataset:qasc", "dataset:quartz", "dataset:quail", "dataset:head_qa", "dataset:sciq", "dataset:social_i_qa", "dataset:wiki_hop", "dataset:wiqa", "dataset:piqa", "dataset:hellaswag", "dataset:pkavumba/balanced-copa", "dataset:12ml/e-CARE", "dataset:art", "dataset:tasksource/mmlu", "dataset:winogrande", "dataset:codah", "dataset:ai2_arc", "dataset:definite_pronoun_resolution", "dataset:swag", "dataset:math_qa", "dataset:metaeval/utilitarianism", "dataset:mteb/amazon_counterfactual", "dataset:SetFit/insincere-questions", "dataset:SetFit/toxic_conversations", "dataset:turingbench/TuringBench", "dataset:trec", "dataset:tals/vitaminc", "dataset:hope_edi", "dataset:strombergnlp/rumoureval_2019", "dataset:ethos", "dataset:tweet_eval", "dataset:discovery", "dataset:pragmeval", "dataset:silicone", "dataset:lex_glue", "dataset:papluca/language-identification", "dataset:imdb", "dataset:rotten_tomatoes", "dataset:ag_news", "dataset:yelp_review_full", "dataset:financial_phrasebank", "dataset:poem_sentiment", "dataset:dbpedia_14", "dataset:amazon_polarity", "dataset:app_reviews", "dataset:hate_speech18", "dataset:sms_spam", "dataset:humicroedit", "dataset:snips_built_in_intents", "dataset:banking77", "dataset:hate_speech_offensive", "dataset:yahoo_answers_topics", "dataset:pacovaldez/stackoverflow-questions", "dataset:zapsdcn/hyperpartisan_news", "dataset:zapsdcn/sciie", "dataset:zapsdcn/citation_intent", "dataset:go_emotions", "dataset:allenai/scicite", "dataset:liar", "dataset:relbert/lexical_relation_classification", "dataset:metaeval/linguisticprobing", "dataset:tasksource/crowdflower", "dataset:metaeval/ethics", "dataset:emo", "dataset:google_wellformed_query", "dataset:tweets_hate_speech_detection", "dataset:has_part", "dataset:wnut_17", "dataset:ncbi_disease", "dataset:acronym_identification", "dataset:jnlpba", "dataset:species_800", "dataset:SpeedOfMagic/ontonotes_english", "dataset:blog_authorship_corpus", "dataset:launch/open_question_type", "dataset:health_fact", "dataset:commonsense_qa", "dataset:mc_taco", "dataset:ade_corpus_v2", "dataset:prajjwal1/discosense", "dataset:circa", "dataset:PiC/phrase_similarity", "dataset:copenlu/scientific-exaggeration-detection", "dataset:quarel", "dataset:mwong/fever-evidence-related", "dataset:numer_sense", "dataset:dynabench/dynasent", "dataset:raquiba/Sarcasm_News_Headline", "dataset:sem_eval_2010_task_8", "dataset:demo-org/auditor_review", "dataset:medmcqa", "dataset:aqua_rat", "dataset:RuyuanWan/Dynasent_Disagreement", "dataset:RuyuanWan/Politeness_Disagreement", "dataset:RuyuanWan/SBIC_Disagreement", "dataset:RuyuanWan/SChem_Disagreement", "dataset:RuyuanWan/Dilemmas_Disagreement", "dataset:lucasmccabe/logiqa", "dataset:wiki_qa", "dataset:metaeval/cycic_classification", "dataset:metaeval/cycic_multiplechoice", "dataset:metaeval/sts-companion", "dataset:metaeval/commonsense_qa_2.0", "dataset:metaeval/lingnli", "dataset:metaeval/monotonicity-entailment", "dataset:metaeval/arct", "dataset:metaeval/scinli", "dataset:metaeval/naturallogic", "dataset:onestop_qa", "dataset:demelin/moral_stories", "dataset:corypaik/prost", "dataset:aps/dynahate", "dataset:metaeval/syntactic-augmentation-nli", "dataset:metaeval/autotnli", "dataset:lasha-nlp/CONDAQA", "dataset:openai/webgpt_comparisons", "dataset:Dahoas/synthetic-instruct-gptj-pairwise", "dataset:metaeval/scruples", "dataset:metaeval/wouldyourather", "dataset:sileod/attempto-nli", "dataset:metaeval/defeasible-nli", "dataset:metaeval/help-nli", "dataset:metaeval/nli-veridicality-transitivity", "dataset:metaeval/natural-language-satisfiability", "dataset:metaeval/lonli", "dataset:tasksource/dadc-limit-nli", "dataset:ColumbiaNLP/FLUTE", "dataset:metaeval/strategy-qa", "dataset:openai/summarize_from_feedback", "dataset:tasksource/folio", "dataset:metaeval/tomi-nli", "dataset:metaeval/avicenna", "dataset:stanfordnlp/SHP", "dataset:GBaker/MedQA-USMLE-4-options-hf", "dataset:GBaker/MedQA-USMLE-4-options", "dataset:sileod/wikimedqa", "dataset:declare-lab/cicero", "dataset:amydeng2000/CREAK", "dataset:metaeval/mutual", "dataset:inverse-scaling/NeQA", "dataset:inverse-scaling/quote-repetition", "dataset:inverse-scaling/redefine-math", "dataset:tasksource/puzzte", "dataset:metaeval/implicatures", "dataset:race", "dataset:metaeval/spartqa-yn", "dataset:metaeval/spartqa-mchoice", "dataset:metaeval/temporal-nli", "dataset:metaeval/ScienceQA_text_only", "dataset:AndyChiang/cloth", "dataset:metaeval/logiqa-2.0-nli", "dataset:tasksource/oasst1_dense_flat", "dataset:metaeval/boolq-natural-perturbations", "dataset:metaeval/path-naturalness-prediction", "dataset:riddle_sense", "dataset:Jiangjie/ekar_english", "dataset:metaeval/implicit-hate-stg1", "dataset:metaeval/chaos-mnli-ambiguity", "dataset:IlyaGusev/headline_cause", "dataset:metaeval/race-c", "dataset:metaeval/equate", "dataset:metaeval/ambient", "dataset:AndyChiang/dgen", "dataset:metaeval/clcd-english", "dataset:civil_comments", "dataset:metaeval/acceptability-prediction", "dataset:maximedb/twentyquestions", "dataset:metaeval/counterfactually-augmented-snli", "dataset:tasksource/I2D2", "dataset:sileod/mindgames", "dataset:metaeval/counterfactually-augmented-imdb", "dataset:metaeval/cnli", "dataset:metaeval/reclor", "dataset:tasksource/oasst1_pairwise_rlhf_reward", "dataset:tasksource/zero-shot-label-nli", "dataset:webis/args_me", "dataset:webis/Touche23-ValueEval", "dataset:tasksource/starcon", "dataset:tasksource/ruletaker", "dataset:lighteval/lsat_qa", "dataset:tasksource/ConTRoL-nli", "dataset:tasksource/tracie", "dataset:tasksource/sherliic", "dataset:tasksource/sen-making", "dataset:tasksource/winowhy", "dataset:mediabiasgroup/mbib-base", "dataset:tasksource/robustLR", "dataset:CLUTRR/v1", "dataset:tasksource/logical-fallacy", "dataset:tasksource/parade", "dataset:tasksource/cladder", "dataset:tasksource/subjectivity", "dataset:tasksource/MOH", "dataset:tasksource/VUAC", "dataset:tasksource/TroFi", "dataset:sharc_modified", "dataset:tasksource/conceptrules_v2", "dataset:tasksource/disrpt", "dataset:conll2000", "dataset:DFKI-SLT/few-nerd", "dataset:tasksource/com2sense", "dataset:tasksource/scone", "dataset:tasksource/winodict", "dataset:tasksource/fool-me-twice", "dataset:tasksource/monli", "dataset:tasksource/corr2cause", "dataset:tasksource/apt", "dataset:zeroshot/twitter-financial-news-sentiment", "dataset:tasksource/icl-symbol-tuning-instruct", "dataset:tasksource/SpaceNLI", "dataset:sihaochen/propsegment", "dataset:HannahRoseKirk/HatemojiBuild", "dataset:tasksource/regset", "dataset:lmsys/chatbot_arena_conversations", "dataset:tasksource/nlgraph", "arxiv:2301.05948", "license:apache-2.0", "model-index", "autotrain_compatible", "endpoints_compatible", "region:us"], "description": "---\nlicense: apache-2.0\nlanguage: en\ntags:\n- deberta-v3-base\n- deberta-v3\n- deberta\n- text-classification\n- nli\n- natural-language-inference\n- multitask\n- multi-task\n- pipeline\n- extreme-multi-task\n- extreme-mtl\n- tasksource\n- zero-shot\n- rlhf\ndatasets:\n- glue\n- nyu-mll/multi_nli\n- multi_nli\n- super_glue\n- anli\n- tasksource/babi_nli\n- sick\n- snli\n- scitail\n- OpenAssistant/oasst1\n- universal_dependencies\n- hans\n- qbao775/PARARULE-Plus\n- alisawuffles/WANLI\n- metaeval/recast\n- sileod/probability_words_nli\n- joey234/nan-nli\n- pietrolesci/nli_fever\n- pietrolesci/breaking_nli\n- pietrolesci/conj_nli\n- pietrolesci/fracas\n- pietrolesci/dialogue_nli\n- pietrolesci/mpe\n- pietrolesci/dnc\n- pietrolesci/gpt3_nli\n- pietrolesci/recast_white\n- pietrolesci/joci\n- martn-nguyen/contrast_nli\n- pietrolesci/robust_nli\n- pietrolesci/robust_nli_is_sd\n- pietrolesci/robust_nli_li_ts\n- pietrolesci/gen_debiased_nli\n- pietrolesci/add_one_rte\n- metaeval/imppres\n- pietrolesci/glue_diagnostics\n- hlgd\n- PolyAI/banking77\n- paws\n- quora\n- medical_questions_pairs\n- conll2003\n- nlpaueb/finer-139\n- Anthropic/hh-rlhf\n- Anthropic/model-written-evals\n- truthful_qa\n- nightingal3/fig-qa\n- tasksource/bigbench\n- blimp\n- cos_e\n- cosmos_qa\n- dream\n- openbookqa\n- qasc\n- quartz\n- quail\n- head_qa\n- sciq\n- social_i_qa\n- wiki_hop\n- wiqa\n- piqa\n- hellaswag\n- pkavumba/balanced-copa\n- 12ml/e-CARE\n- art\n- tasksource/mmlu\n- winogrande\n- codah\n- ai2_arc\n- definite_pronoun_resolution\n- swag\n- math_qa\n- metaeval/utilitarianism\n- mteb/amazon_counterfactual\n- SetFit/insincere-questions\n- SetFit/toxic_conversations\n- turingbench/TuringBench\n- trec\n- tals/vitaminc\n- hope_edi\n- strombergnlp/rumoureval_2019\n- ethos\n- tweet_eval\n- discovery\n- pragmeval\n- silicone\n- lex_glue\n- papluca/language-identification\n- imdb\n- rotten_tomatoes\n- ag_news\n- yelp_review_full\n- financial_phrasebank\n- poem_sentiment\n- dbpedia_14\n- amazon_polarity\n- app_reviews\n- hate_speech18\n- sms_spam\n- humicroedit\n- snips_built_in_intents\n- banking77\n- hate_speech_offensive\n- yahoo_answers_topics\n- pacovaldez/stackoverflow-questions\n- zapsdcn/hyperpartisan_news\n- zapsdcn/sciie\n- zapsdcn/citation_intent\n- go_emotions\n- allenai/scicite\n- liar\n- relbert/lexical_relation_classification\n- metaeval/linguisticprobing\n- tasksource/crowdflower\n- metaeval/ethics\n- emo\n- google_wellformed_query\n- tweets_hate_speech_detection\n- has_part\n- wnut_17\n- ncbi_disease\n- acronym_identification\n- jnlpba\n- species_800\n- SpeedOfMagic/ontonotes_english\n- blog_authorship_corpus\n- launch/open_question_type\n- health_fact\n- commonsense_qa\n- mc_taco\n- ade_corpus_v2\n- prajjwal1/discosense\n- circa\n- PiC/phrase_similarity\n- copenlu/scientific-exaggeration-detection\n- quarel\n- mwong/fever-evidence-related\n- numer_sense\n- dynabench/dynasent\n- raquiba/Sarcasm_News_Headline\n- sem_eval_2010_task_8\n- demo-org/auditor_review\n- medmcqa\n- aqua_rat\n- RuyuanWan/Dynasent_Disagreement\n- RuyuanWan/Politeness_Disagreement\n- RuyuanWan/SBIC_Disagreement\n- RuyuanWan/SChem_Disagreement\n- RuyuanWan/Dilemmas_Disagreement\n- lucasmccabe/logiqa\n- wiki_qa\n- metaeval/cycic_classification\n- metaeval/cycic_multiplechoice\n- metaeval/sts-companion\n- metaeval/commonsense_qa_2.0\n- metaeval/lingnli\n- metaeval/monotonicity-entailment\n- metaeval/arct\n- metaeval/scinli\n- metaeval/naturallogic\n- onestop_qa\n- demelin/moral_stories\n- corypaik/prost\n- aps/dynahate\n- metaeval/syntactic-augmentation-nli\n- metaeval/autotnli\n- lasha-nlp/CONDAQA\n- openai/webgpt_comparisons\n- Dahoas/synthetic-instruct-gptj-pairwise\n- metaeval/scruples\n- metaeval/wouldyourather\n- sileod/attempto-nli\n- metaeval/defeasible-nli\n- metaeval/help-nli\n- metaeval/nli-veridicality-transitivity\n- metaeval/natural-language-satisfiability\n- metaeval/lonli\n- tasksource/dadc-limit-nli\n- ColumbiaNLP/FLUTE\n- metaeval/strategy-qa\n- openai/summarize_from_feedback\n- tasksource/folio\n- metaeval/tomi-nli\n- metaeval/avicenna\n- stanfordnlp/SHP\n- GBaker/MedQA-USMLE-4-options-hf\n- GBaker/MedQA-USMLE-4-options\n- sileod/wikimedqa\n- declare-lab/cicero\n- amydeng2000/CREAK\n- metaeval/mutual\n- inverse-scaling/NeQA\n- inverse-scaling/quote-repetition\n- inverse-scaling/redefine-math\n- tasksource/puzzte\n- metaeval/implicatures\n- race\n- metaeval/spartqa-yn\n- metaeval/spartqa-mchoice\n- metaeval/temporal-nli\n- metaeval/ScienceQA_text_only\n- AndyChiang/cloth\n- metaeval/logiqa-2.0-nli\n- tasksource/oasst1_dense_flat\n- metaeval/boolq-natural-perturbations\n- metaeval/path-naturalness-prediction\n- riddle_sense\n- Jiangjie/ekar_english\n- metaeval/implicit-hate-stg1\n- metaeval/chaos-mnli-ambiguity\n- IlyaGusev/headline_cause\n- metaeval/race-c\n- metaeval/equate\n- metaeval/ambient\n- AndyChiang/dgen\n- metaeval/clcd-english\n- civil_comments\n- metaeval/acceptability-prediction\n- maximedb/twentyquestions\n- metaeval/counterfactually-augmented-snli\n- tasksource/I2D2\n- sileod/mindgames\n- metaeval/counterfactually-augmented-imdb\n- metaeval/cnli\n- metaeval/reclor\n- tasksource/oasst1_pairwise_rlhf_reward\n- tasksource/zero-shot-label-nli\n- webis/args_me\n- webis/Touche23-ValueEval\n- tasksource/starcon\n- tasksource/ruletaker\n- lighteval/lsat_qa\n- tasksource/ConTRoL-nli\n- tasksource/tracie\n- tasksource/sherliic\n- tasksource/sen-making\n- tasksource/winowhy\n- mediabiasgroup/mbib-base\n- tasksource/robustLR\n- CLUTRR/v1\n- tasksource/logical-fallacy\n- tasksource/parade\n- tasksource/cladder\n- tasksource/subjectivity\n- tasksource/MOH\n- tasksource/VUAC\n- tasksource/TroFi\n- sharc_modified\n- tasksource/conceptrules_v2\n- tasksource/disrpt\n- conll2000\n- DFKI-SLT/few-nerd\n- tasksource/com2sense\n- tasksource/scone\n- tasksource/winodict\n- tasksource/fool-me-twice\n- tasksource/monli\n- tasksource/corr2cause\n- tasksource/apt\n- zeroshot/twitter-financial-news-sentiment\n- tasksource/icl-symbol-tuning-instruct\n- tasksource/SpaceNLI\n- sihaochen/propsegment\n- HannahRoseKirk/HatemojiBuild\n- tasksource/regset\n- tasksource/babi_nli\n- lmsys/chatbot_arena_conversations\n- tasksource/nlgraph\nmetrics:\n- accuracy\nlibrary_name: transformers\npipeline_tag: zero-shot-classification\nmodel-index:\n- name: deberta-v3-base-tasksource-nli\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: glue\n      type: glue\n      config: rte\n      split: validation\n    metrics:\n    - type: accuracy\n      value: 0.89\n  - task:\n      type: natural-language-inference\n      name: Natural Language Inference\n    dataset:\n      name: anli-r3\n      type: anli\n      config: plain_text\n      split: validation\n    metrics:\n    - type: accuracy\n      value: 0.52\n      name: Accuracy\n---\n\n# Model Card for DeBERTa-v3-base-tasksource-nli\n\n---\n**NOTE**\n\nDeprecated: use https://huggingface.co/tasksource/deberta-small-long-nli for longer context and better accuracy.\n\n---\n\nThis is [DeBERTa-v3-base](https://hf.co/microsoft/deberta-v3-base) fine-tuned with multi-task learning on 600+ tasks of the [tasksource collection](https://github.com/sileod/tasksource/).\nThis checkpoint has strong zero-shot validation performance on many tasks (e.g. 70% on WNLI), and can be used for:\n- Zero-shot entailment-based classification for arbitrary labels [ZS].\n- Natural language inference [NLI]\n- Hundreds of previous tasks with tasksource-adapters [TA].\n- Further fine-tuning on a new task or tasksource task (classification, token classification or multiple-choice) [FT].\n\n# [ZS] Zero-shot classification pipeline\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\",model=\"sileod/deberta-v3-base-tasksource-nli\")\n\ntext = \"one day I will see the world\"\ncandidate_labels = ['travel', 'cooking', 'dancing']\nclassifier(text, candidate_labels)\n```\nNLI training data of this model includes [label-nli](https://huggingface.co/datasets/tasksource/zero-shot-label-nli), a NLI dataset specially constructed to improve this kind of zero-shot classification.\n\n# [NLI] Natural language inference pipeline\n\n```python\nfrom transformers import pipeline\npipe = pipeline(\"text-classification\",model=\"sileod/deberta-v3-base-tasksource-nli\")\npipe([dict(text='there is a cat',\n  text_pair='there is a black cat')]) #list of (premise,hypothesis)\n# [{'label': 'neutral', 'score': 0.9952911138534546}]\n```\n\n# [TA] Tasksource-adapters: 1 line access to hundreds of tasks \n\n```python\n# !pip install tasknet\nimport tasknet as tn\npipe = tn.load_pipeline('sileod/deberta-v3-base-tasksource-nli','glue/sst2') # works for 500+ tasksource tasks\npipe(['That movie was great !', 'Awful movie.'])\n# [{'label': 'positive', 'score': 0.9956}, {'label': 'negative', 'score': 0.9967}]\n```\nThe list of tasks is available in model config.json.\nThis is more efficient than ZS since it requires only one forward pass per example, but it is less flexible.\n\n\n# [FT] Tasknet: 3 lines fine-tuning\n\n```python\n# !pip install tasknet\nimport tasknet as tn\nhparams=dict(model_name='sileod/deberta-v3-base-tasksource-nli', learning_rate=2e-5)\nmodel, trainer = tn.Model_Trainer([tn.AutoTask(\"glue/rte\")], hparams)\ntrainer.train()\n```\n\n## Evaluation\nThis model ranked 1st among all models with the microsoft/deberta-v3-base architecture according to the IBM model recycling evaluation.\nhttps://ibm.github.io/model-recycling/\n\n### Software and training details\n\nThe model was trained on 600 tasks for 200k steps with a batch size of 384 and a peak learning rate of 2e-5. Training took 15 days on Nvidia A30 24GB gpu.\nThis is the shared model with the MNLI classifier on top. Each task had a specific CLS embedding, which is dropped 10% of the time to facilitate model use without it. All multiple-choice model used the same classification layers. For classification tasks, models shared weights if their labels matched.\n\n\nhttps://github.com/sileod/tasksource/ \\\nhttps://github.com/sileod/tasknet/ \\\nTraining code: https://colab.research.google.com/drive/1iB4Oxl9_B5W3ZDzXoWJN-olUbqLBxgQS?usp=sharing\n\n# Citation\n\nMore details on this [article:](https://arxiv.org/abs/2301.05948) \n```\n@article{sileo2023tasksource,\n  title={tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation},\n  author={Sileo, Damien},\n  url= {https://arxiv.org/abs/2301.05948},\n  journal={arXiv preprint arXiv:2301.05948},\n  year={2023}\n}\n```\n\n\n# Model Card Contact\n\ndamien.sileo@inria.fr\n\n\n</details>", "downloads": 76279, "likes": 128, "meta": {"datasets": ["glue", "nyu-mll/multi_nli", "multi_nli", "super_glue", "anli", "tasksource/babi_nli", "sick", "snli", "scitail", "OpenAssistant/oasst1", "universal_dependencies", "hans", "qbao775/PARARULE-Plus", "alisawuffles/WANLI", "metaeval/recast", "sileod/probability_words_nli", "joey234/nan-nli", "pietrolesci/nli_fever", "pietrolesci/breaking_nli", "pietrolesci/conj_nli", "pietrolesci/fracas", "pietrolesci/dialogue_nli", "pietrolesci/mpe", "pietrolesci/dnc", "pietrolesci/gpt3_nli", "pietrolesci/recast_white", "pietrolesci/joci", "martn-nguyen/contrast_nli", "pietrolesci/robust_nli", "pietrolesci/robust_nli_is_sd", "pietrolesci/robust_nli_li_ts", "pietrolesci/gen_debiased_nli", "pietrolesci/add_one_rte", "metaeval/imppres", "pietrolesci/glue_diagnostics", "hlgd", "PolyAI/banking77", "paws", "quora", "medical_questions_pairs", "conll2003", "nlpaueb/finer-139", "Anthropic/hh-rlhf", "Anthropic/model-written-evals", "truthful_qa", "nightingal3/fig-qa", "tasksource/bigbench", "blimp", "cos_e", "cosmos_qa", "dream", "openbookqa", "qasc", "quartz", "quail", "head_qa", "sciq", "social_i_qa", "wiki_hop", "wiqa", "piqa", "hellaswag", "pkavumba/balanced-copa", "12ml/e-CARE", "art", "tasksource/mmlu", "winogrande", "codah", "ai2_arc", "definite_pronoun_resolution", "swag", "math_qa", "metaeval/utilitarianism", "mteb/amazon_counterfactual", "SetFit/insincere-questions", "SetFit/toxic_conversations", "turingbench/TuringBench", "trec", "tals/vitaminc", "hope_edi", "strombergnlp/rumoureval_2019", "ethos", "tweet_eval", "discovery", "pragmeval", "silicone", "lex_glue", "papluca/language-identification", "imdb", "rotten_tomatoes", "ag_news", "yelp_review_full", "financial_phrasebank", "poem_sentiment", "dbpedia_14", "amazon_polarity", "app_reviews", "hate_speech18", "sms_spam", "humicroedit", "snips_built_in_intents", "banking77", "hate_speech_offensive", "yahoo_answers_topics", "pacovaldez/stackoverflow-questions", "zapsdcn/hyperpartisan_news", "zapsdcn/sciie", "zapsdcn/citation_intent", "go_emotions", "allenai/scicite", "liar", "relbert/lexical_relation_classification", "metaeval/linguisticprobing", "tasksource/crowdflower", "metaeval/ethics", "emo", "google_wellformed_query", "tweets_hate_speech_detection", "has_part", "wnut_17", "ncbi_disease", "acronym_identification", "jnlpba", "species_800", "SpeedOfMagic/ontonotes_english", "blog_authorship_corpus", "launch/open_question_type", "health_fact", "commonsense_qa", "mc_taco", "ade_corpus_v2", "prajjwal1/discosense", "circa", "PiC/phrase_similarity", "copenlu/scientific-exaggeration-detection", "quarel", "mwong/fever-evidence-related", "numer_sense", "dynabench/dynasent", "raquiba/Sarcasm_News_Headline", "sem_eval_2010_task_8", "demo-org/auditor_review", "medmcqa", "aqua_rat", "RuyuanWan/Dynasent_Disagreement", "RuyuanWan/Politeness_Disagreement", "RuyuanWan/SBIC_Disagreement", "RuyuanWan/SChem_Disagreement", "RuyuanWan/Dilemmas_Disagreement", "lucasmccabe/logiqa", "wiki_qa", "metaeval/cycic_classification", "metaeval/cycic_multiplechoice", "metaeval/sts-companion", "metaeval/commonsense_qa_2.0", "metaeval/lingnli", "metaeval/monotonicity-entailment", "metaeval/arct", "metaeval/scinli", "metaeval/naturallogic", "onestop_qa", "demelin/moral_stories", "corypaik/prost", "aps/dynahate", "metaeval/syntactic-augmentation-nli", "metaeval/autotnli", "lasha-nlp/CONDAQA", "openai/webgpt_comparisons", "Dahoas/synthetic-instruct-gptj-pairwise", "metaeval/scruples", "metaeval/wouldyourather", "sileod/attempto-nli", "metaeval/defeasible-nli", "metaeval/help-nli", "metaeval/nli-veridicality-transitivity", "metaeval/natural-language-satisfiability", "metaeval/lonli", "tasksource/dadc-limit-nli", "ColumbiaNLP/FLUTE", "metaeval/strategy-qa", "openai/summarize_from_feedback", "tasksource/folio", "metaeval/tomi-nli", "metaeval/avicenna", "stanfordnlp/SHP", "GBaker/MedQA-USMLE-4-options-hf", "GBaker/MedQA-USMLE-4-options", "sileod/wikimedqa", "declare-lab/cicero", "amydeng2000/CREAK", "metaeval/mutual", "inverse-scaling/NeQA", "inverse-scaling/quote-repetition", "inverse-scaling/redefine-math", "tasksource/puzzte", "metaeval/implicatures", "race", "metaeval/spartqa-yn", "metaeval/spartqa-mchoice", "metaeval/temporal-nli", "metaeval/ScienceQA_text_only", "AndyChiang/cloth", "metaeval/logiqa-2.0-nli", "tasksource/oasst1_dense_flat", "metaeval/boolq-natural-perturbations", "metaeval/path-naturalness-prediction", "riddle_sense", "Jiangjie/ekar_english", "metaeval/implicit-hate-stg1", "metaeval/chaos-mnli-ambiguity", "IlyaGusev/headline_cause", "metaeval/race-c", "metaeval/equate", "metaeval/ambient", "AndyChiang/dgen", "metaeval/clcd-english", "civil_comments", "metaeval/acceptability-prediction", "maximedb/twentyquestions", "metaeval/counterfactually-augmented-snli", "tasksource/I2D2", "sileod/mindgames", "metaeval/counterfactually-augmented-imdb", "metaeval/cnli", "metaeval/reclor", "tasksource/oasst1_pairwise_rlhf_reward", "tasksource/zero-shot-label-nli", "webis/args_me", "webis/Touche23-ValueEval", "tasksource/starcon", "tasksource/ruletaker", "lighteval/lsat_qa", "tasksource/ConTRoL-nli", "tasksource/tracie", "tasksource/sherliic", "tasksource/sen-making", "tasksource/winowhy", "mediabiasgroup/mbib-base", "tasksource/robustLR", "CLUTRR/v1", "tasksource/logical-fallacy", "tasksource/parade", "tasksource/cladder", "tasksource/subjectivity", "tasksource/MOH", "tasksource/VUAC", "tasksource/TroFi", "sharc_modified", "tasksource/conceptrules_v2", "tasksource/disrpt", "conll2000", "DFKI-SLT/few-nerd", "tasksource/com2sense", "tasksource/scone", "tasksource/winodict", "tasksource/fool-me-twice", "tasksource/monli", "tasksource/corr2cause", "tasksource/apt", "zeroshot/twitter-financial-news-sentiment", "tasksource/icl-symbol-tuning-instruct", "tasksource/SpaceNLI", "sihaochen/propsegment", "HannahRoseKirk/HatemojiBuild", "tasksource/regset", "tasksource/babi_nli", "lmsys/chatbot_arena_conversations", "tasksource/nlgraph"], "language": "en", "library_name": "transformers", "license": "apache-2.0", "metrics": ["accuracy"], "pipeline_tag": "zero-shot-classification", "tags": ["deberta-v3-base", "deberta-v3", "deberta", "text-classification", "nli", "natural-language-inference", "multitask", "multi-task", "pipeline", "extreme-multi-task", "extreme-mtl", "tasksource", "zero-shot", "rlhf"], "model-index": [{"name": "deberta-v3-base-tasksource-nli", "results": [{"task": {"type": "text-classification", "name": "Text Classification"}, "dataset": {"name": "glue", "type": "glue", "config": "rte", "split": "validation"}, "metrics": [{"type": "accuracy", "value": 0.89, "verified": false}]}, {"task": {"type": "natural-language-inference", "name": "Natural Language Inference"}, "dataset": {"name": "anli-r3", "type": "anli", "config": "plain_text", "split": "validation"}, "metrics": [{"type": "accuracy", "value": 0.52, "name": "Accuracy", "verified": false}]}]}]}, "inference_type": "huggingface"}
{"id": "Helsinki-NLP/opus-mt-en-ru", "pipeline_tag": "translation", "tags": ["transformers", "pytorch", "tf", "rust", "marian", "text2text-generation", "translation", "en", "ru", "license:apache-2.0", "endpoints_compatible", "region:us"], "description": "---\ntags:\n- translation\nlicense: apache-2.0\n---\n\n### opus-mt-en-ru\n\n* source languages: en\n* target languages: ru\n*  OPUS readme: [en-ru](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/en-ru/README.md)\n\n*  dataset: opus\n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* download original weights: [opus-2020-02-11.zip](https://object.pouta.csc.fi/OPUS-MT-models/en-ru/opus-2020-02-11.zip)\n* test set translations: [opus-2020-02-11.test.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-ru/opus-2020-02-11.test.txt)\n* test set scores: [opus-2020-02-11.eval.txt](https://object.pouta.csc.fi/OPUS-MT-models/en-ru/opus-2020-02-11.eval.txt)\n\n## Benchmarks\n\n| testset               | BLEU  | chr-F |\n|-----------------------|-------|-------|\n| newstest2012.en.ru \t| 31.1 \t| 0.581 |\n| newstest2013.en.ru \t| 23.5 \t| 0.513 |\n| newstest2015-enru.en.ru \t| 27.5 \t| 0.564 |\n| newstest2016-enru.en.ru \t| 26.4 \t| 0.548 |\n| newstest2017-enru.en.ru \t| 29.1 \t| 0.572 |\n| newstest2018-enru.en.ru \t| 25.4 \t| 0.554 |\n| newstest2019-enru.en.ru \t| 27.1 \t| 0.533 |\n| Tatoeba.en.ru \t| 48.4 \t| 0.669 |\n\n, 48 |\n| newstest2017-enru.en.ru \t| 29.1 \t| 0.572 |\n| newstest2018-enru.en.ru \t| 25.4 \t| 0.554 |\n| newstes48 |\n| newstest2017-enru.en.ru \t| 29.1 \t| 0.572 |\n| newstest2018-enru.en.ru \t| 25.4 \t| 0.554 |\n| newstes48 |\n| newstest2017-enru.en.ru \t| 29.1 \t| 0.572 |\n| newstest2018-enru.en.ru \t| 25.4 \t| 0.554 |\n| newstes48 |\n| newstest2017-enru.en.ru \t| 29.1 \t| 0.572 |\n| newstest2018-enru.en.ru \t| 25.4 \t| 0.554 |\n| newstes48 |\n| newstest2017-enru.en.ru \t| 29.1 \t| 0.572 |\n| newstest2018-enru.en.ru \t| 25.4 \t| 0.554 |\n| newstest2019-enru.en.ru \t| 27.1 \t| 0.533 |\n| Tatoeba.en.ru \t| 48.4 \t| 0.669 |\n\n", "downloads": 59498, "likes": 88, "meta": {"license": "apache-2.0", "tags": ["translation"]}, "inference_type": "huggingface"}
